[{"title":"Deep-ADMM-Net","url":"/2019/09/23/Deep-ADMM-Net/","content":"## Introduction of Deep ADMM-Net\nI am currently working in Math AI lab, and Deep ADMM Net is one of our main targets recently depends on papers:\n- Yang, Y., Sun, J., Li, H., & Xu, Z. (2016). Deep ADMM-Net for compressive sensing MRI\n- Yang, Y., Sun, J., Li, H., & Xu, Z. (2017). ADMM-Net: A Deep Learning Approach for Compressive Sensing MRI\n\n## A deep dive into Deep ADMM-Net\n### Deep ADMM-Net for FastMRI\n#### Compressive Sensing MRI Model and ADMM Algorithm\n*General CS-MRI Model problem*\n$$\n\\hat{x}=\\underset{x}{\\arg \\min }\\left\\{\\frac{1}{2}\\|A x-y\\|_{2}^{2}+\\sum_{l=1}^{L} \\lambda_{l} g\\left(D_{l} x\\right)\\right\\}\n$$\nwhere $A=P F \\in \\mathbb{R}^{N^{\\prime} \\times N}$ is a  measurement matrix, $P \\in \\mathbb{R}^{N^{\\prime} \\times N}$ is a under-sampling matrix and $F$ is a Fourier transform.\n\n*ADMM solver*\nBy introducing $ z=\\left\\{z_{1}, z_{2}, \\cdots, z_{L}\\right\\} $, \nthe problem equals to \n$$\n\\min _{x, z} \\frac{1}{2}\\|A x-y\\|_{2}^{2}+\\sum_{l=1}^{L} \\lambda_{l} g\\left(z_{l}\\right) \\quad \\text { s.t. } z_{l}=D_{l} x, \\quad \\forall l \\in[1,2, \\cdots, L]\n$$\n\nand its augmented Lagrangian function is\n$$\n\\mathfrak{L}_{\\rho}(x, z, \\alpha)=\\frac{1}{2}\\|A x-y\\|_{2}^{2}+\\sum_{l=1}^{L} \\lambda_{l} g\\left(z_{l}\\right)-\\sum_{l=1}^{L}\\left\\langle\\alpha_{l}, z_{l}-D_{l} x\\right\\rangle+\\sum_{l=1}^{L} \\frac{\\rho_{l}}{2}\\left\\|z_{l}-D_{l} x\\right\\|_{2}^{2}\n$$\n\nwhere $α = {α_l}$ are Lagrangian multipliers and $ρ = {ρ_l}$ are penalty parameters. ADMM alternatively optimizes ${x, z, α}$ by solving the following three subproblems:\n\n$$\n\\left\\{\\begin{array}{l}{x^{(n+1)}=\\arg \\min _{x} \\frac{1}{2}\\|A x-y\\|_{2}^{2}-\\sum_{l=1}^{L}\\left\\langle\\alpha_{l}^{(n)}, z_{l}^{(n)}-D_{l} x\\right\\rangle+\\sum_{l=1}^{L} \\frac{\\rho_{l}}{2}\\left\\|z_{l}^{(n)}-D_{l} x\\right\\|_{2}^{2}} \\\\ {z^{(n+1)}=\\arg \\min _{z} \\sum_{l=1}^{L} \\lambda_{l} g\\left(z_{l}\\right)-\\sum_{l=1}^{L}\\left\\langle\\alpha_{l}^{(n)}, z_{l}-D_{l} x^{(n+1)}\\right\\rangle+\\sum_{l=1}^{L} \\frac{\\rho_{l}}{2}\\left\\|z_{l}-D_{l} x^{(n+1)}\\right\\|_{2}^{2}} \\\\ {\\alpha^{(n+1)}=\\underset{\\alpha}{\\arg \\min } \\sum_{l=1}^{L}\\left\\langle\\alpha_{l}, D_{l} x^{(n+1)}-z_{l}^{(n+1)}\\right\\rangle}\\end{array}\\right.\n$$\n\n#### Deep ADMM-Net\n- Reconstruction layer\n- Convolution layer\n- Nonlinear Transform layer\n- Multiplier update layer\n\n### Network Training\nGiven pairs of training data, the loss between the network output and ground truth is deﬁned as\n$$\nE(\\Theta)=\\frac{1}{|\\Gamma|} \\sum_{\\left(y, x^{g t}\\right) \\in \\Gamma} \\frac{\\sqrt{\\left\\|\\hat{x}(y, \\Theta)-x^{g t}\\right\\|_{2}^{2}}}{\\sqrt{\\left\\|x^{g t}\\right\\|_{2}^{2}}}\n$$\n\n#### Initialization\n$$\n\\underset{x}{\\arg \\min }\\left\\{\\frac{1}{2}\\|A x-y\\|_{2}^{2}+\\lambda \\sum_{l=1}^{L}\\left\\|D_{l} x\\right\\|_{1}\\right\\}\n$$\n\n#### Gradient Computation by Backpropagation over Data Flow Graph\n- Multiplier update layer\n- Nonlinear Transform layer\n- Convolution layer\n- Reconstruction layer\n","categories":["Computer Vision"]},{"title":"SQL cheetsheet","url":"/2019/08/29/SQL-cheetsheet/","content":"## SQL Cheetsheet\nIt's more convenient to store the data in your MySQL instead of Hive to improve the efficiency and facilitate retrieval of the data if the amount of data is not that large. Here are some tips about SQL, according to [IBM Databases and SQL for Data Science](https://www.coursera.org/learn/sql-data-science/)\n\n------\n\n### String Patterns, Ranges, Sorting and Grouping\n\n**using string pattern**: like '%%'\n\n**using a range**: between ... and ...\n\n**using a set of values**: in ('', '')\n\n**Sorting**: order by ... / order by ... desc / order by 2(column number)\n\n**Eliminating Duplicates**: distinct \n\nIt seems like you don't need to write distinct(), distinct also works.\n\n**Group by clause**\n\n**Restricting the result set - Having clause**: Having \n\nworks only with the GROUP BY clause.\n\n------\n\n### Functions, Sub-Queries, Multiple Tables\n\n**Aggregate Functions**: sum(), min(), max(), avg()\n\n**Scaler and String functions**: round(), length(), ucase, lcase\n\n**Date and Time functions**: year(), month(), day(), dayofmonth(), dayofweek(), dayofyear(), week(), hour(), minute(), second()\n\n**Date or time arithmetic**: + 1 Days, CURRENT_DATE, CURRENT_TIME\n\n**Accessing multiple tables with with Implicit join**: \n\nselect * from employees E, departments D where E.DEP_ID = D.DEPT_ID_DEP;","categories":["Computer Science"]},{"title":"Discovery towards Web Scrawler","url":"/2019/07/14/webscrawler/","content":"\n### Web Scrawler is dangerous\n\nFirst and foremost, do not touch web scrawler if you are 100% sure that you wanna do this.\n\n### Beginning\n\nThe target website is enlightent, a third-party data website which have data of my company and component. \n\nWe need to do some background research first. The problems I encountered are listed:\n\n- Need to log in with WeChat account by QR code\n- Simulate click (by package selenium)\n- It's a dynamic website, you need to wait for its information loaded (by package time)\n- Write into MySQL (by package Pymysql)\n\n### Process\n\n##### First problem cannot be solve due to the security of WeChat.\n\n##### Second problem\n\nStep 1: Find the pattern in html. Using chrome, just ctrl+u or ctrl+shift+i. It needs your patience to find the thing you want. If you mistaken the pattern, you cannot get the information you want\n\nStep 2: Choose the function: by_path or by_class. The tricky point is that if there is only one class, it's okay to use by_class, if there are more than two classes, selenium would choose the first class as your output. As a result, I choose by_path\n\nStep 3: Install chrome driver according to your chrome version. Be sure to download into /anaconda3/lib/site-packages.\n\n```python\n# Choose daily\ntime.sleep(5)\ndriver.find_elements_by_id(\"rank-date-btn\")[0].click()\n# Choose year\ntime.sleep(1)\ndriver.find_element_by_class_name(\"datepicker-switch\").click()\n# Choose target month\ntime.sleep(1)\n# driver.find_element_by_class_name(\"month\").click()\nmonth_url = \"/html/body/div[2]/div[3]/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[1]/div/div/div/div[2]/table/tbody/tr/td/span[%s]\" % (l+1)\ndriver.find_element_by_xpath(month_url).click()\n# Choose target Date\ntime.sleep(1)\n# day = driver.find_element_by_class_name(\"day\").click()\nxpath = \"/html/body/div[2]/div[3]/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[1]/div/div/div/div/table/tbody/tr[%d]/td[%d]\" % ((start_date) // 7 + 1, (start_date) % 7 + 1)\ndriver.find_element_by_xpath(xpath).click()\n# Press enter\ntime.sleep(1)\ndriver.find_element_by_id(\"choose-rank\").click()\ntime.sleep(1)\n\n```\n\n##### Third Problem: Data Processing\n\n```python\nalbum_separately = string_list[j][string_list[j].find('data-name='):string_list[j].find('data-channeltype=\"tv\"')]\nif j != 0: \n    album.append(album_separately.replace('data-name=','').replace('\"',''))\npercentage_separately = string_list[j][string_list[j].find('<td class=\"sort rank-playTimesPredicted active\" style=\"\"><span>'):string_list[j].find('</span></td><td class=\"rank-playTimes\" style=\"\">')]\npercentage.append(percentage_separately.replace('<td class=\"sort rank-playTimesPredicted active\" style=\"\"><span>',''))\nclick_separately = string_list[j][string_list[j].find('</td><td class=\"rank-playTimes\" style=\"\"><span>'):string_list[j].find('</span><span class=\"star-playtimes\">')]\nif len(click_separately) >= 10:\n    click_separately = string_list[j][string_list[j].find('</td><td class=\"rank-playTimes\" style=\"\"><span>'):string_list[j].find('</span></td><td class=\"rank-average m-change\" style=\"\">')]\nclick.append(click_separately.replace('</td><td class=\"rank-playTimes\" style=\"\"><span>','').replace('</span><span class=\"star-playtimes\">',''))\n```\n\n##### Fourth problem: Log into your MySQL and use python like MySQL!\n\n```python\ndb = DB('your database')\ndb.insert(dataset)\n```\n\n \n\n### Take Care! Be sure to add time.sleep() when you do it!","categories":["Computer Science"]},{"title":"Random Forest Classification from the bottom layer","url":"/2019/05/05/RandomForest/","content":"import packages we need\n\n    import random\n    import numpy as np\n    import sys\n\nThe feature list is an array of the possible feature indicies to use. This prevents splitting on the same feature multiple times and runs a rough C45 algorithm.\n\nIn pseudocode, the general algorithm for building decision trees is:\n\n1.Check for base cases\n2.For each attribute a\n3.Find the normalized information gain ratio from splitting on a\n4.Let a_best be the attribute with the highest normalized information gain\n5.Create a decision node that splits on a_best\n6.Recur on the sublists obtained by splitting on a_best, and add those nodes as children of node\n\n    class TreeNode:\n        def __init__(self, dataSet, featureList, parent=None):\n            self.featureNumber = None  #This is the trained index of the feature to split on\n            self.featureList = featureList \n            self.threshold = None     #This is the trained threshold of the feature to split on\n            self.leftChild = None\n            self.rightChild = None\n            self.dataSet = dataSet\n            self.parent = parent\n    \n        def c45Train(self):\n            if(self.dataSet.isPure()):\n                #gets the label of the first data instance and makes a leaf node\n                #classifying it. \n                label = self.dataSet.getData()[0].getLabel()\n                leaf = LeafNode(label)\n                return leaf\n            #If there are no more features in the feature list\n            if len(self.featureList) == 0:\n                labels = self.dataSet.getLabelStatistics()\n                bestLabel = None\n                mostTimes = 0\n    \n                for key in labels:\n                    if labels[key] > mostTimes:\n                        bestLabel = key\n                        mostTimes = labels[key]\n                #Make the leaf node with the best label\n                leaf = LeafNode(bestLabel)\n                return leaf\n    \n            #Check all of the features for the split with the most \n            #information gain. Use that split.\n            currentEntropy = self.dataSet.getEntropy()\n            currentLength = self.dataSet.getLength()\n            infoGain = -1 * sys.maxsize\n            bestFeature = 0\n            bestLeft = None\n            bestRight = None\n            bestThreshold = 0\n    \n            #Feature Bagging, Random subspace\n            num = int(np.ceil(np.sqrt(len(self.featureList))))\n            featureSubset = random.sample(self.featureList, num)\n    \n            for featureIndex in featureSubset:\n                #Calculate the threshold to use for that feature\n                threshold = self.dataSet.betterThreshold(featureIndex)\n    \n                (leftSet, rightSet) = self.dataSet.splitOn(featureIndex, threshold)\n    \n                leftEntropy = leftSet.getEntropy()\n                rightEntropy = rightSet.getEntropy()\n                #Weighted entropy for this split\n                newEntropy = (leftSet.getLength() / currentLength) * leftEntropy + (rightSet.getLength() / currentLength) * rightEntropy\n                #Calculate the gain for this test\n                newIG = currentEntropy - newEntropy\n    \n                if(newIG > infoGain):\n                    #Update the best stuff\n                    infoGain = newIG\n                    bestLeft = leftSet\n                    bestRight = rightSet\n                    bestFeature = featureIndex\n                    bestThreshold = threshold\n    \n            newFeatureList = list(self.featureList)\n            newFeatureList.remove(bestFeature)\n    \n            #Another base case, if there are no good features to split on\n            if bestLeft.getLength() == 0 or bestRight.getLength() == 0:\n                labels = self.dataSet.getLabelStatistics()\n                bestLabel = None\n                mostTimes = 0\n    \n                for key in labels:\n                    if labels[key] > mostTimes:\n                        bestLabel = key\n                        mostTimes = labels[key]\n                #Make the leaf node with the best label\n                leaf = LeafNode(bestLabel)\n                return leaf\n    \n            self.threshold = bestThreshold\n            self.featureNumber = bestFeature\n    \n            leftChild = TreeNode(bestLeft, newFeatureList, self)\n            rightChild = TreeNode(bestRight, newFeatureList, self)\n    \n            self.leftChild = leftChild.c45Train()\n            self.rightChild = rightChild.c45Train()\n    \n            return self\n            \n        def __str__(self):\n            return str(self.featureList)\n    \n        def __repr__(self):\n            return self.__str__()\n                    \n        def classify(self, sample):\n            '''\n            Recursivly traverse the tree to classify the sample that is passed in. \n            '''\n    \n            value = sample.getFeatures()[self.featureNumber]\n    \n            if(value < self.threshold):\n                #Continue down the left child    \n                return self.leftChild.classify(sample)\n    \n            else:\n                #continue down the right child\n                return self.rightChild.classify(sample)\n\n\n    class LeafNode:\n        '''\n        A leaf node is a node that just has a classification \n        and is used to cap off a tree.\n        '''\n    \n        def __init__(self, classification):\n            self.classification = classification\n    \n        def classify(self, sample):\n            #A leaf node simply is a classification, return that\n            #This is the base case of the classify recursive function for TreeNodes\n            return self.classification\n\n\n    class C45Tree:\n        '''\n        A tree contains a root node and from here\n        does the training and classification. Tree objects also\n        are responsible for having the data that they use to train.\n        '''\n    \n        def __init__(self, data):\n            self.rootNode = None\n            self.data = data\n    \n        def train(self):\n            '''\n            Trains a decision tree classifier on data set passed in. \n            The data set should contain a good mix of each class to be\n            classified.\n            '''\n            length = self.data.getFeatureLength()\n            featureIndices = range(length)\n            self.rootNode = TreeNode(self.data, featureIndices)\n            self.rootNode.c45Train()\n    \n        def classify(self, sample):\n            '''\n            Classify a sample based off of this trained tree.\n            '''\n    \n            return self.rootNode.classify(sample)\n\nThen we introduce the RandomForest algorithm\n\n\n    from C45Tree import C45Tree\n\n\n    class RandomForest(object):\n        \"\"\"A random forest object with the default of using a C45Tree \n        for each of the trees in the random forest. To train the forest\n        create an instance of it then call train on a TraingData object\"\"\"\n        def __init__(self, data, numberOfTrees=100):\n            '''\n            Initialize the random forest. \n            Each tree has a bag of the data associated with it.\n            '''\n            self.data = data    #The data that the trees will be trained on\n            self.numberOfTrees = numberOfTrees\n            self.forest = []\n    \n            for i in range(numberOfTrees):\n                bag = data.getBag()\n                self.forest.append(C45Tree(bag))\n    \n        def train(self):\n            '''\n            Train the random forest trees.\n            '''\n            for tree in self.forest:\n                tree.train()\n    \n        def classify(self, sample):\n            '''\n            Classify a data sample by polling the trees.\n            '''\n    \n            #Create an empty dictionary\n            votes = {}\n            #Tally the votes, for each tree classify the sample\n            for tree in self.forest:\n                label = tree.classify(sample)\n                if label in votes:\n                    votes[label] += 1\n                else:\n                    votes[label] = 1\n    \n            bestLabel = None\n            mostTimes = 0\n            #Find the label with the most votes\n            for key in votes:\n                if votes[key] > mostTimes:\n                    bestLabel = key\n                    mostTimes = votes[key]\n    \n            #Return the most popular label\n            return bestLabel      ","categories":["Computer Science"]},{"title":"Deep dive into KAGGLE","url":"/2019/04/16/kaggle数据集从入门到放弃/","content":"[click here to kaggle official website for this dataset](https://www.kaggle.com/ronitf/heart-disease-uci)\n\n## Context\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n\n## Content\n\n### Attribute Information: \n> 1. age \n> 2. sex \n> 3. chest pain type (4 values) \n> 4. resting blood pressure \n> 5. serum cholestoral in mg/dl \n> 6. fasting blood sugar > 120 mg/dl\n> 7. resting electrocardiographic results (values 0,1,2)\n> 8. maximum heart rate achieved \n> 9. exercise induced angina \n> 10. oldpeak = ST depression induced by exercise relative to rest \n> 11. the slope of the peak exercise ST segment \n> 12. number of major vessels (0-3) colored by flourosopy \n> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values. One file has been \"processed\", that one containing the Cleveland database. All four unprocessed files also exist in this directory.\n\nTo see Test Costs (donated by Peter Turney), please see the folder \"Costs\"\n\n# Introduction\nTo solve this problem, here we introduce jupyter notebook by python, implementing random forest to give us a brief scope of this dataset.\nFirstly, we import the packages needed.\n\n\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns #for plotting\n    from sklearn.ensemble import RandomForestClassifier #for the model\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.tree import export_graphviz #plot tree\n    from sklearn.metrics import roc_curve, auc #for model evaluation\n    from sklearn.metrics import classification_report #for model evaluation\n    from sklearn.metrics import confusion_matrix #for model evaluation\n    from sklearn.model_selection import train_test_split #for data splitting\n    import eli5 #for purmutation importance\n    from eli5.sklearn import PermutationImportance\n    import shap #for SHAP values\n    from pdpbox import pdp, info_plots #for partial plots\n    np.random.seed(123) #ensure reproducibility\n    pd.options.mode.chained_assignment = None  #hide any pandas warnings\n\n# The data\nNext, we load the data.\n\n    dt = pd.read_csv(\"../input/heart.csv\")\n\nIt's a clean, easy to understand set of data. However, the meaning of some of the column headers are not obvious. Here's what they mean,\n\n**age**: The person's age in years\n**sex**: The person's sex (1 = male, 0 = female)\n**cp**: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n**trestbps**: The person's resting blood pressure (mm Hg on admission to the hospital)\n**chol**: The person's cholesterol measurement in mg/dl\n**fbs**: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n**restecg**: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n**thalach**: The person's maximum heart rate achieved\n**exang**: Exercise induced angina (1 = yes; 0 = no)\n**oldpeak**: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n**slope**: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n**ca**: The number of major vessels (0-3)\n**thal**: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n**target**: Heart disease (0 = no, 1 = yes)\n\n**Diagnosis**: The diagnosis of heart disease is done on a combination of clinical signs and test results. The types of tests run will be chosen on the basis of what the physician thinks is going on 1, ranging from electrocardiograms and cardiac computerized tomography (CT) scans, to blood tests and exercise stress tests 2.\n\nLooking at information of heart disease risk factors led me to the following: high cholesterol, high blood pressure, diabetes, weight, family history and smoking 3. According to another source 4, the major factors that can't be changed are: increasing age, male gender and heredity. Note that thalassemia, one of the variables in this dataset, is heredity. Major factors that can be modified are: Smoking, high cholesterol, high blood pressure, physical inactivity, and being overweight and having diabetes. Other factors include stress, alcohol and poor diet/nutrition.\n\nI can see no reference to the 'number of major vessels', but given that the definition of heart disease is \"...what happens when your heart's blood supply is blocked or interrupted by a build-up of fatty substances in the coronary arteries\", it seems logical the more major vessels is a good thing, and therefore will reduce the probability of heart disease.\n\nGiven the above, I would hypothesis that, if the model has some predictive ability, we'll see these factors standing out as the most important.\n\nDeclare some of the columns to make the indication clear\n\n    dt.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']\n\n\n    dt['sex'][dt['sex'] == 0] = 'female'\n    dt['sex'][dt['sex'] == 1] = 'male'\n    \n    dt['chest_pain_type'][dt['chest_pain_type'] == 1] = 'typical angina'\n    dt['chest_pain_type'][dt['chest_pain_type'] == 2] = 'atypical angina'\n    dt['chest_pain_type'][dt['chest_pain_type'] == 3] = 'non-anginal pain'\n    dt['chest_pain_type'][dt['chest_pain_type'] == 4] = 'asymptomatic'\n    \n    dt['fasting_blood_sugar'][dt['fasting_blood_sugar'] == 0] = 'lower than 120mg/ml'\n    dt['fasting_blood_sugar'][dt['fasting_blood_sugar'] == 1] = 'greater than 120mg/ml'\n    \n    dt['rest_ecg'][dt['rest_ecg'] == 0] = 'normal'\n    dt['rest_ecg'][dt['rest_ecg'] == 1] = 'ST-T wave abnormality'\n    dt['rest_ecg'][dt['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n    \n    dt['exercise_induced_angina'][dt['exercise_induced_angina'] == 0] = 'no'\n    dt['exercise_induced_angina'][dt['exercise_induced_angina'] == 1] = 'yes'\n    \n    dt['st_slope'][dt['st_slope'] == 1] = 'upsloping'\n    dt['st_slope'][dt['st_slope'] == 2] = 'flat'\n    dt['st_slope'][dt['st_slope'] == 3] = 'downsloping'\n    \n    dt['thalassemia'][dt['thalassemia'] == 1] = 'normal'\n    dt['thalassemia'][dt['thalassemia'] == 2] = 'fixed defect'\n    dt['thalassemia'][dt['thalassemia'] == 3] = 'reversable defect'\n\n# The Model\nDepending on package sklearn, we split the data - test data 70%, training data 30%\n\n    X_train, X_test, y_train, y_test = train_test_split(dt.drop('target', 1), dt['target'], test_size = .2, random_state=10) #split the data\n\nUsing random forest methodology\n\n    model = RandomForestClassifier(max_depth=5)\n    model.fit(X_train, y_train)\n\nplot the result of random forest\n\n    estimator = model.estimators_[1]\n    feature_names = [i for i in X_train.columns]\n    \n    y_train_str = y_train.astype('str')\n    y_train_str[y_train_str == '0'] = 'no disease'\n    y_train_str[y_train_str == '1'] = 'disease'\n    y_train_str = y_train_str.values\n\ncode from https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c\n\n    export_graphviz(estimator, out_file='tree.dot', \n                    feature_names = feature_names,\n                    class_names = y_train_str,\n                    rounded = True, proportion = True, \n                    label='root',\n                    precision = 2, filled = True)\n    \n    from subprocess import call\n    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n    \n    from IPython.display import Image\n    Image(filename = 'tree.png')\n\nEvaluate the model\n\n    y_predict = model.predict(X_test)\n    y_pred_quant = model.predict_proba(X_test)[:, 1]\n    y_pred_bin = model.predict(X_test)\n\nFit with Cnfusion model\n\n    confusion_matrix = confusion_matrix(y_test, y_pred_bin)\n\nHere we propose the sensitvity and specificity\n$$sensitivity = \\frac{TF}{TP+FN}$$\n$$specificity = \\frac{TN}{TN+FP}$$\n\n    total=sum(sum(confusion_matrix))\n    \n    sensitivity = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[1,0])\n    print('Sensitivity : ', sensitivity )\n    \n    specificity = confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[0,1])\n    print('Specificity : ', specificity)\n\nNext, we check ROC curve.\n\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\n    \n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for diabetes classifier')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.grid(True)\n\nAnother common metric is the Area Under the Curve, or AUC.\n\n    auc(fpr, tpr)\n\n# Explanation\nHere we implement the shap value to explain our model.\n\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X_test)\n    \n    shap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")\n\n[for more](https://www.kaggle.com/tentotheminus9/what-causes-heart-disease-explaining-the-model)","categories":["Kaggle"]}]