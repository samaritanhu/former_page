[{"title":"Deep-ADMM-Net","url":"/2019/09/23/Deep-ADMM-Net/","content":"## Introduction of Deep ADMM-Net\nI am currently working in Math AI lab, and Deep ADMM Net is one of our main targets recently depends on papers:\n- Yang, Y., Sun, J., Li, H., & Xu, Z. (2016). Deep ADMM-Net for compressive sensing MRI\n- Yang, Y., Sun, J., Li, H., & Xu, Z. (2017). ADMM-Net: A Deep Learning Approach for Compressive Sensing MRI\n\n## A deep dive into Deep ADMM-Net\n### Deep ADMM-Net for FastMRI\n#### Compressive Sensing MRI Model and ADMM Algorithm\n*General CS-MRI Model problem*\n$$\n\\hat{x}=\\underset{x}{\\arg \\min }\\left\\{\\frac{1}{2}\\|A x-y\\|_{2}^{2}+\\sum_{l=1}^{L} \\lambda_{l} g\\left(D_{l} x\\right)\\right\\}\n$$\nwhere $A=P F \\in \\mathbb{R}^{N^{\\prime} \\times N}$ is a  measurement matrix, $P \\in \\mathbb{R}^{N^{\\prime} \\times N}$ is a under-sampling matrix and $F$ is a Fourier transform.\n\n*ADMM solver*\nBy introducing $ z=\\left\\{z_{1}, z_{2}, \\cdots, z_{L}\\right\\} $, \nthe problem equals to \n$$\n\\min _{x, z} \\frac{1}{2}\\|A x-y\\|_{2}^{2}+\\sum_{l=1}^{L} \\lambda_{l} g\\left(z_{l}\\right) \\quad \\text { s.t. } z_{l}=D_{l} x, \\quad \\forall l \\in[1,2, \\cdots, L]\n$$\n\nand its augmented Lagrangian function is\n$$\n\\mathfrak{L}_{\\rho}(x, z, \\alpha)=\\frac{1}{2}\\|A x-y\\|_{2}^{2}+\\sum_{l=1}^{L} \\lambda_{l} g\\left(z_{l}\\right)-\\sum_{l=1}^{L}\\left\\langle\\alpha_{l}, z_{l}-D_{l} x\\right\\rangle+\\sum_{l=1}^{L} \\frac{\\rho_{l}}{2}\\left\\|z_{l}-D_{l} x\\right\\|_{2}^{2}\n$$\n\nwhere $α = {α_l}$ are Lagrangian multipliers and $ρ = {ρ_l}$ are penalty parameters. ADMM alternatively optimizes ${x, z, α}$ by solving the following three subproblems:\n\n$$\n\\left\\{\\begin{array}{l}{x^{(n+1)}=\\arg \\min _{x} \\frac{1}{2}\\|A x-y\\|_{2}^{2}-\\sum_{l=1}^{L}\\left\\langle\\alpha_{l}^{(n)}, z_{l}^{(n)}-D_{l} x\\right\\rangle+\\sum_{l=1}^{L} \\frac{\\rho_{l}}{2}\\left\\|z_{l}^{(n)}-D_{l} x\\right\\|_{2}^{2}} \\\\ {z^{(n+1)}=\\arg \\min _{z} \\sum_{l=1}^{L} \\lambda_{l} g\\left(z_{l}\\right)-\\sum_{l=1}^{L}\\left\\langle\\alpha_{l}^{(n)}, z_{l}-D_{l} x^{(n+1)}\\right\\rangle+\\sum_{l=1}^{L} \\frac{\\rho_{l}}{2}\\left\\|z_{l}-D_{l} x^{(n+1)}\\right\\|_{2}^{2}} \\\\ {\\alpha^{(n+1)}=\\underset{\\alpha}{\\arg \\min } \\sum_{l=1}^{L}\\left\\langle\\alpha_{l}, D_{l} x^{(n+1)}-z_{l}^{(n+1)}\\right\\rangle}\\end{array}\\right.\n$$\n\n#### Deep ADMM-Net\n- Reconstruction layer\n- Convolution layer\n- Nonlinear Transform layer\n- Multiplier update layer\n\n### Network Training\nGiven pairs of training data, the loss between the network output and ground truth is deﬁned as\n$$\nE(\\Theta)=\\frac{1}{|\\Gamma|} \\sum_{\\left(y, x^{g t}\\right) \\in \\Gamma} \\frac{\\sqrt{\\left\\|\\hat{x}(y, \\Theta)-x^{g t}\\right\\|_{2}^{2}}}{\\sqrt{\\left\\|x^{g t}\\right\\|_{2}^{2}}}\n$$\n\n#### Initialization\n$$\n\\underset{x}{\\arg \\min }\\left\\{\\frac{1}{2}\\|A x-y\\|_{2}^{2}+\\lambda \\sum_{l=1}^{L}\\left\\|D_{l} x\\right\\|_{1}\\right\\}\n$$\n\n#### Gradient Computation by Backpropagation over Data Flow Graph\n- Multiplier update layer\n- Nonlinear Transform layer\n- Convolution layer\n- Reconstruction layer\n","categories":["Computer Vision"]},{"title":"Discovery towards Web Scrawler","url":"/2019/07/14/webscrawler/","content":"\n### Web Scrawler is dangerous\n\nFirst and foremost, do not touch web scrawler if you are 100% sure that you wanna do this.\n\n### Beginning\n\nThe target website is enlightent, a third-party data website which have data of my company and component. \n\nWe need to do some background research first. The problems I encountered are listed:\n\n- Need to log in with WeChat account by QR code\n- Simulate click (by package selenium)\n- It's a dynamic website, you need to wait for its information loaded (by package time)\n- Write into MySQL (by package Pymysql)\n\n### Process\n\n##### First problem cannot be solve due to the security of WeChat.\n\n##### Second problem\n\nStep 1: Find the pattern in html. Using chrome, just ctrl+u or ctrl+shift+i. It needs your patience to find the thing you want. If you mistaken the pattern, you cannot get the information you want\n\nStep 2: Choose the function: by_path or by_class. The tricky point is that if there is only one class, it's okay to use by_class, if there are more than two classes, selenium would choose the first class as your output. As a result, I choose by_path\n\nStep 3: Install chrome driver according to your chrome version. Be sure to download into /anaconda3/lib/site-packages.\n\n```python\n# 选择日榜\ntime.sleep(5)\ndriver.find_elements_by_id(\"rank-date-btn\")[0].click()\n# 选择换年份\ntime.sleep(1)\ndriver.find_element_by_class_name(\"datepicker-switch\").click()\n# 选择目标月份\ntime.sleep(1)\n# driver.find_element_by_class_name(\"month\").click()\nmonth_url = \"/html/body/div[2]/div[3]/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[1]/div/div/div/div[2]/table/tbody/tr/td/span[%s]\" % (l+1)\ndriver.find_element_by_xpath(month_url).click()\n# 选择目标日期\ntime.sleep(1)\n# day = driver.find_element_by_class_name(\"day\").click()\nxpath = \"/html/body/div[2]/div[3]/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[1]/div/div/div/div/table/tbody/tr[%d]/td[%d]\" % ((start_date) // 7 + 1, (start_date) % 7 + 1)\ndriver.find_element_by_xpath(xpath).click()\n# 点击确定\ntime.sleep(1)\ndriver.find_element_by_id(\"choose-rank\").click()\ntime.sleep(1)\n\n```\n\n##### Third Problem: Data Processing\n\n```python\nalbum_separately = string_list[j][string_list[j].find('data-name='):string_list[j].find('data-channeltype=\"tv\"')]\nif j != 0: \n    album.append(album_separately.replace('data-name=','').replace('\"',''))\npercentage_separately = string_list[j][string_list[j].find('<td class=\"sort rank-playTimesPredicted active\" style=\"\"><span>'):string_list[j].find('</span></td><td class=\"rank-playTimes\" style=\"\">')]\npercentage.append(percentage_separately.replace('<td class=\"sort rank-playTimesPredicted active\" style=\"\"><span>',''))\nclick_separately = string_list[j][string_list[j].find('</td><td class=\"rank-playTimes\" style=\"\"><span>'):string_list[j].find('</span><span class=\"star-playtimes\">')]\nif len(click_separately) >= 10:\n    click_separately = string_list[j][string_list[j].find('</td><td class=\"rank-playTimes\" style=\"\"><span>'):string_list[j].find('</span></td><td class=\"rank-average m-change\" style=\"\">')]\nclick.append(click_separately.replace('</td><td class=\"rank-playTimes\" style=\"\"><span>','').replace('</span><span class=\"star-playtimes\">',''))\n```\n\n##### Fourth problem: Log into your MySQL and use python like MySQL!\n\n```python\ndb = DB('your database')\ndb.insert(dataset)\n```\n\n \n\n### Take Care! Be sure to add time.sleep() when you do it!","categories":["Web Scrawler"]},{"title":"Random Forest Classification from the bottom layer","url":"/2019/05/05/RandomForest/","content":"import packages we need\n\n    import random\n    import numpy as np\n    import sys\n\nThe feature list is an array of the possible feature indicies to use. This prevents splitting on the same feature multiple times and runs a rough C45 algorithm.\n\nIn pseudocode, the general algorithm for building decision trees is:\n\n1.Check for base cases\n2.For each attribute a\n3.Find the normalized information gain ratio from splitting on a\n4.Let a_best be the attribute with the highest normalized information gain\n5.Create a decision node that splits on a_best\n6.Recur on the sublists obtained by splitting on a_best, and add those nodes as children of node\n\n    class TreeNode:\n        \"\"\"docstring for treeNode\"\"\"\n        def __init__(self, dataSet, featureList, parent=None):\n            self.featureNumber = None  #This is the trained index of the feature to split on\n            self.featureList = featureList \n            self.threshold = None     #This is the trained threshold of the feature to split on\n            self.leftChild = None\n            self.rightChild = None\n            self.dataSet = dataSet\n            self.parent = parent\n    \n        def c45Train(self):\n            #Base Cases:\n            \n            #All instances in dataSet are the same\n            if(self.dataSet.isPure()):\n                #gets the label of the first data instance and makes a leaf node\n                #classifying it. \n                label = self.dataSet.getData()[0].getLabel()\n                leaf = LeafNode(label)\n                return leaf\n            #If there are no more features in the feature list\n            if len(self.featureList) == 0:\n                labels = self.dataSet.getLabelStatistics()\n                bestLabel = None\n                mostTimes = 0\n    \n                for key in labels:\n                    if labels[key] > mostTimes:\n                        bestLabel = key\n                        mostTimes = labels[key]\n                #Make the leaf node with the best label\n                leaf = LeafNode(bestLabel)\n                return leaf\n    \n            #Check all of the features for the split with the most \n            #information gain. Use that split.\n            currentEntropy = self.dataSet.getEntropy()\n            currentLength = self.dataSet.getLength()\n            infoGain = -1 * sys.maxsize\n            bestFeature = 0\n            bestLeft = None\n            bestRight = None\n            bestThreshold = 0\n    \n            #Feature Bagging, Random subspace\n            num = int(np.ceil(np.sqrt(len(self.featureList))))\n            featureSubset = random.sample(self.featureList, num)\n    \n            for featureIndex in featureSubset:\n                #Calculate the threshold to use for that feature\n                threshold = self.dataSet.betterThreshold(featureIndex)\n    \n                (leftSet, rightSet) = self.dataSet.splitOn(featureIndex, threshold)\n    \n                leftEntropy = leftSet.getEntropy()\n                rightEntropy = rightSet.getEntropy()\n                #Weighted entropy for this split\n                newEntropy = (leftSet.getLength() / currentLength) * leftEntropy + (rightSet.getLength() / currentLength) * rightEntropy\n                #Calculate the gain for this test\n                newIG = currentEntropy - newEntropy\n    \n                if(newIG > infoGain):\n                    #Update the best stuff\n                    infoGain = newIG\n                    bestLeft = leftSet\n                    bestRight = rightSet\n                    bestFeature = featureIndex\n                    bestThreshold = threshold\n    \n            newFeatureList = list(self.featureList)\n            newFeatureList.remove(bestFeature)\n    \n            #Another base case, if there are no good features to split on\n            if bestLeft.getLength() == 0 or bestRight.getLength() == 0:\n                labels = self.dataSet.getLabelStatistics()\n                bestLabel = None\n                mostTimes = 0\n    \n                for key in labels:\n                    if labels[key] > mostTimes:\n                        bestLabel = key\n                        mostTimes = labels[key]\n                #Make the leaf node with the best label\n                leaf = LeafNode(bestLabel)\n                return leaf\n    \n            self.threshold = bestThreshold\n            self.featureNumber = bestFeature\n    \n            leftChild = TreeNode(bestLeft, newFeatureList, self)\n            rightChild = TreeNode(bestRight, newFeatureList, self)\n    \n            self.leftChild = leftChild.c45Train()\n            self.rightChild = rightChild.c45Train()\n    \n            return self\n            \n        def __str__(self):\n            return str(self.featureList)\n    \n        def __repr__(self):\n            return self.__str__()\n                    \n        def classify(self, sample):\n            '''\n            Recursivly traverse the tree to classify the sample that is passed in. \n            '''\n    \n            value = sample.getFeatures()[self.featureNumber]\n    \n            if(value < self.threshold):\n                #Continue down the left child    \n                return self.leftChild.classify(sample)\n    \n            else:\n                #continue down the right child\n                return self.rightChild.classify(sample)\n\n\n    class LeafNode:\n        '''\n        A leaf node is a node that just has a classification \n        and is used to cap off a tree.\n        '''\n    \n        def __init__(self, classification):\n            self.classification = classification\n    \n        def classify(self, sample):\n            #A leaf node simply is a classification, return that\n            #This is the base case of the classify recursive function for TreeNodes\n            return self.classification\n\n\n    class C45Tree:\n        '''\n        A tree contains a root node and from here\n        does the training and classification. Tree objects also\n        are responsible for having the data that they use to train.\n        '''\n    \n        def __init__(self, data):\n            self.rootNode = None\n            self.data = data\n    \n        def train(self):\n            '''\n            Trains a decision tree classifier on data set passed in. \n            The data set should contain a good mix of each class to be\n            classified.\n            '''\n            length = self.data.getFeatureLength()\n            featureIndices = range(length)\n            self.rootNode = TreeNode(self.data, featureIndices)\n            self.rootNode.c45Train()\n    \n        def classify(self, sample):\n            '''\n            Classify a sample based off of this trained tree.\n            '''\n    \n            return self.rootNode.classify(sample)\n\nThen we introduce the RandomForest algorithm\n\n\n    from C45Tree import C45Tree\n\n\n    class RandomForest(object):\n        \"\"\"A random forest object with the default of using a C45Tree \n        for each of the trees in the random forest. To train the forest\n        create an instance of it then call train on a TraingData object\"\"\"\n        def __init__(self, data, numberOfTrees=100):\n            '''\n            Initialize the random forest. \n            Each tree has a bag of the data associated with it.\n            '''\n            self.data = data    #The data that the trees will be trained on\n            self.numberOfTrees = numberOfTrees\n            self.forest = []\n    \n            for i in range(numberOfTrees):\n                bag = data.getBag()\n                self.forest.append(C45Tree(bag))\n    \n        def train(self):\n            '''\n            Train the random forest trees.\n            '''\n            for tree in self.forest:\n                tree.train()\n    \n        def classify(self, sample):\n            '''\n            Classify a data sample by polling the trees.\n            '''\n    \n            #Create an empty dictionary\n            votes = {}\n            #Tally the votes, for each tree classify the sample\n            for tree in self.forest:\n                label = tree.classify(sample)\n                if label in votes:\n                    votes[label] += 1\n                else:\n                    votes[label] = 1\n    \n            bestLabel = None\n            mostTimes = 0\n            #Find the label with the most votes\n            for key in votes:\n                if votes[key] > mostTimes:\n                    bestLabel = key\n                    mostTimes = votes[key]\n    \n            #Return the most popular label\n            return bestLabel      ","categories":["Kaggle"]},{"title":"国际金融半期复习","url":"/2019/04/19/management/","content":"因为下周五就要国际金融的小测了，特此做一下期中考试前的突击=。=\n【结果是我考了90/100，最高分95】\n\n**专业**：上海交通大学安泰经济与管理学院 金融学\n**课程名**：国际金融\n**教材**：International Financial Management by Cheol Eun\n\n**课程安排**：\nLecture1 汇率制度与国际货币体系, 教材第2章\nLe2  国际收支与国际资本流动，教材第3章\nLe3  国家投资分析\nLe4  国际固定收益市场，教材第5,11,12章\nLe5 国际股票市场，教材第13,15章 \nLe6 汇率平价公式，教材第6章\nLe7 经济政策及汇率预测（Krugman第12版，及教材第6章）\nLe8 Currency Derivatives ，教材第7章\nLe9 Direct Foreign Investment ：定性与定量分析，阅读教材第16，17，18章\nLe10 国际证券组合理论，教材第15章[高盛Country Analysis]\nLe11 企业的外汇交易风险 ，教材第8章\nLe12 企业的外汇经营与折算风险 ，教材第9-10章\nLe13 利率互换与货币互换，教材第14章\n\n**重要网站与APPs**\n#### 国际股票市场投资\n[新浪美股或者新浪财经客户端](http://i.finance.sina.com.cn/zixuan,usstock)\n登陆账号：fi401@126.com, 111119 【含货币，债券、股票ETF】\n[全球股价与组合投资信息](finance.yahoo.com),雅虎财经可下载excel格式数据\n[国际股票ETF： 通过下载有港股信息的客户端，比如同花顺、通达信授权的券商（申万证券）](http://www.sywg.com/sywg/portal/portal_softdown.html)\n雪球财经\n#### 货币及外汇市场投资\n[GMAC嘉盛集团保证金杠杆交易](www.forex.com)\n[下载外汇数据](http://finance.yahoo.com/currency-investing/asia-pacific) 或者[点击](http://fx.sauder.ubc.ca/data.html)\n同花顺、通达信授权的券商股票分析软件的拓展市场行情也加入了外汇数据\n#### 国际债券市场投资\n[Ishares bond](bing.com)\n[全球收益率曲线](http://www.worldgovernmentbonds.com/)\nYahoo Bond Screener\n[全球债券](http://finance.yahoo.com/bonds) 交易所上市的债券投资组合 Bing 搜 Bond ETF\n#### 全球大宗商品市场、衍生产品\n[期货数据(黄金石油大宗商品) 通达信](http://finance.sina.com.cn/futuremarket/)、[同花顺](http://www.sge.sh/publish/sge/xqzx/xqyb/index.html)\n[期货（外汇）](http://www.barchart.com/futures/Currencies)； \n#### 分国家投资与行业投资\n各种国家投资ETF，搜索 ishares； 分行业投资SPDRs.com推出的 ETF\n[全球行业分析](trefis.com)  CGTN 非洲时间 &America Now\n#### 国家投资基本信息\n[各国经济与金融：国际金融杂志数据](http://www.gfmag.com/global-data/country-data/)\n[各国社会统计](https://www.cia.gov/library/publications/the-world-factbook/rankorder/2187rank.html)\n国际经济热点网站Podcast Itunes Rss \n订阅  bbc world service/business daily； business matters, world business report, tech tent\n边走边听国际金融与经济新闻\nIheartRadio应用上收听 bloomberg 华尔街新闻；FM收音机应用有：R Sputnik俄罗斯；SAFM 南非； \n\n\n## Lecture 1: 汇率制度与国际货币体系\n### 8种汇率制度简介\n**国际货币体系三步曲**：\n1876~1913, 金本位。银行券自由兑换黄金，各国汇率取决于货币的黄金相对含量\n如果英镑、美元货币的黄金含量分别为1和0.5盎司,则1英镑的汇率为2美元\n1944 Bretton Woods Agreement （布雷顿协议），双挂钩（每盎司黄金=35美元）\n1973年牙买加协议。西方国家主要货币汇率浮动工资与金融风险传播的开始，并推动外汇交易市场发展\n**汇率制度分类**：IMF，1999. 按浮动程度由小到大排列: \n无独立法定货币（美元化）：本国货币被美元替代，或无法定货币， 厄瓜多尔\n货币局制度：本国货币与某一主要货币保持在一固定比率， HK\n固定（传统）钉住制：与某一浮动货币保持固定比率，但政府可以对这一比率宣布调节\n水平带内的钉住\n爬行钉住\n爬行带内的汇率安排\n管理浮动：无限制+干预\n单独浮动：汇率由市场因素决定\n\n### 政府直接干预外汇的两种形式\n**政策的四大目标**：GDP增长，就业充分，物价稳定，国际收支平衡\n**两大政策**：货币政策（三大法宝：利率、准备率、公开市场业务），财政政策：税收、政府购买、转移支付\n**政府干预动机**：外汇干预应围绕经济政策的四大目标，短期目标（减少波动，设定上下限，减少意外事件冲击）\n**干预方法**：\n直接干预（消极干预，干预外汇时不调整本币投放；积极干预，抛售本币时，还抛售一些债券，以保持货币供给平衡）\n间接干预（通过调节影响汇率的变量，来影响汇率，如利率；外汇管制）\n*宏观经济三元悖论：固定汇率、资本开放与独立货币政策，只能取其二*\n### 人民币汇率制度演变\n改革开放前\n1,建国初期．由于国内物价猛涨而国外物价稳定甚至下跌，1950年3月调至42 000旧人民币／美元，,1951年5月调至22 380旧人民币／美元         \n2,社会主义建设初期至1971年11月，人民币汇率在近16年时间里保持2.4618．\n3,西方货币实行浮动汇率后，人民币（1973—1978) 钉住一篮子货币\n改革开放后\n爬行带内汇率安排\n### 人民币汇率对中国经济影响\n**正面影响：**\n1,国外产品价格下降，提高国人的生活水平和产业水平\n2.使用进口原材料比率较高的厂商的生产成本会下降\n3.企业的国外投资能力将增强\n4.人民币升值，使已在华投资的外资企业利润增加.\n5.在非制造业方面，人民币升值有利于人才出国学习.\n6.未偿还外债还本付息所需本币的数量相应减少.\n7.合理估值中国资产:房地产价格高估，卖出1间房送子女国外读书; 大量民企转投海外地产、酒店\n8.有利于中国GDP在国际经济中地位\n**负面影响：**\n1,对出口存在干扰\n2,升值进一步预期，可能导致投机盛行。由于以美元表示的国民财富迅速增加，股市和房地产达到高潮\n3,升值后，可能使国内投资环境恶化，新增的海外投资则会减少，因为这种投资变得相对昂贵\n4,升值后，目前出口的产品大部分是技术含量较低的劳动密集型产品，出口受阻会增加就业压力\n\n### 汇率与人民币国际化\n汇率市场化将推动国际化\n国际化的三个阶段：资本项目可自由兑换，成为结算货币，成为储备货币\n\n## Lecture 2: 国际收支平衡表\n**prerequisite**\n由支出法核算得到的GDP与收入法得到的社会总收入Y相等\nGDP ≡ C+I+G+X-M，支出法核算,得到社会总需求AD\nC(Y) : consumption spending,取决于国民收入Y\nI : capital Investment spending\nG :  Government spending\nX : eXports of goods and services\nM :  iMports of goods and services\nY ≡ C+S+T，收入法核算，形成社会生产总供给AS.\nS : Saving\nT : Tax\n市场均衡时Market equilibrium: GDP≡C(Y)+I+G+X-M=Y\n次字Deficit: 对外赤字X-M<0; 对内赤字T-G<0\n\n### BOP\n**国际收支平衡表（Balance of Payments）的定义**：一定时期本国与外国居民间的所有交易\n#### 3大国际收支项目：经常、资本与金融、平衡项目\n导致资金流入的一般记贷方(credit)，用正号表示,例如：出口，融资;反之，进口或者对外投资时资金流出，记借方(debit)，用负号表示\n#### 中美BOP分析\n逆差集中在\n三级账户a：大宗商品\\高科技产品\n三级账户b：服务业\n二级账户B：投资产生的收益\n\n\n### 影响经常账户、FDI的因素\nInflation, National Income, Government Restrictions and exchange rates(3 different opinions)\n#### 马歇尔-勒纳条件Marshall-lerner condition、J-curve effect\n$\\displaystyle \\eta_{Xe}+|\\eta_{Me}|>1$ 进出口需求对价格的弹性大于1\n### 3个重要BOP项目与3大国际机构间关系\nChanges in Restrictions, Potential Economic Growth, Tax Rates, Exchange Rates\n\n## Lecture 3: 国家投资分析\n### 国家投资动机与投资方法分类\n国家金融投资(Portfolio Investment)分析两种哲学思路：主动投资分析与被动投资。\n国家直接投资(Foreign Direct Investment)以主动分析为主\n国家被动投资\n1.理论基石：有效市场假说(Eugen Fama). 市场难以战胜,价格就是正确的。\n2.尤其在海外投资时，超额收益难以匹配信息成本、语言障碍、交易时间等带来的高额成本\n3.指数基金采用买入并持有策略，交易成本最低，收取客户的费用也最低\n国家主动投资：从上到下进行主动分析，从国家到行业，再到公司选择。\n\n### 国家分析的道格拉斯生产函数与Solow平衡增长理论\n在道格拉斯函数$Y=AKaL(1-a)$中，假设人口(劳动力)$L$增长速度$n$, 技术$A$增长速度$g$, 资本的折旧速度$\\delta$。当经济增长处于稳定状态时，人均资本$k(t)$变动满足下面公式\n$$k'(t)=sf(k(t))-(n+g+\\delta)k(t)$$\n投资比例$s^*$人均产出$f(k(t))$表示投资，人均投资高于$(n+g+\\delta)k(t)$时，社会资本变动$k(t)$为正。\n重要观点:\nGDP增速为$n+g$, 劳动力增速$n$扮演重要角色：放开生育带来GDP增长\n\n### 国家宏观分析模板：以印度为例\n根据道格拉斯生产函数$Y=AKaL(1-a)$, 经济Y增长取决于资本K、劳动力L的增长和技术A进步\n关于K和L.生产函数中，“金砖四国”“Brics have K and L resources respectively.印度的人口与巴西的资源优势\n\n### 国家卖方分析法(评估机构概览)\n宏观经济两种政策： 货币与财政\n3驾马车：GDP增长率，经济增长动因，进出口\n\n## Lecture 4: 国家固定收益市场\n### 外汇市场\n#### 外汇市场参与投机与对冲的效用分析;汇率换算 \n国际金融市场划分：固定收益、股权、衍生产品、另类投资\n国际固定收益分类：外汇市场，货币与信贷市场，债券市场\n投资国际固定收益市场动机：外币升值收益、国际分散化、对冲汇率风险\n\n#### 外汇报价\nThe following attributes of banks are important\ncompetitiveness of quote（价差小）\nspeed of execution交易执行速度\nTrading time(交行周六5：00，中行2：00）\nadvice about current market conditions提供外汇信息\nforecasting advice预测市场\n### 欧洲货币市场与信贷市场\n#### Libor与浮动利率贷款\n欧洲美元：境外美元.  U.S. dollar deposits placed in banks in Europe and other continents are called Eurodollars.\n美元为规避美国银行业管制，将美元存放境外，\n欧洲货币：在发行国以外市场交易的货币。当前货币在发行国外流通统称欧洲货币\n欧洲银行：专门经营境外货币的银行也称欧洲银行，并不一定在欧洲\n\n### 国际债券市场\n#### 外国债券与欧洲债券\n外国债券：债券发行人为外国人，例如中国在美国发行美元债券\n欧洲债券：债券面值为外国货币，例如中国在欧洲发行美元债券\n\n#### 债券的到期收益率\n#### 隐含汇率\n\n## Lecture 5: 国际股票市场：基本分析\n### 全球股票市场\n#### 投资动机与概览\n投资机会集:全球21世纪00-09投资市场概览\n风险：股票间的相关系数低，可降低投资风险\nSolnik研究发现：1）分散投资降低风险；2）股票分散到25个左右，难以继续分散；3）国际投资可进一步降低风险\n\n#### 全球市场相关性的主要实证结果\n分行业分国家\n### 美国股票市场\n#### 多层次融资体系概况与ADR分类\n公司从种子到初创（私募股权）、迅速成长(Nasdaq)、大公司(NYSE)、衰退濒临死亡(OTCBB)都有交易市场\n外国公司在美国可直接上市，或者托管在母国，但在美国发行American Deposit Receipt，花旗担任了阿里巴巴的Depository Bank\n美国聚集全球最大的跨国公司，反应全球经济的基本面，特别是各国的最大互联网公司聚集美股，反应了新经济前景\n\n### 国家基本分析与股票分析的联系与区别\n### 相对与绝对股票估值模型及汇率对估值定价的影响\n#### 以海外上市的中国互联网企业估值为例\n","tags":["Finance"],"categories":["Finance"]},{"title":"kaggle数据集从入门到放弃","url":"/2019/04/16/kaggle数据集从入门到放弃/","content":"[click here to kaggle official website for this dataset](https://www.kaggle.com/ronitf/heart-disease-uci)\n\n## Context\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n\n## Content\n\n### Attribute Information: \n> 1. age \n> 2. sex \n> 3. chest pain type (4 values) \n> 4. resting blood pressure \n> 5. serum cholestoral in mg/dl \n> 6. fasting blood sugar > 120 mg/dl\n> 7. resting electrocardiographic results (values 0,1,2)\n> 8. maximum heart rate achieved \n> 9. exercise induced angina \n> 10. oldpeak = ST depression induced by exercise relative to rest \n> 11. the slope of the peak exercise ST segment \n> 12. number of major vessels (0-3) colored by flourosopy \n> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values. One file has been \"processed\", that one containing the Cleveland database. All four unprocessed files also exist in this directory.\n\nTo see Test Costs (donated by Peter Turney), please see the folder \"Costs\"\n\n# Introduction\nTo solve this problem, here we introduce jupyter notebook by python, implementing random forest to give us a brief scope of this dataset.\nFirstly, we import the packages needed.\n\n\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns #for plotting\n    from sklearn.ensemble import RandomForestClassifier #for the model\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.tree import export_graphviz #plot tree\n    from sklearn.metrics import roc_curve, auc #for model evaluation\n    from sklearn.metrics import classification_report #for model evaluation\n    from sklearn.metrics import confusion_matrix #for model evaluation\n    from sklearn.model_selection import train_test_split #for data splitting\n    import eli5 #for purmutation importance\n    from eli5.sklearn import PermutationImportance\n    import shap #for SHAP values\n    from pdpbox import pdp, info_plots #for partial plots\n    np.random.seed(123) #ensure reproducibility\n    pd.options.mode.chained_assignment = None  #hide any pandas warnings\n\n# The data\nNext, we load the data.\n\n    dt = pd.read_csv(\"../input/heart.csv\")\n\nIt's a clean, easy to understand set of data. However, the meaning of some of the column headers are not obvious. Here's what they mean,\n\n**age**: The person's age in years\n**sex**: The person's sex (1 = male, 0 = female)\n**cp**: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n**trestbps**: The person's resting blood pressure (mm Hg on admission to the hospital)\n**chol**: The person's cholesterol measurement in mg/dl\n**fbs**: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n**restecg**: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n**thalach**: The person's maximum heart rate achieved\n**exang**: Exercise induced angina (1 = yes; 0 = no)\n**oldpeak**: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n**slope**: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n**ca**: The number of major vessels (0-3)\n**thal**: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n**target**: Heart disease (0 = no, 1 = yes)\n\n**Diagnosis**: The diagnosis of heart disease is done on a combination of clinical signs and test results. The types of tests run will be chosen on the basis of what the physician thinks is going on 1, ranging from electrocardiograms and cardiac computerized tomography (CT) scans, to blood tests and exercise stress tests 2.\n\nLooking at information of heart disease risk factors led me to the following: high cholesterol, high blood pressure, diabetes, weight, family history and smoking 3. According to another source 4, the major factors that can't be changed are: increasing age, male gender and heredity. Note that thalassemia, one of the variables in this dataset, is heredity. Major factors that can be modified are: Smoking, high cholesterol, high blood pressure, physical inactivity, and being overweight and having diabetes. Other factors include stress, alcohol and poor diet/nutrition.\n\nI can see no reference to the 'number of major vessels', but given that the definition of heart disease is \"...what happens when your heart's blood supply is blocked or interrupted by a build-up of fatty substances in the coronary arteries\", it seems logical the more major vessels is a good thing, and therefore will reduce the probability of heart disease.\n\nGiven the above, I would hypothesis that, if the model has some predictive ability, we'll see these factors standing out as the most important.\n\nDeclare some of the columns to make the indication clear\n\n    dt.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']\n\n\n    dt['sex'][dt['sex'] == 0] = 'female'\n    dt['sex'][dt['sex'] == 1] = 'male'\n    \n    dt['chest_pain_type'][dt['chest_pain_type'] == 1] = 'typical angina'\n    dt['chest_pain_type'][dt['chest_pain_type'] == 2] = 'atypical angina'\n    dt['chest_pain_type'][dt['chest_pain_type'] == 3] = 'non-anginal pain'\n    dt['chest_pain_type'][dt['chest_pain_type'] == 4] = 'asymptomatic'\n    \n    dt['fasting_blood_sugar'][dt['fasting_blood_sugar'] == 0] = 'lower than 120mg/ml'\n    dt['fasting_blood_sugar'][dt['fasting_blood_sugar'] == 1] = 'greater than 120mg/ml'\n    \n    dt['rest_ecg'][dt['rest_ecg'] == 0] = 'normal'\n    dt['rest_ecg'][dt['rest_ecg'] == 1] = 'ST-T wave abnormality'\n    dt['rest_ecg'][dt['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n    \n    dt['exercise_induced_angina'][dt['exercise_induced_angina'] == 0] = 'no'\n    dt['exercise_induced_angina'][dt['exercise_induced_angina'] == 1] = 'yes'\n    \n    dt['st_slope'][dt['st_slope'] == 1] = 'upsloping'\n    dt['st_slope'][dt['st_slope'] == 2] = 'flat'\n    dt['st_slope'][dt['st_slope'] == 3] = 'downsloping'\n    \n    dt['thalassemia'][dt['thalassemia'] == 1] = 'normal'\n    dt['thalassemia'][dt['thalassemia'] == 2] = 'fixed defect'\n    dt['thalassemia'][dt['thalassemia'] == 3] = 'reversable defect'\n\n# The Model\nDepending on package sklearn, we split the data - test data 70%, training data 30%\n\n    X_train, X_test, y_train, y_test = train_test_split(dt.drop('target', 1), dt['target'], test_size = .2, random_state=10) #split the data\n\nUsing random forest methodology\n\n    model = RandomForestClassifier(max_depth=5)\n    model.fit(X_train, y_train)\n\nplot the result of random forest\n\n    estimator = model.estimators_[1]\n    feature_names = [i for i in X_train.columns]\n    \n    y_train_str = y_train.astype('str')\n    y_train_str[y_train_str == '0'] = 'no disease'\n    y_train_str[y_train_str == '1'] = 'disease'\n    y_train_str = y_train_str.values\n\ncode from https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c\n\n    export_graphviz(estimator, out_file='tree.dot', \n                    feature_names = feature_names,\n                    class_names = y_train_str,\n                    rounded = True, proportion = True, \n                    label='root',\n                    precision = 2, filled = True)\n    \n    from subprocess import call\n    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n    \n    from IPython.display import Image\n    Image(filename = 'tree.png')\n\nEvaluate the model\n\n    y_predict = model.predict(X_test)\n    y_pred_quant = model.predict_proba(X_test)[:, 1]\n    y_pred_bin = model.predict(X_test)\n\nFit with Cnfusion model\n\n    confusion_matrix = confusion_matrix(y_test, y_pred_bin)\n\nHere we propose the sensitvity and specificity\n$$sensitivity = \\frac{TF}{TP+FN}$$\n$$specificity = \\frac{TN}{TN+FP}$$\n\n    total=sum(sum(confusion_matrix))\n    \n    sensitivity = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[1,0])\n    print('Sensitivity : ', sensitivity )\n    \n    specificity = confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[0,1])\n    print('Specificity : ', specificity)\n\nNext, we check ROC curve.\n\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\n    \n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for diabetes classifier')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.grid(True)\n\nAnother common metric is the Area Under the Curve, or AUC.\n\n    auc(fpr, tpr)\n\n# Explanation\nHere we implement the shap value to explain our model.\n\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X_test)\n    \n    shap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")\n\n[for more](https://www.kaggle.com/tentotheminus9/what-causes-heart-disease-explaining-the-model)","categories":["Kaggle"]}]