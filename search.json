[{"title":"Stanford CS224n Natural Language Processing Course10","url":"/2020/03/30/CS224n Classnotes Course9/","content":"\n# Course 10 - Question Answering\n\n## Motivation/History\n\nWith massive collections of full-text documents, return relevant documents.\n\n2 parts\n\n1. find documents that might contain an answer\n2. find an answer in a paragraph or a document\n\nMCTest Reading Comprehension: Passage+Question=Answer\n\n![image-20200330135135377.png](https://i.loli.net/2020/04/04/xpvqj1TbEcJkyOL.png)\n\n## The SQuAD dataset\n\nEvalution \n\n- Systems are scored on two metrics\n\n  - exact match\n  - f1:  Precision = tp/(tp+fp), Recall = tp/(tp+tn), F1=2PR/(P+R) - taken as primary\n\n  Both metrics ignore punctuation and articles (a, an, the only)\n\nLimiations\n\n- Only span-based answers\n- Questions were constructed looking at passages\n- Barely any multi-facts/sentence inference beyonce coreference\n\nBut still, well-targeted, well-structured, clean dataset\n\n## The Stanford Attentive Reader model\n\n## BiDAF\n\n![image-20200331105339872.png](https://i.loli.net/2020/04/04/wpZ5DlvJ7AK4xVY.png)\n\ncentral idea: the Attention Flow layer\n\nIdea: attention should flow both ways - from the context to the question and from the question to the context\n\nMake the similarity matrix:\n$$\nS_{ij} = w_{sim}^{T}[c_i;q_j;c_i \\cdot q_j]\n$$\nContext-to-Question attention:\n$$\n\\alpha^{i} = softmax(S_i,:) \\in \\mathbb{R}^M\\\\\n\\alpha_i=\\sum_{j=1}^M \\alpha_j^i q_j \\in \\mathbb{R}^{2h}\n$$\n\nAttention Flow Idea: attention should flow both ways\n\nQuestion-to-Context attention:\n$$\nm_i=max_j S_{ij} \\in \\mathbb{R}\\\\\n\\beta = softmax(m) \\in \\mathbb{R}^N\\\\\nc' = \\sum_{i=1}^{N} \\beta_i c_i \\in \\mathbb{R}^{2h}\n$$\n\n## Recent, more advanced architectures\n\nFusionNet\n\n## ELMo and BERT preview\n\n","categories":["NLP"]},{"title":"Stanford CS224n Natural Language Processing Course11","url":"/2020/03/30/CS224n Classnotes Course11/","content":"\n# Course 11 - Convolutional Network for NLP\n\n","categories":["NLP"]},{"title":"Object Oriented Programming in Java: Data Structure","url":"/2020/03/29/Object-Oriented-Programming-in-Java-Data-Structure/","content":"\n### Course notes toward [Data Structure by University of California, San Diego](https://www.coursera.org/learn/object-oriented-java/home/welcome)\n\n\n\n## Course 4 - Data Visualization\n\n### Week 4\n\n##### Inheritance Constructors - hierarchy.\n\nOverloading - **Same class** has same method name with **different** parameters.\n\nOverriding - **Subclass** has same method name with the **same parameters** as the superclass.\n\n##### Polymorphism\n\n```Java\nPerson s = new Student(\"Cara\", 1234);\n```\n\nStudent, Faculty extends Person class. \n\n1. Compile Time Rules - only knows reference type\n2. Runtime Rules - follow exact runtime type of object\n\n###### Casting\n\n- Automatic type promotion (like `int` to `double`)\n  - Superclass superRef = new Subclass();\n- Explicit casting (like `double` to `int`)\n  - Subclass ref = (Subclass) superRef;\n\nRuntime type check - `instanceof`\n\n![image-20200329152100707](C:\\Users\\surface\\AppData\\Roaming\\Typora\\typora-user-images\\image-20200329152100707.png)\n\n##### Abstract Classes and Interfaces\n\n`public abstract class Person{}`\n\n`public abstract void monthlyStatement(){}`\n\nAbstract classes offer inheritance of both\n\n- Implementation \n- Interface - only define required methods & inherit from multiple interfaces","tags":["Java"]},{"title":"Stanford CS224n Natural Language Processing Course8","url":"/2020/03/16/CS224n Classnotes Course8/","content":"\n# Course 8 - Translation, Seq2Seq, Attention\n\n## Machine Translation\n\nfrom the source language to the target language\n\n### Statistical Machine Translation\n\nCore idea: learn a probabilistic model from data.\n\nbest  English sentence $y$, given French sentence $x$\n$$\nargmax_y P(y|x)\\\\\n=argmax_y P(x|y) P(y)\n$$\nTranslation model($P(x|y)$) - Models how words and phrases should be translated. Learnt from **parallel data**\n\nnote: parallel data - pairs of human-translated french/english sentences.\n\nLanguage model($P(y)$) - Models how to write good English. Learnt from monolingual data.\n\n#### Learning alignment for SMT\n\nalignment is the *correspondence between particular words* in the translated sentence pair.\n\nAlignment can be **many-to-one** & **one-to-many** & **many-to-many**\n\nWe learn $P(x, a|y)$ as a combination of many factors.\n\n#### Decoding for SMT\n\nUse a **heuristic search algorithm** to search for the best translation, discarding hypotheses that are too low-probability. This process is called *decoding*\n\n### Neural Machine Translation\n\n**sequence-to-sequence(Seq2seq)** involves 2 RNNs. a **conditional language model**\n\n- conditional - its predictions are conditioned on the source sentence $x$\n\n![image-20200328233749255.png](https://i.loli.net/2020/04/04/gixM6LmGa5Ce7Js.png)\n\nOther tasks \n\n- summarization\n- dialogue\n- parsing\n- code generation (natural language -> Python code)\n\nNMT directly calculates $P(y|x)$\n$$\nP(y | x)=P\\left(y_{1} | x\\right) P\\left(y_{2} | y_{1}, x\\right) P\\left(y_{3} | y_{1}, y_{2}, x\\right) \\ldots P\\left(y_{T} | y_{1}, \\ldots, y_{T-1}, x\\right)\n$$\nQuestion: How to train?\n\nAnswer: Get a big parallel corpus...\n\nGreedy decoding has many problems, it's an **exhaustive search decoding**, instead we use **beam search decoding**\n\nCore idea: On each step of decoder, keep track of the *k most probable* partial translations (*hypotheses*)\n\n- *k* is the **beam size**\n\n$$\nscore(y_1,\\cdots,y_t)=logP_{LM}(y_1,\\cdots,y_t|x)=\\sum_{i=1}^t logP_{LM}(y_i|y_1,\\cdots,y_{i-1}, x)\n$$\n\n- Scores are all negative, and higher score is betterr\n- We search for high-scoring hypotheses, tracking top *k* on each step\n\nBeam search is *not guaranteed* to find optimal solution, but efficient.\n\nIn greedy decoding, usually we decode until the model produces a <END> token. For example, <START> he hit me with a pie <END>\n\nProblem：How to select top one with highest score? longer hypotheses have lower scores\n\nFix: Normalize by length. \n$$\n\\frac{1}{t}\\sum_{i=1}^t logP_{LM}(y_i|y_1,\\cdots,y_{i-1}, x)\n$$\n\n#### Advantages of NMT\n\n- better performance.\n  - more fluent\n  - better use of context\n  - better use of phrase similarities\n- A single neural network to be optimized end-to-end\n  - No subcomponents to be individually optimized\n- Requires much less human engineering effort\n  - No feature engineering\n\n#### Disadvantages of NMT\n\nCompared to SMT:\n\n- NMT is less interpretable\n  - Hard to debug\n- NMT is difficult to control\n\n#### How to evaluate?\n\nBLEU(Bilingual Evaluation Understudy)\n\nBLEU compares the machine-written translation to one or several human-written translations, and computes similarity-score based on:\n\n- n-gram precision\n- Plus a penalty for too-short system tranlations\n\n#### Difficulties remain\n\nOut-of-vocabulary words\n\nDomain mismatch between train and test data\n\nMaintaining context over longer text\n\nLow-resource language pairs\n\nNMT picks up **biases** in training data\n\nUninterpretable system do strange things.\n\n#### Attention\n\nBottleneck problem.\n\ndefinition - Given a set of vector *values*, and a vector *query*, **attention** is a technique to compute a weighted sum of the values, dependent on the query. \n\nvariants\n\n- Basic dot-product attention\n- Multiplicative attention\n- Additive attention","categories":["NLP"]},{"title":"Stanford CS224n Natural Language Processing Course7","url":"/2020/03/16/CS224n Classnotes Course7/","content":"\n# Course 7 - Vanishing Gradients, Fancy RNN\n\n## Vanishing Gradients\n\nGradient can be viewed as a measure of *the effect of the past on the future*\n\n- There is no dependency between step $t$ and $t+n$ in the data\n- We have wrong parameters to capture the true dependency between  $t$ and $t+n$ \n\n## Effect of vanishing gradient on RNN-LM\n\nLM task - unable to predict similar long-distance dependencies\n\nSyntactic recency: The *writer* of the books **is**\n\nSequential recency: The writer of *books* **are**\n\nDue to vanishing gradient, RNN-LMs are better at learning from sequential recency than syntactic \n\n## Why is exploding gradient a problem?\n\n$$\n\\theta^{new} = \\theta^{old}-\\alpha \\nabla_{\\theta} J(\\theta)\n$$\n\n### Solution: gradient clipping\n\nAlgorithm 1: Pseudo-code for norm clipping\n\n$\\hat{g} \\leftarrow \\frac{\\partial \\epsilon}{\\partial \\theta}$\n\nif $||\\hat{g}|| \\geq threshold $ then\n\n​\t$\\hat{g} \\leftarrow \\frac{threshold}{||\\hat{g}||} \\hat{g}$\n\nend if \n\n## Long Short-Term Memory(LSTM)\n\nOn step $t$, there is a hidden state $h^{(t)}$ and a cell state $c^{(t)}$\n\n- Both are vectors length $n$\n- The cell stores long-term information\n- The LSTM can erase, write and read information from the cell\n\nThe selection is controlled by 3 corresponding **gates**\n\n- vector length $n$\n- each element of the gates can be open(1), closed(0), or in between\n- dynamic: their value is computed based on the current context\n\nWe have a sequence of input $x^{(t)}$, and we will compute a sequence of hidden states $h^{(t)}$ and cell states $c^{(t)}$. On timestep $t$:\n\nForget gate - controls what is kept vs forgotten, from previous cell state\n$$\n\\boldsymbol{f}^{(t)}={\\sigma}\\left(\\boldsymbol{W}_{f} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{U}_{f} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{f}\\right)\n$$\nInput gate - controls what parts of the new cell content are written to cell\n$$\n\\boldsymbol{i}^{(t)}=\\sigma\\left(\\boldsymbol{W}_{i} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{U}_{i} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{i}\\right)\n$$\nOutput gate - controls what parts of cell are output to hidden state\n$$\n\\boldsymbol{o}^{(t)}={\\sigma}\\left(\\boldsymbol{W}_{o} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{U}_{o} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{o}\\right)\n$$\nNew cell content - this is the new content to be written to the cell\n$$\n\\tilde{\\boldsymbol{c}}^{(t)}=\\tanh \\left(\\boldsymbol{W}_{c} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{U}_{c} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{c}\\right)\n$$\nCell state - erase(\"forget\") some content from last cell state, and write(\"input\") some new cell content\n$$\n\\boldsymbol{c}^{(t)}=\\boldsymbol{f}^{(t)} \\circ \\boldsymbol{c}^{(t-1)}+\\boldsymbol{i}^{(t)} \\circ \\tilde{\\boldsymbol{c}}^{(t)}\n$$\nHidden state: read(\"output\") some content from the cell\n$$\n\\boldsymbol{h}^{(t)}=\\boldsymbol{o}^{(t)} \\circ \\tanh \\boldsymbol{c}^{(t)}\n$$\n![image-20200316220913640.png](https://i.loli.net/2020/04/04/pW7mMj9aDN53h8C.png)\n\n## Gated Recurrent Units(GRU)\n\na simpler alternative to the LSTM\n\nOn each timestep $t$ we have input $x^{(t)}$ and hidden state $h^{(t)}$ (no cell state)\n\nUpdate gate - controls what parts of hidden state are updated vs preserved\n$$\n\\boldsymbol{u}^{(t)}=\\sigma\\left(\\boldsymbol{W}_{u} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{U}_{u} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{u}\\right)\n$$\nReset gate - controls what parts of previous hidden state are used to compute new content\n$$\n\\boldsymbol{r}^{(t)}=\\sigma\\left(\\boldsymbol{W}_{r} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{U}_{r} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{r}\\right)\n$$\nNew hidden state content - reset gate selects useful parts of prev hidden state. Use this and current input to compute new hidden content.\n$$\n\\tilde{\\boldsymbol{h}}^{(t)}=\\tanh \\left(\\boldsymbol{W}_{h}\\left(\\boldsymbol{r}^{(t)} \\circ \\boldsymbol{h}^{(t-1)}\\right)+\\boldsymbol{U}_{h} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{h}\\right)\n$$\nHidden state - update gate simultaneously controls what is kept from previous hidden state, and what is updated to new hidden state content\n$$\n\\boldsymbol{h}^{(t)}=\\left(1-\\boldsymbol{u}^{(t)}\\right) \\circ \\boldsymbol{h}^{(t-1)}+\\boldsymbol{u}^{(t)} \\circ \\tilde{\\boldsymbol{h}}^{(t)}\n$$\n\n## LSTM vs GRU\n\nLSTM is a good default choice.\n\nRule of thumb: start with LSTM, but switch to GRU if you want something more efficient\n\n## Vanishing/exploding gradient\n\nadd more direct connections.\n\ne.g.: Residual connections, \"ResNet\". Skip-connections. The identity connection preserves information by default.\n\ne.g.: Dense connections, \"DenseNet\". \n\ne.g.:Highway connections, \"HighwayNet\".\n\n## Bidirectional RNNs\n\nmotivation - task: Sentiment Classification\n\n![image-20200317001951410.png](https://i.loli.net/2020/04/04/on9a2s1Y5jJiADb.png)\n\nNote: bidirection RNNs are only applicable if you have access to the entire input sentence. \n\nBERT is built on bidirectionality.\n\n## Multi-layer RNNs = stacked RNNs\n\nHigh-performing RNNs are often multi-layer(but aren't as deep as cn)\n\n","categories":["NLP"]},{"title":"How powerful is Graph Neural Networks?","url":"/2020/03/10/How_powerful_is_gnn/","content":"\n# How powerful is Graph Neural Networks?\n\nGNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. \n\nThere are two tasks of interest: \n\n(1) Node classiﬁcation\n\n(2) Graph classiﬁcation\n\nx Formally, the k-th layer of a GNN is\n$$\na_{v}^{(k)}=\\operatorname{AGGREGATE}^{(k)}\\left(\\left\\{h_{u}^{(k-1)}: u \\in \\mathcal{N}(v)\\right\\}\\right), \\quad h_{v}^{(k)}=\\mathrm{COMBINE}^{(k)}\\left(h_{v}^{(k-1)}, a_{v}^{(k)}\\right)\n$$\n\n\n In the pooling variant of GraphSAGE, AGGREGATE has been formulated as \n$$\na_{v}^{(k)}=\\operatorname{MAX}\\left(\\left\\{\\operatorname{ReLU}\\left(W \\cdot h_{u}^{(k-1)}\\right), \\forall u \\in \\mathcal{N}(v)\\right\\}\\right)\n$$\nand COMBINE could be concatenation followed by a linear mapping \n$$\nW \\cdot [h_{v}^{(k-1)}, a_{v}^{(k)}]\n$$\nGraph Convolutional Networks (GCN) - the element-wise *mean* pooling is used. AGGREGATE and COMBINE step\n$$\nh_{v}^{(k)}=\\operatorname{ReLU}\\left(W \\cdot \\operatorname{MEAN}\\left\\{h_{u}^{(k-1)}, \\forall u \\in \\mathcal{N}(v) \\cup\\{v\\}\\right\\}\\right)\n$$\nthe READOUT function aggregates node features from the ﬁnal iteration to obtain the entire graph’s representation $h_G$\n$$\nh_{G}=\\operatorname{READOUT}\\left(\\left\\{h_{v}^{(K)} | v \\in G\\right\\}\\right)\n$$\n\n**Deﬁnition1 (Multiset).** A multiset is a generalized concept of a set that allows multiple instances for its elements. More formally, a multiset is a 2-tuple $X = (S,m)$ where $S$ is the underlying set of $X$ that is formed from its distinct elements, and $m : S \\rightarrow \\mathbb{N}_{\\geq 1}$ gives the multiplicity of the elements. \n\n**Lemma2**. Let $G_1$ and $G_2$ be any two non-isomorphic graphs. If a graph neural network $A : G→\\mathbb{R}^d $ maps $G_1$ and $G_2$ to different embeddings, the Weisfeiler-Lehman graph isomorphism test also decides $G_1$ and $G_2$ are not isomorphic. \n\n**Theorem 3**. Let $A : G → \\mathbb{R}^d$ be a GNN. With a sufﬁcient number of GNN layers, A maps any graphs $G_1$ and $G_2$ that the Weisfeiler-Lehman test of isomorphism decides as non-isomorphic, to different embeddings if the following conditions hold: \n\na) $A$ aggregates and updates node features iteratively with \n$$\nh_{v}^{(k)}=\\phi\\left(h_{v}^{(k-1)}, f\\left(\\left\\{h_{u}^{(k-1)}: u \\in \\mathcal{N}(v)\\right\\}\\right)\\right)\n$$\nwhere the functions $f$, which operates on multisets, and $φ$ are injective. \n\nb) $A$’s graph-level readout, which operates on the multiset of node features ${h_{v}^{(k)}}$, is injective. \n\n**Lemma4**. Assume the input feature space $\\mathcal{X}$ is *countable*. Let $g^{(k)}$ be the function parameterized by a GNN’s k-th layer for $k = 1,...,L$, where $g^{(1)}$ is deﬁned on multisets $X \\subset \\mathcal{X}$ of bounded size. The range of $g^{(k)}$, i.e., the space of node hidden features ${h_{v}^{(k)}}$, is also countable for all  $k = 1,...,L$.\n\n*countable*: If a set A has the same cardinality as $\\mathbb{N}$, then we say that A is *countable*.\n\n**Lemma5**. Assume $\\mathcal{X}$ is countable. There exists a function $f : \\mathcal{X} →\\mathbb{R}^n$ so that  $h(X) =\\sum _{x\\in X} f(x)$ is unique for each multiset  $X \\subset \\mathcal{X}$  of bounded size. Moreover, any multiset function $g$ can be decomposed as $g (X) = \\varphi(\\sum _{x\\in X} f(x))$ for some function $\\varphi$.\n\n**Corollary 6**. Assume $\\mathcal{X}$ is countable. There exists a function $f : \\mathcal{X} →\\mathbb{R}^n$ so that for inﬁnitely many choices of $\\epsilon$, including all irrational numbers, $h(c,X) = (1 + \\epsilon) · f(c) + \\sum_{x \\in X} f(x)$ is unique for each pair $(c,X)$, where $c \\in \\mathcal{X}$ and $X \\subset \\mathcal{X}$ is a multiset of bounded size. Moreover, any function $g$ over such pairs can be decomposed as $g (c,X) = \\varphi((1 + \\epsilon)·f(c) + \\sum_{x\\in X} f(x))$ for some function $\\varphi$. \n\n## GIN - GRAPH ISOMORPHISM NETWORK \n\nTo update node representation - \n$$\nh_{v}^{(k)}=\\operatorname{MLP}^{(k)}\\left(\\left(1+\\epsilon^{(k)}\\right) \\cdot h_{v}^{(k-1)}+\\sum_{u \\in \\mathcal{N}(v)} h_{u}^{(k-1)}\\right)\n$$\nnode learnt can be directly used for tasks like **node classification and link prediction**.\n\nReadout function for graph classiﬁcation tasks. Given embeddings of individual nodes, readout function produces the embedding of the entire graph. \n$$\nh_{G}=\\operatorname{CONCAT}\\left(\\operatorname{READOUT}\\left(\\left\\{h_{v}^{(k)} | v \\in G\\right\\}\\right) | k=0,1, \\ldots, K\\right)\n$$\n\n## GNN - GRAPH NEURAL NETWORK\n\nGNN do not satisfy the conditions in Theorem 3, and we conduct ablation studies in \n\n*An ablation study typically refers to removing some “feature” of the model or algorithm, and seeing how that affects performance.\n\n(1) 1-layer perceptrons instead of MLPs\n\nWe are interested in understanding whether 1-layer perceptrons are enough for graph learning.\n\n**Lemma 7.** There exist ﬁnite multisets $X1 \\neq X2$ so that for any linear mapping $W$, $\\sum_{x\\in X_1} ReLU(Wx) =\\sum_{x\\in X_2} ReLU(Wx). $\n\n(2) mean or max-pooling instead of the sum.\n\nMean learns distributions, and max-pooling learns sets with distinct elements.\n\nConsider $X_1 = (S,m)$ and $X_2 = (S,k ·m)$, where $X_1$ and $X_2$ have the same set of distinct elements, but $X_2$ contains $k$ copies of each element of $X_1$.\n\n**Corollary 8.** Assume $X$ is countable. There exists a function $f : \\mathcal{X} → \\mathbb{R^n}$ so that for $h(X) = \\frac{1}{|X|}\\sum{x\\in X} f(x), h(X_1) = h(X_2)$ if and only if multisets $X_1$ and $X_2$ have the same distribution. That is, assuming $|X_2|\\geq|X_1|$, we have $X_1 = (S,m)$ and $X_2 = (S,k\\cdot m)$ for some $k \\in \\mathbb{N}_{\\geq 1}.$\n\n**Corollary 9**. Assume $X$  is countable. Then there exists a function $f : X → \\mathbb{R}^{\\infty}$ so that for $h(X) = max_{x\\in X} f(x), h(X_1) = h(X_2)$ if and only if $X_1$ and $X_2$ have the same underlying set. ","tags":["GNN"]},{"title":"GNN survey","url":"/2020/03/02/GNN-survey/","content":"\n# Background and Definitions\n\n## Background\n\nEarly studies fall into the category of recurrent graph neural networks(**RecGNNs**). With the success of CNNs,  new approach developed. (**ConvGNNs**)  ConvGNNs are divided into two main streams, the **spectral-based** approaches and the **spatial-based** approaches. \n\nNetwork embedding aims at representing network nodes as low-dimensional vector representations, preserving both network topology structure and node content information.\n\nGraph kernel methods employ a kernel function to measure the similarity between pairs of graphs.  Graph kernels can embed graphs or nodes into vector spaces by a mapping function. The difference between GNN and graph kernels is that this mapping function is deterministic rather than learnable.\n\n## Definitions\n\n**Deﬁnition 1 (Graph):** A graph is represented as $G = (V,E)$ where $V$ is the set of vertices or nodes, and $E$ is the set of edges. Let $v_i \\in V$ to denote a node and $e_{ij} = (v_i,v_j) \\in E$ to denote an edge pointing from $v_j$ to $v_i$. The neighborhood of a node $v$ is deﬁned as $N(v) = {u \\in V|(v,u) \\in E}$. The adjacency matrix $A$ is a $n \\times n$ matrix with $A_{ij} = 1$ if $e_{ij} \\in E$ and $A_{ij} = 0$ if $e_{ij} \\notin E$. A graph may have node attributes $ x_{v}$ , where $X \\in R^{n×d}$ is a node feature matrix with $x_{v} \\in R^d$  representing the feature vector of a node $v$. Meanwhile, a graph may have edge attributes $X^e$, where $X^e \\in R^{m×c}$ is an edge feature matrix with $x^{e}_{v,u} \\in R^c$ representing the feature vector of an edge $(v,u)$.\n\n**Deﬁnition 2 (Directed Graph):** A directed graph is a graph with all edges directed from one node to another. An undirected graph is considered as a special case of directed graphs where there is a pair of edges with inverse directions if two nodes are connected. A graph is undirected if and only if the adjacency matrix is symmetric. \n\n**Deﬁnition 3 (Spatial-Temporal Graph)**: A spatial-temporal graph is an attributed graph where the node attributes change dynamically over time. The spatial-temporal graph is deﬁned as $G^{(t)} = (V,E,X^{(t)})$ with $X^{(t)} \\in R^{n×d}$. \n\n# Categorization and Frameworks\n\nRecurrent Neural Network - aim to learn node representations with recurrent neural architectures.\n\nConvolutional graph neural networks - generate a node $v$’s representation by aggregating its own features $x_v$ and neighbors’ features $x_u$, where $u \\in N(v)$ \n\nGraph autoencoders(GAE)  learn network embeddings and graph generative distributions\n\n","tags":["GNN"]},{"title":"ubuntu18.04+nvidia driver+cuda+cudnn install","url":"/2020/02/29/ubuntu18-04-nvidia-driver-cuda-cudnn爬坑全记录/","content":"\n# Install Ubuntu18.04 \n\naccording to this blog: https://blog.csdn.net/silver1225/article/details/100393719\n\n# install nvidia driver\n\n## First, disable original nvidia driver, nouveau\n\n1. delete the original nvidia driver\n\n```\nsudo apt-get remove nvidia-*\n```\n\n2. forbid nouveau\n\n```\nsudo gedit /etc/modprobe.d/blacklist.conf\n```\n\nand add this in the bottom\n\n```\nblacklist nouveau\noptions nouveau modeset=0\n```\n\n3. execute\n\n```\nsudo update-initramfs -u\nreboot \n```\n\n4. see the version of nouveau, if nothing returns, you succeed.\n\n```\nlsmod | grep nouveau\n```\n\n## Second, install your nvidia driver.\n\n1. search your nvidia driver according to your version at https://www.nvidia.cn/Download/index.aspx?lang=cn\n\n2. update all\n\n```\nsudo apt-get update\n```\n\n3. install gcc\n\n```\nsudo apt install build-essential\n```\n\n4. show the version of your nvidia\n\n```\nlshw -numeric -C display\n```\n\n5. install\n\n```\nsudo chmod a+x NVIDIA-Linux-x86_64-418.43.run\nsudo ./NVIDIA-Linux-x86_64-418.43.run --no-opengl-files --no-x-check --no-nouveau-check\n```\n\n6. check whether you succeed or not\n\n```\nnvidia-smi\n```\n\n## Third, download cuda\n\ndownload cuda at https://developer.nvidia.com/cuda-downloads\n\n```\nsudo sh cuda_10.0.130_410.48_linux.run\n```\n\nand select \n\n```\n1 accept #同意安装\n2 n #不安装Driver，因为已安装最新驱动\n3 y #安装CUDA Toolkit\n4 <Enter> #安装到默认目录\n5 y #创建安装目录的软链接\n6 y #复制Samples一份到家目录\n```\n\nThen\n\n```\nsudo vim ~/.bashrc\n```\n\nAdd this to the file\n\n```\nexport PATH=/usr/local/cuda-10.1/bin${PATH:+:$PATH}} \nexport LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n```\n\nGo back to command line\n\n```\nsource ~/.bashrc\nsudo vim /etc/profile\nexport PATH=/usr/local/cuda/bin:$PATH\nsudo vim /etc/ld.so.conf.d/cuda.conf\n```\n\nAdd this to the file.\n\n# Last\n\n```\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable, grad\n\nSIZE=[1, 1, 171*21, 171*21]\ninput = Variable(torch.cuda.FloatTensor(*SIZE).uniform_(), requires_grad=True)\nconv1 = nn.Conv2d(1, 1, kernel_size=3, stride=1, dilation=1, padding=1,bias=False).cuda()\noutput = conv1(input)\nloss = output.sum()\nloss.backward()\n```\n\nrun this, if no error, you SUCCEED!","tags":["Ubuntu18.04"]},{"title":"Weibo - Delete all your followings and posts","url":"/2020/02/11/weibo_delete_all/","content":"\n## Delete all your followings \n\n1. open Google Chrome\n2. Press F12\n3. click `console`\n4. paste the codes\n\n```\n/* 点击批量管理 */\n$(\".btn_link.S_txt1\").click();\n/* 勾选全部 */\n$$('.member_li').forEach(l => l.click());\n/* 点击取消关注 */\n$('.W_btn_a[node-type=\"cancelFollowBtn\"]').click();\n/* 点击确认按钮 */\n$('[node-type=\"ok\"]').click();\n```\n\n## Delete all your posts\n\n```\n// ==UserScript==\n// @name         Weibored.js\n// @namespace    https://vito.sdf.org\n// @version      0.2.0\n// @description  删除所有微博\n// @author       Vito Van\n// @match        https://weibo.com/p/*\n// @grant        none\n// ==/UserScript==\n'use strict';\nvar s = document.createElement('script');\ns.setAttribute(\n'src',\n'https://lib.sinaapp.com/js/jquery/2.0.3/jquery-2.0.3.min.js'\n);\ns.onload = function() {\nsetInterval(function() {\nif (!$('a[action-type=\"feed_list_delete\"]')) {\n$('a.next').click();\n} else {\n$('a[action-type=\"feed_list_delete\"]')[0].click();\n$('a[action-type=\"ok\"]')[0].click();\n}\n// scroll bottom let auto load\n$('html, body').animate({ scrollTop: $(document).height() }, 'slow');\n}, 800);\n};\ndocument.head.appendChild(s);\n```\n\n","categories":["Computer Science"]},{"title":"Stanford CS224n Natural Language Processing Course6","url":"/2020/02/10/CS224n Classnotes Course6/","content":"\n# Course 6 - Language Models and RNNs\n\n## Language Modeling\n\nis the task of predicting what word comes next. \n$$\nP\\left(\\boldsymbol{x}^{(t+1)} | \\boldsymbol{x}^{(t)}, \\ldots, \\boldsymbol{x}^{(1)}\\right)\n$$\nalso, **assigns probability to a piece of text**\n$$\n\\begin{aligned} P\\left(\\boldsymbol{x}^{(1)}, \\ldots, \\boldsymbol{x}^{(T)}\\right) &=P\\left(\\boldsymbol{x}^{(1)}\\right) \\times P\\left(\\boldsymbol{x}^{(2)} | \\boldsymbol{x}^{(1)}\\right) \\times \\cdots \\times P\\left(\\boldsymbol{x}^{(T)} | \\boldsymbol{x}^{(T-1)}, \\ldots, \\boldsymbol{x}^{(1)}\\right) \\\\ &=\\prod_{t=1}^{T} P\\left(\\boldsymbol{x}^{(t)} | \\boldsymbol{x}^{(t-1)}, \\ldots, \\boldsymbol{x}^{(1)}\\right) \\end{aligned}\n$$\n\n### n-gram Language Models\n\n#### Definition\n\nA **n-gram** is a chunk of n consecutive words\n\n#### Idea\n\nCollect statistics about how frequent different n-grams are, and use these to predict next word.\n\n#### Sparity Problem 1: what if \"students opened their *w*\" never occurred in data\n\nSolution-Smoothing: Add small delta to the count for every *w* \n\n#### Sparity Problem2: what if \"students opened their\" never occurred in data? \n\nSolution-backoff: Just condition on \"opened their\" instead\n\n## How to build a *neural* Language Model - Recurrent Neural Networks\n\n### A fixed-window neural Language Model\n\n### Recurrent Neural Networks(RNN)\n\n![image-20200210211740128.png](https://i.loli.net/2020/03/04/VGcU3SeqL7JhZoM.png)\n\n\n\n**Core idea**: Apply the same weight W repeatedly. \n\n![image-20200210220449695.png](https://i.loli.net/2020/03/04/2vle5KkWsRcUJ8F.png)\n\n\n\n**Advantages**\n\n- Can process any length input\n- use information from many steps back in theory\n- Model size doesn't increase for longer input\n- Same weights applied on every steps. \n\n**Disadvantages**\n\n- slow computation\n- difficult to access information from many steps back practically \n\n#### Training a RNN Language Model\n\n- Get a **big corpus of text** which is a sequence of word $x^{1}, \\cdots, x^{T}$\n\n- Feed into RNN-LM\n\n- Loss function on step t is **cross-entropy**\n  $$\n  J^{(t)}(\\theta)=C E\\left(\\boldsymbol{y}^{(t)}, \\hat{\\boldsymbol{y}}^{(t)}\\right)=-\\sum_{w \\in V} \\boldsymbol{y}_{w}^{(t)} \\log \\hat{\\boldsymbol{y}}_{w}^{(t)}=-\\log \\hat{\\boldsymbol{y}}_{\\boldsymbol{x}_{t+1}}^{(t)}\n  $$\n\n- Average this to get **overall loss **for entire training set\n  $$\n  J(\\theta)=\\frac{1}{T} \\sum_{t=1}^{T} J^{(t)}(\\theta)=\\frac{1}{T} \\sum_{t=1}^{T}-\\log \\hat{y}_{x_{t+1}}^{(t)}\n  $$\n\n- However, computing loss and gradients across **entire corpus** is too expensive. In practice, consider $x^{1}, \\cdots, x^{T}$ as a **sentence** (or a **document**)\n- Instead, using **SGD** to compute loss $J(\\theta)$ for a sentence, compute gradients and update weights. Repeat.\n\n#### Backpropagation for RNNs\n\nQuestions: derivative of $J^{(t)}(\\theta)$ w.r.t the **repeated** weight matrix \n\nAnswer:\n$$\n\\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{W}_{\\boldsymbol{h}}}=\\left.\\sum_{i=1}^{t} \\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{W}_{\\boldsymbol{h}}}\\right|_{(i)}\n$$\n\n#### Generating text with a RNN Language Model.\n\nrepeated sampling \n\n#### Evaluating Language Models: Perplexity\n\n$$\nperplexity =\\prod_{t=1}^{T}\\left(\\frac{1}{P_{\\mathrm{LM}}\\left(x^{(t+1)} | \\boldsymbol{x}^{(t)}, \\ldots, \\boldsymbol{x}^{(1)}\\right)}\\right)^{1 / T} = \\exp(J(\\theta))\n$$\n\nlower perplexity is better!\n\n#### Why should we care about Language Modeling?\n\nLanguage Modeling is a **benchmark task** that helps us **measure our progress** on understanding language.\n\n**subcomponent** of many NLP tasks.\n\nRNNs can be used for tagging, NER, part-of-speech tagging.\n\nRNNs can be used for sentence classification, sentiment classification.\n\nRNNs can be used as an encoder module, question answering, machine translation.\n\nRNN described is called **vanilla RNN**\n\nGRU, LSTM(chocolate), multi-layer RNNs","categories":["NLP"]},{"title":"Weibo Hot Topic Web Scrawler - to monitor the public sentiment in China","url":"/2020/02/10/weibo hottopics crawler/","content":"\n### Web Scrawler is dangerous\n\nFirst and foremost, do not touch web scrawler if you are 100% sure that you wanna do this.\n\n### Similar topic:\n\nhttps://nobugs.dev/2019/07/14/webscrawler/\n\nwebsite: enlightent\n\n### This time\n\nWe choose to use the package of `requests` and package `json` to get the result of hot topics  in weibo, similar website to twitter which has real-time hot topics, a great way to monitor the public sentiment in China.\n\n### Code Implementation\n\n\n\n```python\nimport requests\nimport json\n\n\nheaders = {\n    'charset': \"utf-8\",\n    'Accept-Encoding': \"gzip\",\n    'referer': \"https://servicewechat.com/wx90ae92bbd13ec629/11/page-frame.html\",\n    'content-type': \"application/x-www-form-urlencoded\",\n    'User-Agent': \"Mozilla/5.0 (Linux; Android 9; Redmi Note 7 Build/PKQ1.180904.001; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/68.0.3440.91 Mobile Safari/537.36 MicroMessenger/7.0.3.1400(0x2700033B) Process/appbrand0 NetType/WIFI Language/zh_CN\",\n    'Host': \"www.eecso.com\",\n    'Connection': \"keep-alive\",\n    'cache-control': \"no-cache\",\n    'Origin': 'https://www.weibotop.cn',\n}\n\n\nwith open('微博热搜.csv', 'w', encoding='gbk') as f:\n    f.write('时间,排名,热搜内容,上榜时间,最后时间\\n')\n\ntimeid = 77594\ndateUrl = \"https://www.eecso.com/test/weibo/apis/getlatest.php?timeid={}\"\ncontentUrl = \"https://www.eecso.com/test/weibo/apis/currentitems.php?timeid={}\"\nn = 1\ndays = 42 #需获取2.10往前多少天的数据\ninterval = 720 #改为1则是爬所有数据（该网站2分钟记录一次） 24*30 = 720\nwhile True:\n    dateResponse = requests.request(\"GET\", dateUrl.format(timeid), headers=headers,verify=False)\n    contentResponse = requests.request(\"GET\", contentUrl.format(timeid), headers=headers,verify=False)\n    timeid = 77594-interval*n #77594为2020/2/10 12:00的timeid，720为一天timeid的间隔\n    print(timeid)\n    n += 1\n    dateJson = json.loads(dateResponse.text)\n    json_obj = json.loads(contentResponse.text)\n    #print(dateJson)\n    \n    for index,item in enumerate(json_obj):\n        date = dateJson[1]\n        rank = str(index+1)\n        hotTopic = item[0]\n        onTime = item[1]\n        lastTime = item[2]\n        save_res = date+\",\"+rank+\",\"+hotTopic+','+onTime+','+lastTime+'\\n'\n        with open('微博热搜.csv','a',encoding='gbk',errors='ignore') as f:\n            f.write(save_res)\n    if n > days:\n        break\n\n```\n\n","categories":["Computer Science"]},{"title":"Walter Rudin, Functional Analysis brief theorems(Chapter General Theory) - ongoing","url":"/2020/02/10/Functional Analysis brief theorems/","content":"\n[Booklink](https://59clc.files.wordpress.com/2012/08/functional-analysis-_-rudin-2th.pdf)\n\n# Part I - General Theory\n\n## Chapter 1 -  Topological Vector Spaces\n\n**1.2 Normed spaces** A vector space X is said to be a normed space if to every x in X there is associated a nonnegative real number llxll, called the norm of x, in such a way that \n\n(a) $\\quad\\|x+y\\| \\leq\\|x\\|+\\|y\\|$ for all $x$ and $y$ in $X$\n(b) $\\quad\\|\\alpha x\\|=|\\alpha|\\|x\\|$ if $x \\in X$ and $\\alpha$ is a scalar,\n(c) $\\quad\\|x\\|>0$ if $x \\neq 0$\n\nIn any metric space, the open ball with center at x and radius r is the set \n$$\nB_r(x) = \\{y:d(x,y)<r\\}\n$$\n\n\n\n\n","categories":["mathematics"]},{"title":"Stanford CS224n Natural Language Processing Course5","url":"/2020/02/04/CS224n Classnotes Course5/","content":"\n# Course 5 - Dependency Parsing\n\n## Syntactic Structure: Consistency and Dependency\n\n### 1. Two views of linguistic structure: Constituency  = phrase stucture grammar = context-free grammars (CFGs)\n\n**Phrase structure** organizes words into nested constituents\n\n## Dependency Grammar and Treebanks\n\nDependency structure shows which words depend on which other words.\n\nDependency syntax postulates that syntactic structure consists of relations between lexical items, normally binary asymmetric relations(\"arrows\") called **dependencies**\n\nThe arrows are commonly **typed** with the name of grammatical relations(subject, prepositional object, apposition, etc.)\n\nUsually, dependencies form a tree(connected, acyclic, single-head)\n\n## Transition-based dependency parsing\n\nA simple form of greedy discriminative dependency parser.\n$$\nStart: \\sigma=[\\mathrm{ROOT}], \\beta=w_{1}, \\ldots, w_{n}, A=\\emptyset\\\\\n1. Shift\\qquad \\sigma, w_{i}|\\beta, A \\rightarrow \\sigma| w_{i}, \\beta, A\\\\\n2. Left-Arc_r\\qquad \\sigma\\left|w_{i}\\right| w_{j}, \\beta, A \\rightarrow\n\\sigma | w_{j}, \\beta, \\operatorname{A}\\mathop{\\cup} \\left\\{r\\left(w_{j}, w_{i}\\right)\\right\\}\n\\\\\n3. Right-Arc_r \\qquad \\sigma\\left[w_{i} | w_{j}, \\beta, A \\rightarrow\\right.\n\n\\sigma | w_{i}, \\beta, \\operatorname{A}\\mathop{\\cup}\\left\\{r\\left(w_{i}, w_{j}\\right)\\right\\}\n\\\\\nFinish: \\beta=\\varnothing\n$$\n\n### 2005 MaltParser, Nivre and Hall 2005\n\nEach action is predicted by a discriminative classifier over each legal move(like softmax classifier)\n\n- Max of 3 untyped choices, max of |R| * 2 + 1 when typed\n- Features: top of stack word, POS; first in the buffer word, POS\n\nIt provides very **fast linear time parsing** with fractionally lower accuracy\n\n### Evaluation of Dependency Parsing: (labeled) dependency accuracy\n\n![image-20200205111717409.png](https://i.loli.net/2020/04/04/DRoBKOlQ2tEhA9W.png)\n\n## Neural dependency parsing\n\n### Why train a neural dependency parsing?\n\nProblem #1: sparse\n\nProblem #2: incomplete\n\nProblem #3: expensive computation(95%+ of parsing time is consumed by feature computation)\n\n### 2014, chen and manning\n\nEnglish parsing to Stanford Dependencies:\n\n- Unlabeled attachment score = head\n- Labeled attachment score = head and label\n\n### Google \n\nbigger, deeper networks + beam search + CRF-style inference\n\n### Graph-based neural dependency parsing\n\nover 95%\n\n","categories":["NLP"]},{"title":"Stanford CS224n Natural Language Processing Course4","url":"/2020/02/03/CS224n Classnotes Course4/","content":"\n# Course 4 - Backpropagation and computation graphs\n\n## Matrix gradients for our simple neural net and some tips\n\nChain rule:\n$$\n\\begin{aligned}\n\\boldsymbol{s}=&\\boldsymbol{u}^{T} \\boldsymbol{h} \\\\\n\\boldsymbol{h}=& f(\\boldsymbol{z}) \\\\ \\boldsymbol{z}=& W \\boldsymbol{x}+\\boldsymbol{b} \\end{aligned}\n$$\n\n$$\n\\begin{aligned} \n\\frac{\\partial s}{\\partial \\boldsymbol{W}}=&\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\\n=& \\delta\\frac{\\partial z}{\\partial W}\\\\\n=& \\delta \\frac{\\partial}{\\partial W} Wx + b \\\\\n=& \\delta^{T}x+b\n\\end{aligned}\n$$\n\n**Question** Should I use available \"pre-trained\" word vectors?\n\n**Answer** almost always\n\n**Question** Should I update(\"fine tune\") my own word vectors?\n\n**Answer** \n\n- If you only have a **small** training data set, **don't** train the word vectors\n- If you have a **large** dataset, it probably will work better to **train = update = fine-tune** word vectors to the task\n\n## Computation graphs and backpropagation\n\n```python\nclass MultiplyGate(object):\n    def forward(x,y):\n        z = x*y\n        self.x = x\n        self.y = y\n        return z\n    def backward(dz):\n        dx = self.y * dz\n        dy = self.x * dz\n        return [dx,dy]\n```\n\n## Stuff you should know\n\n### Regularization to prevent overfitting\n\n$$\nJ(\\theta)=\\frac{1}{N} \\sum_{i=1}^{N}-\\log \\left(\\frac{e^{f_{y_{i}}}}{\\sum_{c=1}^{C} e^{f_{c}}}\\right)\\left[+\\lambda \\sum_{k} \\theta_{k}^{2}\\right]\n$$\n\nPrevents **overfitting** when we have a lot of features\n\n### Vectorization\n\n### Nonlinearities\n\nlogistic(\"sigmoid\")\n$$\nf(z)=\\frac{1}{1+\\exp (-z)}\n$$\ntanh\n$$\nf(z)=\\tanh (z)=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\\\\n\\tanh (z) = 2\\operatorname{logistic}(2z) - 1\n$$\nhard tanh\n$$\n\\operatorname{HardTanh} (x)=\\left\\{\\begin{array}{cl}{-1} & {\\text { if } x<-1} \\\\ {x} & {\\text { if }-1<=x<=1} \\\\ {1} & {\\text { if } x>1}\\end{array}\\right.\n$$\nReLU: \n$$\nrect(z) = \\max(z,0)\n$$\nLeaky ReLU/Parametric ReLU\n\n### Initialization\n\nXavier initialzation has variance inversely proportional to fan-in n_in and fan-out n_out:\n$$\n\\operatorname{Var}\\left(W_{i}\\right)=\\frac{2}{n_{\\mathrm{in}}+n_{\\mathrm{out}}}\n$$\n\n### Optimizers\n\nSGD/Adagrad/RMSprop/Adak/SparseAdam\n\n### Learning rates\n\n- You can just use a constant learning rate\n\n- Better results can generally be obtained by allowing learning rates to decrease as you train\n\n  - by hand: halve the learning rate every k epochs\n\n  - By a formula, for epoch t\n\n  - $$\n    lr = lr_0 e^{-kt}\n    $$\n\n  - Fancier method like cyclic learning rates(q.v.)\n\n- Fancier optimizers still use a learning rate but it may be an initial rate that the optimizer shrinks - so may be able to start high","categories":["NLP"]},{"title":"Stanford CS224n Natural Language Processing Course3","url":"/2020/02/02/CS224n Classnotes Course3/","content":"\n# Course 3 - Neural Networks\n\n## Classification review/introduction\n\nGenerally we have a **training dataset** consisting of **samples**\n$$\n\\left\\{x_{\\mathrm{i}}, y_{\\mathrm{i}}\\right\\}_{\\mathrm{i}=1}^{\\mathrm{N}}\n$$\nx_i are inputs, eg words, sentences, documents, etc\n\ny_j are labels we try to predict. \n\n- classes: sentiment, named entities, buy/sell decision\n- other words\n- later: multi-word sequences\n\nTraditional ML/Stats approach: assume x_i are fixed, train softmax/logistic regression weights *W in R(C,d)* to determine a decision boundary as in the picture. \n\n### Method: Training with softmax and cross-entropy loss\n\nFor each *x*, predict:\n$$\np(y | x)=\\frac{\\exp \\left(W_{y} \\cdot x\\right)}{\\sum_{c=1}^{C} \\exp \\left(W_{c} \\cdot x\\right)}\n$$\nFor each training example (x,y), to **maximize the probability of the correct class** *y*, or **minimize the negative log probability**\n$$\n-\\log p(y | x)=-\\log \\left(\\frac{\\exp \\left(f_{y}\\right)}{\\sum_{c=1}^{C} \\exp \\left(f_{c}\\right)}\\right)\n$$\nFYI: Cross entropy in the full dataset is \n$$\nJ(\\theta)=\\frac{1}{N} \\sum_{i=1}^{N}-\\log \\left(\\frac{e^{f_{y_{i}}}}{\\sum_{c=1}^{C} e^{f_{c}}}\\right)\n$$\n\n\n## Neural networks introduction\n\nCommonly in NLP deep learning:\n\n- We learn **both** W **and** word vectors X\n- We learn **both** conventional parameters **and** representations\n\n$$\n\\nabla_{\\theta} J(\\theta)=\\left[\\begin{array}{c}{\\nabla_{W_{1}}} \\\\ {\\vdots} \\\\ {\\nabla_{W_{d}}} \\\\ {\\nabla_{x_{a a r d v a r k}}} \\\\ {\\vdots} \\\\ {\\nabla_{x_{z e b r a}}}\\end{array}\\right] \\in \\mathbb{R}^{C d+V d}\n$$\n\n## Named Entity Recognition\n\nThe task: **find** and **classify** name in text.\n\n### NER on word sequences\n\nWe predict entities by classifying words in context and then extracting entities as word subsequences.\n\nBIAO encoding \n\n### Why might NER be hard?\n\n- hard to work out boundaries of entity\n- hard to know if something is an entity\n- hard to know class of unknown/novel entity\n- entity class is ambiguous and depends on context\n\n## Binary true vs. corrupted word window classification\n\n### Binary word window classification\n\nProblem: ambiguity\n\nIdea: **classify a word in its context window** of neighboring words.\n\nSolution: train softmax classifer to classify a center word by taking **concatenation of word vectors surrounding it** in a window\n$$\n(x_1, x_2, \\cdots, x_n)\n$$\n","categories":["NLP"]},{"title":"Stanford CS224n Natural Language Processing Course2","url":"/2020/02/01/CS224n Classnotes Course2/","content":"\n# Course 2 - Word Vectors and Word Senses\n\nFirstly, some analogy by calculating the linear space similarity by word vectors.\n\n![image-20200201062227420.png](https://i.loli.net/2020/04/04/CSBItLwc53PdYuX.png)\n\nDefinition of Code:\n\n```python\ndef analogy(x1, x2, y1):\n    result = model.most_similar(positive=[y1,x2], negative=[x1])\n    return result[0][0]\n```\n\nSome Implementations:\n\n```python\nanalogy('japan','japanese','australia') # australian\nanalogy('man','king','woman') # queen\nanalogy('australia','beer','france') # champagne\n```\n\nNext thing: **PCA Scatterplot**\n\n```python\ndef display_pca_scatterplot(model, words=None, sample=0):\n    if words == None:\n        if sample > 0:\n            words = np.random.choice(list(model.vocab.keys()), sample)\n        else:\n            words = [ word for word in model.vocab ]\n        \n        word_vectors = np.array([model[w] for w in words])\n        \n        twodim = PCA().fit_transform(word_vectors)[:,:2]\n        \n        plt.figure(figsize=(6,6))\n        plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n        for word, (x,y) in zip(words, twodim):\n            plt.text(x+0.05, y+0.05, word)\n```\n\n##  Word Vectors and word2vec\n\n### 1. Review: Main idea of word2vec\n\n- Iterate through each word of the whole corpus\n- Predict surrounding words using word vectors\n\n$$\nP(o | c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\n$$\n\n## Optimization Basics: Gradient Descent(cs229 Optimization)\n\nIdea: for current value of theta, calculate gradient of J(theta), then take **small step in the direction of negative gradient**. \n\n**Update equation**(in matrix notation)\n$$\n\\theta^{n e w}=\\theta^{o l d}-\\alpha \\nabla_{\\theta} J(\\theta)\n$$\n**Update equation**(in a single parameter)\n$$\n\\theta_{j}^{n e w}=\\theta_{j}^{o l d}-\\alpha \\frac{\\partial}{\\partial \\theta_{j}^{o l d}} J(\\theta)\n$$\nAlgorithm:\n\n```python\nwhile True:\n    theta_grad = evaluate_gradient(J, corpus, theta)\n    theta = theta - alpha * theta_grad\n```\n\n### Stochastic Gradient Descent(SGD)\n\nSolution: Repeatedly sample windows, and update after each one\n\nAlgorithm\n\n```python\nwhile True:\n    window = sample_window(corpus)\n    theta_grad = evaluate_gradient(J, window, theta)\n    theta = theta - alpha * theta_grad\n```\n\n### Stochastic Gradient with word vectors:\n\nIteratively take gradients at each such window for SGD\n\nBut in each window, we only have at most *2m+1* words, so it's very sparse!\n$$\n\\nabla_{\\theta} J_{t}(\\theta)=\\left[\\begin{array}{c}{0} \\\\ {\\vdots} \\\\ {\\nabla_{v_{v i k e}}} \\\\ {\\vdots} \\\\ {0} \\\\ {\\vdots} \\\\ {\\nabla_{u_{\\text {learning}}}} \\\\ {\\vdots}\\end{array}\\right] \\in \\mathbb{R}^{2 d V}\n$$\nSolution: \n\n- need sparse matrix update operations to only update certain rows of full embedding matrices *U* and *V*\n- need to keep around a hash for word vectors\n\n### Word2Vec: More details\n\nTwo model variants:\n\n- Skip-grams(SG): Predict context words given center word\n- Continuous Bag of Words(CBOW): Predict center word from context words\n\n=> **Skip-gram model**\n\nAdditional efficiency in training: \n\n- Negative sampling\n\nPaper: \"Distributed Representations of Words and Phrases and their Compositionality\"(Mikolov et al.2013)\n\n**Overall objective function**(maximize):\n$$\nJ(\\theta)=\\frac{1}{T} \\sum_{t=1}^{T} J_{t}(\\theta)\\\\\nJ_{t}(\\theta)=\\log \\sigma\\left(u_{o}^{T} v_{c}\\right)+\\sum_{i=1}^{k} \\mathbb{E}_{j \\sim P(w)}\\left[\\log \\sigma\\left(-u_{j}^{T} v_{c}\\right)\\right]\n$$\nThe sigmoid function:\n$$\n\\sigma(x)=\\frac{1}{1+e^{-x}}\n$$\nSo we maximize the probability of two words co-occurring in the first log\n$$\nJ_{n e g-s a m p l e}\\left(\\boldsymbol{o}, \\boldsymbol{v}_{c}, \\boldsymbol{U}\\right)=-\\log \\left(\\sigma\\left(\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}\\right)\\right)-\\sum_{k=1}^{K} \\log \\left(\\sigma\\left(-\\boldsymbol{u}_{k}^{\\top} \\boldsymbol{v}_{c}\\right)\\right)\n$$\nMaximize probability that real outside word appears, minimize prob. that random word appear around center word.\n\nThe unigram distribution U(w)\n$$\nP(w)=U(w)^{3 / 4} / Z\n$$\nraised to the 3/4 power, which makes less frequent words be sampled more often.\n\n## Can we capture this essence more effectively by counting?\n\nWith a co-occurrence matrix *X*\n\n- 2 options: windows vs. full document\n- Window: Similar to word2vec, use window around each word -> captures both syntactic(POS) and semantic information\n- Word-document co-occurrence matrix will give general topic leading to *Latent Semantic Analysis*\n\nModels are less robust due to increase in size with vocabulary.\n\nSolution: Low dimensional vectors\n\nIdea: store most information in a fixed, small number of dimensions\n\n### Singular Value Decomposition of co-occurrence matrix *X* - SVD\n\n$$\nX = U \\Sigma V^{\\top}\n$$\n\n### Hacks to X (several used in Rohde et al. 2005)\n\nProblem: function words(*the, he, has*) are too frequent -> syntax has too much impact\n\nSolution\n\n-  min(X,t) t~= 100, Ignore them all\n-  Use Pearson correlations instead of counts, then set negative values to 0\n\n## The GloVe model of word vectors\n\n### Crucial insight\n\nRatios of co-occurrence probabilities can encode meaning components\n$$\nw_{i} \\cdot w_{j}=\\log P(i | j)\\\\\nJ=\\sum_{i, j=1}^{V} f\\left(X_{i j}\\right)\\left(w_{i}^{T} \\tilde{w}_{j}+b_{i}+\\tilde{b}_{j}-\\log X_{i j}\\right)^{2}\n$$\n**Merits**\n\n- Fast training\n- Scalable to hug corpora\n- Good performance even with small corpus and small vectors\n\n## Evaluating word vectors\n\n### Intrinsic\n\n- Evaluation on a specific/intermediate subtask\n- Fast to compute\n- Helps to understand that system\n- Not clear if helpful\n\nMethods: \n\n1. Cosine distance of word vectors\n2. Word vector distances and their correlation with human judgments\n\n### Extrinsic\n\n- Evaluation on a real task\n- Can take a long time to compute accuracy\n- Unclear if subsystem is the problem or its interaction\n- If replacing exactly one subsystem with another improves accuracy\n\nExample: NER\n\n## Word senses\n\n#### Word senses and word sense ambiguity\n\nMost words have lots of meanings, especially common words. ","categories":["NLP"]},{"title":"Stanford CS224n Natural Language Processing Course1","url":"/2020/01/31/CS224n Classnotes Course1/","content":"\n[**Course Link(including Slides and Materials)**](http://web.stanford.edu/class/cs224n/index.html#schedule)\n\n[**Online Video Link**](http://onlinehub.stanford.edu/cs224)\n\n[**Online Video Link in Chinese**](https://www.bilibili.com/video/av46216519)\n\n# Course 1 - Introduction and Word Vectors\n\nLanguage isn't a formal system. Language is glorious chaos. *by Chris Manning*\n\n**Target**\n\n- Modern methods: Recurrent networks, attentions\n- Big Picture\n- Build system in PyTorch: Word meaning, dependency parsing, machine translation, question answering.\n\n## 1. how do we represent the meaning of a word?\n\nsignifier(symbol) <=> signified(idea or thing) = denotational semantics\n\n## 2. how do we have usable meaning in a computer?\n\n**Common Solution** Use e.g. WordNet containing lists of **synonym sets** and **hypernyms**\n\n**Problems** \n\n- missing nuance: proficient = good in some contexts\n- new meanings of words: wicked, badass, ninja\n- Subjective\n- Requires human labor to create and adapt\n- Can't compute accurate and word similarity\n\n### Representing words as discrete symbols\n\n**one-hot vectors** => **encode similarity in the vector themselves**\n\n**Distributional semantics**: A word's meaning is given by the words that frequently appear close-by.\n\n**Word Vectors**: a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts.\n\nNote: Word Vectors = Word Embeddings = Word Representations. All distributed representation\n$$\nbanking = \\left[\n \\begin{matrix}\n   0.286 \\\\\n   0.792  \\\\\n   -0.177 \\\\\n   -0.107 \\\\\n   0.109 \\\\\n   -0.542\\\\\n   0.349\\\\\n   0.271\\\\\n   0.487\n  \\end{matrix}\n  \\right] \\tag{1}\n$$\n\n## 3. Word2Vec: Overview\n\nIdea:\n\n- a large corpus of text\n- every word in a fixed vocabulary is represented by a **vector**\n- go through each position *t* in the text, which has a center word *c* and context words *o*\n- Use the **similarity of the word vectors** for *c* and *o* to **calculate the probability** of *o* given *c*\n- **Keep Adjusting the word vectors** to maximize this probability\n\n$$\nlikelihood = L(\\theta) = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m \\atop j \\neq 0} P\\left(w_{t+j} | w_{t} ; \\theta\\right)\n$$\n\nNote: theta is all variables to be optimized\n\n### Objective function J\n\nsometimes called **cost / loss function**\n$$\nJ(\\theta)=-\\frac{1}{T} \\log L(\\theta)=-\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m} \\log P\\left(w_{t+j} | w_{t} ; \\theta\\right)\n$$\n**Minimizing objective function <=> Maximizing predictive accuracy**\n\n*Question: How to calculate P?*\n\n*Answer: We use two vectors per word w*\n\n- `v_w` when *w* is a center word\n- `u_w` when *w* is a context word\n\nThen for a center word *c* and a context word *o*:\n\n### Prediction function\n\n$$\nP(o | c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\n$$\n\n### To train the model: Compute all vector gradients\n\n- Recall: theta represents **all** model parameters, in one long vector\n\n- In our cases\n\n$$\n\\theta=\\left[\\begin{array}{l}{v_{\\text {aardvark}}} \\\\ {v_{a}} \\\\ {\\vdots} \\\\ {v_{z e b r a}} \\\\ {u_{\\text {aardvark}}} \\\\ {u_{a}} \\\\ {\\vdots} \\\\ {u_{z e b r a}}\\end{array}\\right] \\in \\mathbb{R}^{2 d V}\n$$\n\n- Remember: every word has two vectors\n\n- We optimize these parameters by walking down the gradient\n\n\n\n\n","categories":["NLP"]},{"title":"Leetcode MySQL cases","url":"/2020/01/30/MYSQL_Leetcode/","content":"\n## Problem 175 - Combine Two Tables\n\n### Question:\n\nTable: `Person`\n\n```\n+-------------+---------+\n| Column Name | Type    |\n+-------------+---------+\n| PersonId    | int     |\n| FirstName   | varchar |\n| LastName    | varchar |\n+-------------+---------+\nPersonId is the primary key column for this table.\n```\n\nTable: `Address`\n\n```\n+-------------+---------+\n| Column Name | Type    |\n+-------------+---------+\n| AddressId   | int     |\n| PersonId    | int     |\n| City        | varchar |\n| State       | varchar |\n+-------------+---------+\nAddressId is the primary key column for this table.\n```\n\nWrite a SQL query for a report that provides the following information for each person in the Person table, regardless if there is an address for each of those people:\n\n```\nFirstName, LastName, City, State\n```\n\n\n\n### Solution\n\nThis is a easy question that interferes with **LEFT JOIN** technique, FYI, please notice the word **ON**\n\n```mysql\nSELECT FirstName, LastName, City, State\nFROM \nPerson a\nLEFT JOIN\nAddress b\nON a.PersonId = b.PersonId;\n```\n\n\n\n## Problem 176 - Second Highest Salary\n\n### Question:\n\nWrite a SQL query to get the second highest salary from the `Employee` table.\n\n```\n+----+--------+\n| Id | Salary |\n+----+--------+\n| 1  | 100    |\n| 2  | 200    |\n| 3  | 300    |\n+----+--------+\n```\n\nFor example, given the above Employee table, the query should return `200` as the second highest salary. If there is no second highest salary, then the query should return `null`.\n\n```\n+---------------------+\n| SecondHighestSalary |\n+---------------------+\n| 200                 |\n+---------------------+\n```\n\n### Solution:\n\nrecommended way.\n\nRuntime: faster than 92%\n\nMemory: Less than 100%\n\n```mysql\nSELECT MAX(salary) AS SecondHighestSalary\nFROM Employee\nWHERE salary < (SELECT MAX(salary) FROM Employee);\n```\n\nother solution which might be slower for using the **DISTINCT** \n\nRuntime: faster than 39%\n\nMemory: Less than 100%\n\n```mysql\nSELECT \nCASE WHEN COUNT(a.SecondHighestSalary) > 0 THEN a.SecondHighestSalary ELSE null END AS SecondHighestSalary\nFROM\n(SELECT \nDISTINCT Salary AS SecondHighestSalary \nFROM\nEmployee ORDER BY Salary DESC LIMIT 1,1) a;\n```\n\n## Problem 178 - Rank Scores\n\n### Question:\n\nWrite a SQL query to rank scores. If there is a tie between two scores, both should have the same ranking. Note that after a tie, the next ranking number should be the next consecutive integer value. In other words, there should be no \"holes\" between ranks.\n\n```\n+----+-------+\n| Id | Score |\n+----+-------+\n| 1  | 3.50  |\n| 2  | 3.65  |\n| 3  | 4.00  |\n| 4  | 3.85  |\n| 5  | 4.00  |\n| 6  | 3.65  |\n+----+-------+\n```\n\nFor example, given the above `Scores` table, your query should generate the following report (order by highest score):\n\n```\n+-------+------+\n| Score | Rank |\n+-------+------+\n| 4.00  | 1    |\n| 4.00  | 1    |\n| 3.85  | 2    |\n| 3.65  | 3    |\n| 3.65  | 3    |\n| 3.50  | 4    |\n+-------+------+\n```\n\n### Solution:\n\n```mysql\n\n```\n\n\n\n## Problem 181 - Employees Earning More Than Their Managers\n\n### Question:\n\nThe `Employee` table holds all employees including their managers. Every employee has an Id, and there is also a column for the manager Id.\n\n```\n+----+-------+--------+-----------+\n| Id | Name  | Salary | ManagerId |\n+----+-------+--------+-----------+\n| 1  | Joe   | 70000  | 3         |\n| 2  | Henry | 80000  | 4         |\n| 3  | Sam   | 60000  | NULL      |\n| 4  | Max   | 90000  | NULL      |\n+----+-------+--------+-----------+\n```\n\nGiven the `Employee` table, write a SQL query that finds out employees who earn more than their managers. For the above table, Joe is the only employee who earns more than his manager.\n\n```\n+----------+\n| Employee |\n+----------+\n| Joe      |\n+----------+\n```\n\n### Solution:\n\nRuntime: faster than 93.96%\n\nMemory: Less than 100%\n\n```mysql\nSELECT a.Name AS Employee\nFROM\n(SELECT * FROM Employee WHERE ManagerId IS NOT NULL) a\nLEFT JOIN\nEmployee b\nON a.ManagerId = b.Id\nWHERE a.Salary > b.Salary;\n```\n\n","categories":["Leetcode"]},{"title":"SQL cheetsheet","url":"/2019/08/29/SQL-cheetsheet/","content":"## SQL Cheetsheet\nIt's more convenient to store the data in your MySQL instead of Hive to improve the efficiency and facilitate retrieval of the data if the amount of data is not that large. Here are some tips about SQL, according to [IBM Databases and SQL for Data Science](https://www.coursera.org/learn/sql-data-science/)\n\n------\n\n### String Patterns, Ranges, Sorting and Grouping\n\n**using string pattern**: like '%%'\n\n**using a range**: between ... and ...\n\n**using a set of values**: in ('', '')\n\n**Sorting**: order by ... / order by ... desc / order by 2(column number)\n\n**Eliminating Duplicates**: distinct \n\nIt seems like you don't need to write distinct(), distinct also works.\n\n**Group by clause**\n\n**Restricting the result set - Having clause**: Having \n\nworks only with the GROUP BY clause.\n\n------\n\n### Functions, Sub-Queries, Multiple Tables\n\n**Aggregate Functions**: sum(), min(), max(), avg()\n\n**Scaler and String functions**: round(), length(), ucase, lcase\n\n**Date and Time functions**: year(), month(), day(), dayofmonth(), dayofweek(), dayofyear(), week(), hour(), minute(), second()\n\n**Date or time arithmetic**: + 1 Days, CURRENT_DATE, CURRENT_TIME\n\n**Accessing multiple tables with with Implicit join**: \n\nselect * from employees E, departments D where E.DEP_ID = D.DEPT_ID_DEP;","categories":["Computer Science"]},{"title":"Discovery towards Web Scrawler","url":"/2019/07/14/webscrawler/","content":"\n### Web Scrawler is dangerous\n\nFirst and foremost, do not touch web scrawler if you are 100% sure that you wanna do this.\n\n### Beginning\n\nThe target website is enlightent, a third-party data website which have data of my company and component. \n\nWe need to do some background research first. The problems I encountered are listed:\n\n- Need to log in with WeChat account by QR code\n- Simulate click (by package selenium)\n- It's a dynamic website, you need to wait for its information loaded (by package time)\n- Write into MySQL (by package Pymysql)\n\n### Process\n\n##### First problem cannot be solve due to the security of WeChat.\n\n##### Second problem\n\nStep 1: Find the pattern in html. Using chrome, just ctrl+u or ctrl+shift+i. It needs your patience to find the thing you want. If you mistaken the pattern, you cannot get the information you want\n\nStep 2: Choose the function: by_path or by_class. The tricky point is that if there is only one class, it's okay to use by_class, if there are more than two classes, selenium would choose the first class as your output. As a result, I choose by_path\n\nStep 3: Install chrome driver according to your chrome version. Be sure to download into /anaconda3/lib/site-packages.\n\n```python\n# Choose daily\ntime.sleep(5)\ndriver.find_elements_by_id(\"rank-date-btn\")[0].click()\n# Choose year\ntime.sleep(1)\ndriver.find_element_by_class_name(\"datepicker-switch\").click()\n# Choose target month\ntime.sleep(1)\n# driver.find_element_by_class_name(\"month\").click()\nmonth_url = \"/html/body/div[2]/div[3]/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[1]/div/div/div/div[2]/table/tbody/tr/td/span[%s]\" % (l+1)\ndriver.find_element_by_xpath(month_url).click()\n# Choose target Date\ntime.sleep(1)\n# day = driver.find_element_by_class_name(\"day\").click()\nxpath = \"/html/body/div[2]/div[3]/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[1]/div/div/div/div/table/tbody/tr[%d]/td[%d]\" % ((start_date) // 7 + 1, (start_date) % 7 + 1)\ndriver.find_element_by_xpath(xpath).click()\n# Press enter\ntime.sleep(1)\ndriver.find_element_by_id(\"choose-rank\").click()\ntime.sleep(1)\n\n```\n\n##### Third Problem: Data Processing\n\n```python\nalbum_separately = string_list[j][string_list[j].find('data-name='):string_list[j].find('data-channeltype=\"tv\"')]\nif j != 0: \n    album.append(album_separately.replace('data-name=','').replace('\"',''))\npercentage_separately = string_list[j][string_list[j].find('<td class=\"sort rank-playTimesPredicted active\" style=\"\"><span>'):string_list[j].find('</span></td><td class=\"rank-playTimes\" style=\"\">')]\npercentage.append(percentage_separately.replace('<td class=\"sort rank-playTimesPredicted active\" style=\"\"><span>',''))\nclick_separately = string_list[j][string_list[j].find('</td><td class=\"rank-playTimes\" style=\"\"><span>'):string_list[j].find('</span><span class=\"star-playtimes\">')]\nif len(click_separately) >= 10:\n    click_separately = string_list[j][string_list[j].find('</td><td class=\"rank-playTimes\" style=\"\"><span>'):string_list[j].find('</span></td><td class=\"rank-average m-change\" style=\"\">')]\nclick.append(click_separately.replace('</td><td class=\"rank-playTimes\" style=\"\"><span>','').replace('</span><span class=\"star-playtimes\">',''))\n```\n\n##### Fourth problem: Log into your MySQL and use python like MySQL!\n\n```python\ndb = DB('your database')\ndb.insert(dataset)\n```\n\n \n\n### Take Care! Be sure to add time.sleep() when you do it!","categories":["Computer Science"]},{"title":"Random Forest Classification from the bottom layer","url":"/2019/05/05/RandomForest/","content":"import packages we need\n\n    import random\n    import numpy as np\n    import sys\n\nThe feature list is an array of the possible feature indicies to use. This prevents splitting on the same feature multiple times and runs a rough C45 algorithm.\n\nIn pseudocode, the general algorithm for building decision trees is:\n\n1.Check for base cases\n2.For each attribute a\n3.Find the normalized information gain ratio from splitting on a\n4.Let a_best be the attribute with the highest normalized information gain\n5.Create a decision node that splits on a_best\n6.Recur on the sublists obtained by splitting on a_best, and add those nodes as children of node\n\n    class TreeNode:\n        def __init__(self, dataSet, featureList, parent=None):\n            self.featureNumber = None  #This is the trained index of the feature to split on\n            self.featureList = featureList \n            self.threshold = None     #This is the trained threshold of the feature to split on\n            self.leftChild = None\n            self.rightChild = None\n            self.dataSet = dataSet\n            self.parent = parent\n    \n        def c45Train(self):\n            if(self.dataSet.isPure()):\n                #gets the label of the first data instance and makes a leaf node\n                #classifying it. \n                label = self.dataSet.getData()[0].getLabel()\n                leaf = LeafNode(label)\n                return leaf\n            #If there are no more features in the feature list\n            if len(self.featureList) == 0:\n                labels = self.dataSet.getLabelStatistics()\n                bestLabel = None\n                mostTimes = 0\n    \n                for key in labels:\n                    if labels[key] > mostTimes:\n                        bestLabel = key\n                        mostTimes = labels[key]\n                #Make the leaf node with the best label\n                leaf = LeafNode(bestLabel)\n                return leaf\n    \n            #Check all of the features for the split with the most \n            #information gain. Use that split.\n            currentEntropy = self.dataSet.getEntropy()\n            currentLength = self.dataSet.getLength()\n            infoGain = -1 * sys.maxsize\n            bestFeature = 0\n            bestLeft = None\n            bestRight = None\n            bestThreshold = 0\n    \n            #Feature Bagging, Random subspace\n            num = int(np.ceil(np.sqrt(len(self.featureList))))\n            featureSubset = random.sample(self.featureList, num)\n    \n            for featureIndex in featureSubset:\n                #Calculate the threshold to use for that feature\n                threshold = self.dataSet.betterThreshold(featureIndex)\n    \n                (leftSet, rightSet) = self.dataSet.splitOn(featureIndex, threshold)\n    \n                leftEntropy = leftSet.getEntropy()\n                rightEntropy = rightSet.getEntropy()\n                #Weighted entropy for this split\n                newEntropy = (leftSet.getLength() / currentLength) * leftEntropy + (rightSet.getLength() / currentLength) * rightEntropy\n                #Calculate the gain for this test\n                newIG = currentEntropy - newEntropy\n    \n                if(newIG > infoGain):\n                    #Update the best stuff\n                    infoGain = newIG\n                    bestLeft = leftSet\n                    bestRight = rightSet\n                    bestFeature = featureIndex\n                    bestThreshold = threshold\n    \n            newFeatureList = list(self.featureList)\n            newFeatureList.remove(bestFeature)\n    \n            #Another base case, if there are no good features to split on\n            if bestLeft.getLength() == 0 or bestRight.getLength() == 0:\n                labels = self.dataSet.getLabelStatistics()\n                bestLabel = None\n                mostTimes = 0\n    \n                for key in labels:\n                    if labels[key] > mostTimes:\n                        bestLabel = key\n                        mostTimes = labels[key]\n                #Make the leaf node with the best label\n                leaf = LeafNode(bestLabel)\n                return leaf\n    \n            self.threshold = bestThreshold\n            self.featureNumber = bestFeature\n    \n            leftChild = TreeNode(bestLeft, newFeatureList, self)\n            rightChild = TreeNode(bestRight, newFeatureList, self)\n    \n            self.leftChild = leftChild.c45Train()\n            self.rightChild = rightChild.c45Train()\n    \n            return self\n            \n        def __str__(self):\n            return str(self.featureList)\n    \n        def __repr__(self):\n            return self.__str__()\n                    \n        def classify(self, sample):\n            '''\n            Recursivly traverse the tree to classify the sample that is passed in. \n            '''\n    \n            value = sample.getFeatures()[self.featureNumber]\n    \n            if(value < self.threshold):\n                #Continue down the left child    \n                return self.leftChild.classify(sample)\n    \n            else:\n                #continue down the right child\n                return self.rightChild.classify(sample)\n\n\n    class LeafNode:\n        '''\n        A leaf node is a node that just has a classification \n        and is used to cap off a tree.\n        '''\n    \n        def __init__(self, classification):\n            self.classification = classification\n    \n        def classify(self, sample):\n            #A leaf node simply is a classification, return that\n            #This is the base case of the classify recursive function for TreeNodes\n            return self.classification\n\n\n    class C45Tree:\n        '''\n        A tree contains a root node and from here\n        does the training and classification. Tree objects also\n        are responsible for having the data that they use to train.\n        '''\n    \n        def __init__(self, data):\n            self.rootNode = None\n            self.data = data\n    \n        def train(self):\n            '''\n            Trains a decision tree classifier on data set passed in. \n            The data set should contain a good mix of each class to be\n            classified.\n            '''\n            length = self.data.getFeatureLength()\n            featureIndices = range(length)\n            self.rootNode = TreeNode(self.data, featureIndices)\n            self.rootNode.c45Train()\n    \n        def classify(self, sample):\n            '''\n            Classify a sample based off of this trained tree.\n            '''\n    \n            return self.rootNode.classify(sample)\n\nThen we introduce the RandomForest algorithm\n\n\n    from C45Tree import C45Tree\n\n\n    class RandomForest(object):\n        \"\"\"A random forest object with the default of using a C45Tree \n        for each of the trees in the random forest. To train the forest\n        create an instance of it then call train on a TraingData object\"\"\"\n        def __init__(self, data, numberOfTrees=100):\n            '''\n            Initialize the random forest. \n            Each tree has a bag of the data associated with it.\n            '''\n            self.data = data    #The data that the trees will be trained on\n            self.numberOfTrees = numberOfTrees\n            self.forest = []\n    \n            for i in range(numberOfTrees):\n                bag = data.getBag()\n                self.forest.append(C45Tree(bag))\n    \n        def train(self):\n            '''\n            Train the random forest trees.\n            '''\n            for tree in self.forest:\n                tree.train()\n    \n        def classify(self, sample):\n            '''\n            Classify a data sample by polling the trees.\n            '''\n    \n            #Create an empty dictionary\n            votes = {}\n            #Tally the votes, for each tree classify the sample\n            for tree in self.forest:\n                label = tree.classify(sample)\n                if label in votes:\n                    votes[label] += 1\n                else:\n                    votes[label] = 1\n    \n            bestLabel = None\n            mostTimes = 0\n            #Find the label with the most votes\n            for key in votes:\n                if votes[key] > mostTimes:\n                    bestLabel = key\n                    mostTimes = votes[key]\n    \n            #Return the most popular label\n            return bestLabel      ","categories":["Computer Science"]},{"title":"Deep dive into KAGGLE","url":"/2019/04/16/kaggle数据集从入门到放弃/","content":"[click here to kaggle official website for this dataset](https://www.kaggle.com/ronitf/heart-disease-uci)\n\n## Context\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n\n## Content\n\n### Attribute Information: \n> 1. age \n> 2. sex \n> 3. chest pain type (4 values) \n> 4. resting blood pressure \n> 5. serum cholestoral in mg/dl \n> 6. fasting blood sugar > 120 mg/dl\n> 7. resting electrocardiographic results (values 0,1,2)\n> 8. maximum heart rate achieved \n> 9. exercise induced angina \n> 10. oldpeak = ST depression induced by exercise relative to rest \n> 11. the slope of the peak exercise ST segment \n> 12. number of major vessels (0-3) colored by flourosopy \n> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values. One file has been \"processed\", that one containing the Cleveland database. All four unprocessed files also exist in this directory.\n\nTo see Test Costs (donated by Peter Turney), please see the folder \"Costs\"\n\n# Introduction\nTo solve this problem, here we introduce jupyter notebook by python, implementing random forest to give us a brief scope of this dataset.\nFirstly, we import the packages needed.\n\n\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns #for plotting\n    from sklearn.ensemble import RandomForestClassifier #for the model\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.tree import export_graphviz #plot tree\n    from sklearn.metrics import roc_curve, auc #for model evaluation\n    from sklearn.metrics import classification_report #for model evaluation\n    from sklearn.metrics import confusion_matrix #for model evaluation\n    from sklearn.model_selection import train_test_split #for data splitting\n    import eli5 #for purmutation importance\n    from eli5.sklearn import PermutationImportance\n    import shap #for SHAP values\n    from pdpbox import pdp, info_plots #for partial plots\n    np.random.seed(123) #ensure reproducibility\n    pd.options.mode.chained_assignment = None  #hide any pandas warnings\n\n# The data\nNext, we load the data.\n\n    dt = pd.read_csv(\"../input/heart.csv\")\n\nIt's a clean, easy to understand set of data. However, the meaning of some of the column headers are not obvious. Here's what they mean,\n\n**age**: The person's age in years\n**sex**: The person's sex (1 = male, 0 = female)\n**cp**: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n**trestbps**: The person's resting blood pressure (mm Hg on admission to the hospital)\n**chol**: The person's cholesterol measurement in mg/dl\n**fbs**: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n**restecg**: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n**thalach**: The person's maximum heart rate achieved\n**exang**: Exercise induced angina (1 = yes; 0 = no)\n**oldpeak**: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n**slope**: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n**ca**: The number of major vessels (0-3)\n**thal**: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n**target**: Heart disease (0 = no, 1 = yes)\n\n**Diagnosis**: The diagnosis of heart disease is done on a combination of clinical signs and test results. The types of tests run will be chosen on the basis of what the physician thinks is going on 1, ranging from electrocardiograms and cardiac computerized tomography (CT) scans, to blood tests and exercise stress tests 2.\n\nLooking at information of heart disease risk factors led me to the following: high cholesterol, high blood pressure, diabetes, weight, family history and smoking 3. According to another source 4, the major factors that can't be changed are: increasing age, male gender and heredity. Note that thalassemia, one of the variables in this dataset, is heredity. Major factors that can be modified are: Smoking, high cholesterol, high blood pressure, physical inactivity, and being overweight and having diabetes. Other factors include stress, alcohol and poor diet/nutrition.\n\nI can see no reference to the 'number of major vessels', but given that the definition of heart disease is \"...what happens when your heart's blood supply is blocked or interrupted by a build-up of fatty substances in the coronary arteries\", it seems logical the more major vessels is a good thing, and therefore will reduce the probability of heart disease.\n\nGiven the above, I would hypothesis that, if the model has some predictive ability, we'll see these factors standing out as the most important.\n\nDeclare some of the columns to make the indication clear\n\n    dt.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']\n\n\n    dt['sex'][dt['sex'] == 0] = 'female'\n    dt['sex'][dt['sex'] == 1] = 'male'\n    \n    dt['chest_pain_type'][dt['chest_pain_type'] == 1] = 'typical angina'\n    dt['chest_pain_type'][dt['chest_pain_type'] == 2] = 'atypical angina'\n    dt['chest_pain_type'][dt['chest_pain_type'] == 3] = 'non-anginal pain'\n    dt['chest_pain_type'][dt['chest_pain_type'] == 4] = 'asymptomatic'\n    \n    dt['fasting_blood_sugar'][dt['fasting_blood_sugar'] == 0] = 'lower than 120mg/ml'\n    dt['fasting_blood_sugar'][dt['fasting_blood_sugar'] == 1] = 'greater than 120mg/ml'\n    \n    dt['rest_ecg'][dt['rest_ecg'] == 0] = 'normal'\n    dt['rest_ecg'][dt['rest_ecg'] == 1] = 'ST-T wave abnormality'\n    dt['rest_ecg'][dt['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n    \n    dt['exercise_induced_angina'][dt['exercise_induced_angina'] == 0] = 'no'\n    dt['exercise_induced_angina'][dt['exercise_induced_angina'] == 1] = 'yes'\n    \n    dt['st_slope'][dt['st_slope'] == 1] = 'upsloping'\n    dt['st_slope'][dt['st_slope'] == 2] = 'flat'\n    dt['st_slope'][dt['st_slope'] == 3] = 'downsloping'\n    \n    dt['thalassemia'][dt['thalassemia'] == 1] = 'normal'\n    dt['thalassemia'][dt['thalassemia'] == 2] = 'fixed defect'\n    dt['thalassemia'][dt['thalassemia'] == 3] = 'reversable defect'\n\n# The Model\nDepending on package sklearn, we split the data - test data 70%, training data 30%\n\n    X_train, X_test, y_train, y_test = train_test_split(dt.drop('target', 1), dt['target'], test_size = .2, random_state=10) #split the data\n\nUsing random forest methodology\n\n    model = RandomForestClassifier(max_depth=5)\n    model.fit(X_train, y_train)\n\nplot the result of random forest\n\n    estimator = model.estimators_[1]\n    feature_names = [i for i in X_train.columns]\n    \n    y_train_str = y_train.astype('str')\n    y_train_str[y_train_str == '0'] = 'no disease'\n    y_train_str[y_train_str == '1'] = 'disease'\n    y_train_str = y_train_str.values\n\ncode from https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c\n\n    export_graphviz(estimator, out_file='tree.dot', \n                    feature_names = feature_names,\n                    class_names = y_train_str,\n                    rounded = True, proportion = True, \n                    label='root',\n                    precision = 2, filled = True)\n    \n    from subprocess import call\n    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n    \n    from IPython.display import Image\n    Image(filename = 'tree.png')\n\nEvaluate the model\n\n    y_predict = model.predict(X_test)\n    y_pred_quant = model.predict_proba(X_test)[:, 1]\n    y_pred_bin = model.predict(X_test)\n\nFit with Cnfusion model\n\n    confusion_matrix = confusion_matrix(y_test, y_pred_bin)\n\nHere we propose the sensitvity and specificity\n$$sensitivity = \\frac{TF}{TP+FN}$$\n$$specificity = \\frac{TN}{TN+FP}$$\n\n    total=sum(sum(confusion_matrix))\n    \n    sensitivity = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[1,0])\n    print('Sensitivity : ', sensitivity )\n    \n    specificity = confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[0,1])\n    print('Specificity : ', specificity)\n\nNext, we check ROC curve.\n\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\n    \n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for diabetes classifier')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.grid(True)\n\nAnother common metric is the Area Under the Curve, or AUC.\n\n    auc(fpr, tpr)\n\n# Explanation\nHere we implement the shap value to explain our model.\n\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X_test)\n    \n    shap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")\n\n[for more](https://www.kaggle.com/tentotheminus9/what-causes-heart-disease-explaining-the-model)","categories":["Kaggle"]}]