[{"title":"Data Structure","url":"/2020/10/21/DataStructure1/","content":"\n# Algorithm Analysis\n\n## The RAM model of computation\n\nMachine-independent algorithm design depends upon a hypothetical computer called the Random Access Machine or RAM. Under this model of computation, we are confronted with a computer where:\n\n- Each simple operation (+, *, –, =, if, call) takes exactly one time step. \n- Loops and subroutines are not considered simple operations.\n\n## The Big Oh Notation\n\n- $f(n)=O(g(n))$ means $c \\cdot g(n)$ is an *upper bound* on f(n). Thus there exists some constant $c$ such that $f(n)$ is always $\\leq c \\cdot g(n)$, for large enough $n$ (i.e. , $n \\geq n_0$ for some constant $n_0$).\n- $f(n)=\\Omega(g(n))$ means $c \\cdot g(n)$ is an *lower bound* on f(n). Thus there exists some constant $c$ such that $f(n)$ is always $\\geq c \\cdot g(n)$, for all $n\\geq n_0$.\n- $f(n)=\\Theta(g(n))$ means $c_1 \\cdot g(n)$ is an *upper bound* on f(n),  $c_2 \\cdot g(n)$ is an *lower bound* on f(n).  Thus there exist constants $c_1$ and $c_2$ such that $f(n) \\leq c_1 ·g(n)$ and $f(n) \\geq c_2 ·g(n)$. This means that $g(n)$ provides a nice, tight bound on $f(n)$.\n\nNote: The Big Oh notation and worst-case analysis are tools that greatly simplify our ability to compare the efficiency of algorithms.\n\n## Dominance Relations\n\n$n! > 2^n > n^3 > n\\log n > n > \\log n > 1$\n\n## Working with the Big Oh\n\n### Adding functions\n\n- $O(f(n)) + O(g(n)) \\rightarrow O(max(f(n),g(n)))$\n- $\\Omega(f(n)) + \\Omega(g(n)) \\rightarrow \\Omega(max(f(n),g(n)))$\n- $\\Theta(f(n)) + \\Theta(g(n)) \\rightarrow \\Theta(max(f(n),g(n)))$\n\n### Multiplying functions\n\n- $O(c · f(n)) \\rightarrow O(f(n))$\n- $\\Omega(c · f(n)) \\rightarrow \\Omega(f(n))$\n- $\\Theta(c · f(n)) \\rightarrow \\Theta(f(n))$\n- $O(f(n)) ∗ O(g(n)) \\rightarrow O(f(n) ∗ g(n))$\n- $\\Omega(f(n)) ∗ \\Omega(g(n)) \\rightarrow \\Omega(f(n) ∗ g(n))$\n- $\\Theta(f(n)) ∗ \\Theta(g(n)) \\rightarrow \\Theta(f(n) ∗ g(n))$\n\n##  Limits and Dominance Relations\n\nWe say that $f(n)$ dominates $g(n)$ if $limn_{→\\infty} g(n)/f(n) = 0.$\n\n# Data Structures\n\n##  Contiguous vs. Linked Data Structures\n\nData structures can be neatly classified as either contiguous or linked, depending upon whether they are based on arrays or pointers:\n\n- *Contiguously-allocated structures* are composed of single slabs of memory, and include arrays, matrices, heaps, and hash tables. \n- *Linked data structures* are composed of distinct chunks of memory bound together by pointers, and include lists, trees, and graph adjacency lists.\n\n### Array\n\nThe *array* is the fundamental contiguously-allocated data structure. Arrays are structures of fixed-size data records such that each element can be efficiently located by its *index* or (equivalently) address.\n\n#### Advantages\n\n- *Constant-time access given the index* – Because the index of each element maps directly to a particular memory address, we can access arbitrary data items instantly provided we know the index. \n- *Space efficiency* – Arrays consist purely of data, so no space is wasted with links or other formatting information. Further, end-of-record information is not needed because arrays are built from fixed-size records. \n- *Memory locality* – A common programming idiom involves iterating through all the elements of a data structure. Arrays are good for this because they exhibit excellent memory locality. Physical continuity between successive data accesses helps exploit the high-speed cache memory on modern computer architectures.\n\n#### Disadvantages\n\nWe cannot adjust their size in the middle of a program’s execution.\n\n### Pointers and Linked Structures\n\n*Pointers* are the connections that hold the pieces of linked structures together. Pointers represent the address of a location in memory.\n\nPointer syntax and power differ significantly across programming languages.\n\n#### Advantages\n\n- Overflow on linked structures can never occur unless the memory is actually full.\n- Insertions and deletions are simpler than for contiguous (array) lists. \n- With large records, moving pointers is easier and faster than moving the items themselves.\n\n#### Disadvantages\n\n- Linked structures require extra space for storing pointer fields. \n- Linked lists do not allow efficient random access to items.\n- Arrays allow better memory locality and cache performance than random pointer jumping.\n\nNote: Dynamic memory allocation provides us with flexibility on how and where we use our limited storage resources.\n\nOne final thought about these fundamental structures is that they can be thought of as recursive objects: \n\n- Lists – Chopping the first element off a linked list leaves a smaller linked list. This same argument works for strings, since removing characters from string leaves a string. Lists are recursive objects. \n- Arrays – Splitting the first k elements off of an n element array gives two smaller arrays, of size k and n − k, respectively. Arrays are recursive objects.\n\n##  Stacks and Queues\n\n- Stacks – Support retrieval by last-in, first-out (LIFO) order. Stacks are simple to implement and very efficient. \n  - `Push(x,s)`: Insert item x at the top of stack s.\n  - `Pop(s)`: Return (and remove) the top item of stack s\n- Queues – Support retrieval in first in, first out (FIFO) order. This is surely the fairest way to control waiting times for services. You want the container holding jobs to be processed in FIFO order to minimize the maximum time spent waiting. Note that the average waiting time will be the same regardless of whether FIFO or LIFO is used. \n  - `Enqueue(x,q)`: Insert item x at the back of queue q. \n  - `Dequeue(q)`: Return (and remove) the front item from queue q.\n\n## Dictionaries\n\nThe dictionary data type permits access to data items by content. You stick an item into a dictionary so you can find it when you need it. The primary operations of dictionary support are: \n\n- `Search(D,k)` – Given a search key k, return a pointer to the element in dictionary D whose key value is k, if one exists.\n- `Insert(D,x)` – Given a data item x, add it to the set in the dictionary D. \n- `Delete(D,x)` – Given a pointer to a given data item x in the dictionary D, remove it from D.\n\n## Binary Search Trees\n\nA binary search tree labels each node in a binary tree with a single key such that for any node labeled $x$, all nodes in the left subtree of x have keys < $x$ while all nodes in the right subtree of x have keys > $x$.\n\n|           | Best Case | Average Case | Worst Case |\n| --------- | --------- | ------------ | ---------- |\n| Insert    | $O(1)$    | $O(\\log h)$  | $O(h)$     |\n| Delete    | $O(1)$    | $O(\\log h)$  | $O(h)$     |\n| Search    | $O(1)$    | $O(\\log h)$  | $O(h)$     |\n| Traversal | $O(1)$    | $O(\\log h)$  | $O(h)$     |\n\n### Balanced Search Trees\n\nbalanced binary search tree data structures have been developed that guarantee the height of the tree always to be O(log n). Therefore, all dictionary operations (insert, delete, query) take O(log n) time each.\n\n##  Priority Queues\n\nPriority queues are data structures that provide more flexibility than simple sorting, because they allow new elements to enter a system at arbitrary intervals. It is much more cost-effective to insert a new job into a priority queue than to re-sort everything on each such arrival. The basic priority queue supports three primary operations: \n\n- Insert(Q,x)– Given an item x with key k, insert it into the priority queue Q. \n- Find-Minimum(Q) or Find-Maximum(Q)– Return a pointer to the item whose key value is smaller (larger) than any other key in the priority queue Q. \n- Delete-Minimum(Q) or Delete-Maximum(Q)– Remove the item from the priority queue Q whose key is minimum (maximum).\n\n## Hashing and Strings\n\nHash tables are a very practical way to maintain a dictionary. They exploit the fact that looking an item up in an array takes constant time once you have its index. A hash function is a mathematical function that maps keys to integers. We will use the value of our hash function as an index into an array, and store our item at that position.\n\nThe first step of the hash function is usually to map each key to a big integer. Let α be the size of the alphabet on which a given string S is written. Let char(c) be a function that maps each symbol of the alphabet to a unique integer from 0 to α − 1. The function\n$$\nH(s) = \\sum_{i=0}^{|S|-1} \\alpha^{|S|-(i+1)}\\times char(s_i)\n$$\nmaps each string to a unique (but large) integer by treating the characters of the string as “digits” in a base-α number system. The result is unique identifier numbers, but they are so large they will quickly exceed the number of slots in our hash table (denoted by m). We must reduce this number to an integer between 0 and m−1, by taking the remainder of $H(S) \\mod m$.\n\n### Collision Resolution\n\nNo matter how good our hash function is, we had better be prepared for collisions, because two distinct keys will occasionally hash to the same value.\n\n- *Chaining* is the easiest approach to collision resolution. The i-th list will contain all the items that hash to the value of i. Thus search, insertion, and deletion reduce to the corresponding problem in linked lists. If the n keys are distributed uniformly in a table, each list will contain roughly n/m elements, making them a constant size when m ≈ n.\n- *Open addressing*. On an insertion, we check to see if the desired position is empty. If so, we insert it. If not, we must find some other place to insert it instead. The simplest possibility (called *sequential probing*) inserts the item in the next open spot in the table. If the table is not too full, the contiguous runs of items should be fairly small, hence this location should be only a few slots from its intended position.\n\n|                 | Hash Table (expected) | Hash Table (worst case) |\n| --------------- | --------------------- | ----------------------- |\n| Search          | $O(n/m)$              | $O(n)$                  |\n| Insert          | $O(1)$                | $O(1)$                  |\n| Delete          | $O(1)$                | $O(1)$                  |\n| Maximum/Minimum | $O(n + m)$            | $O(n + m)$              |\n\n","categories":["DataStructure"]},{"title":"Hulu AI Class - Feature Engineering","url":"/2020/07/29/Hulu AI Class 2/","content":"\n# ![image.png](https://i.loli.net/2020/07/29/nfbzsIS6Awe2kLZ.png)\n\n# Feature Engineering\n\n## Feature Design\n\n- Brainstorming\n- Ask Business Experts\n- Ask Senior RSDE\n\n### Data Cleaning\n\nRaw Data has\n\n- Null value\n  - Fill(low missing rate)\n    - Mean Value\n    - Medium Number\n    - RandomForestRegressor\n  - Drop column\n- Outlier\n  - Fill\n    - Mean value\n    - Medium number\n  - Drop row\n\n## Feature Transform\n\n- Category\n  - one-hot\n  - multi-hot\n- Numeric\n  - Normalization\n  - Standardization\n  - Discretization\n- Time\n- Text\n- Statistical\n- Combined\n\n![image.png](https://i.loli.net/2020/07/29/LN61GwPOAgzMlsn.png)\n\n## Feature Selection\n\n- Filter\n- Wrapper\n- Embedded","categories":["RecommenderSystems"]},{"title":"UC Berkeley CS162 Operating Systems Lecture 2 - Introduction to Processes","url":"/2020/07/21/OS162_Lecture2/","content":"\n[**Course Link(including Slides and Materials)**](https://inst.eecs.berkeley.edu/~cs162/sp19/)\n\n[**Online Video Link**](https://www.bilibili.com/video/BV1e7411B7Ja?from=search&seid=15268514960428356326)\n\n# Four Fundamental OS Concepts\n\n### **Thread**\n\n- Single unique execution context: fully describes program state\n- Program Counter, Registers, Execution Flags, Stack\n\n### **Address** **space** (with **translation**)\n\n- Programs execute in an *address space* that is distinct from the memory space of the physical machine\n\n### **Process**\n\n- An instance of an executing program is *a process consisting of an address space and one or more threads of control*\n\n### **Dual mode operation / Protection**\n\n- Only the “system” has the ability to access certain resources\n- The OS and the hardware are protected from user programs and user programs are isolated from one another by *controlling the translation* from program virtual addresses to machine physical addresses\n\n# OS Bottom Line: Run Programs\n\n- Load instruction and data segments of executable file into memory\n- Create stack and heap\n- “Transfer control to program”\n- Provide services to program\n- While protecting OS and program\n\n# What happens during program execution?\n\n- Execution sequence:\n  - Fetch Instruction at PC  \n  - Decode\n  - Execute (possibly using registers)\n  - Write results to registers/mem\n  - PC = Next Instruction(PC)\n  - Repeat\n\n# First OS Concept: Thread of Control\n\n- Certain registers hold the *context* of thread\n  - Stack pointer holds the address of the top of stack\n    - Other conventions: Frame pointer, Heap pointer, Data\n  - May be defined by the instruction set architecture or by compiler conventions\n\n- **Thread**: Single unique execution context\n  - Program Counter, Registers, Execution Flags, Stack\n\n- A thread is executing on a processor when it is resident in the processor registers.\n\n- PC register holds the address of executing instruction in the thread\n\n- Registers hold the root state of the thread.\n  - The rest is “in memory”\n\n# Second OS Concept: Program’s Address Space\n\n- Address space $\\Rightarrow$ the set of accessible addresses + state associated with them:\n  - For a 32-bit processor there are 232 = 4 billion addresses\n\n- What happens when you read or write to an address?\n  - Perhaps nothing\n  - Perhaps acts like regular memory\n  - Perhaps ignores writes\n  - Perhaps causes I/O operation\n    - (Memory-mapped I/O)\n  - Perhaps causes exception (fault)\n\n# How can we give the illusion of multiple processors?\n\n![image.png](https://i.loli.net/2020/07/21/UKcseMStzlERg3X.png)\n\n- Assume a single processor. How do we provide the illusion of multiple processors?\n  - Multiplex in time!\n\n- Each virtual “CPU” needs a structure to hold:\n  - Program Counter (PC), Stack Pointer (SP)\n  - Registers (Integer, Floating point, others…?)\n\n- How switch from one virtual CPU to the next?\n  - Save PC, SP, and registers in current state block\n  - Load PC, SP, and registers from new state block\n\n- What triggers switch?\n  - Timer, voluntary yield, I/O, other things\n\n# The Basic Problem of Concurrency\n\n- The basic problem of concurrency involves resources:\n  - Hardware: single CPU, single DRAM, single I/O devices\n  - Multiprogramming API: processes think they have exclusive access to shared resources\n\n- OS has to coordinate all activity\n  - Multiple processes, I/O interrupts, …\n  - How can it keep all these things straight?\n\n- Basic Idea: Use Virtual Machine abstraction\n  - Simple machine abstraction for processes\n  - Multiplex these abstract machines\n\n- Dijkstra did this for the “THE system”\n  - Few thousand lines vs 1 million lines in OS 360 (1K bugs)\n\n# Properties of this simple multiprogramming technique\n\n- All virtual CPUs share same non-CPU resources\n  - I/O devices the same\n  - Memory the same\n\n- Consequence of sharing:\n  - Each thread can access the data of every other thread (good for sharing, bad for protection)\n  - Threads can share instructions\n     (good for sharing, bad for protection)\n  - Can threads overwrite OS functions? \n\n- This (unprotected) model is common in:\n  - Embedded applications\n  - Windows 3.1/Early Macintosh (switch only with yield)\n  - Windows 95—ME (switch with both yield and timer)\n\n# Protection\n\n- Operating System must protect itself from user programs\n  - Reliability: compromising the operating system generally causes it to crash\n  - Security: limit the scope of what processes can do\n  - Privacy: limit each process to the data it is permitted to access\n  - Fairness: each should be limited to its appropriate share of system resources (CPU time, memory, I/O, etc)\n\n- It must protect User programs from one another\n\n- Primary Mechanism: limit the translation from program address space to physical memory space\n  - Can only touch what is mapped into process *address space*\n\n- Additional Mechanisms:\n  - Privileged instructions, in/out instructions, special registers\n  - syscall processing, subsystem implementation \n    - (e.g., file access rights, etc) \n\n# Third OS Concept: Process\n\n- Process: execution environment with Restricted Rights\n  - Address Space with One or More Threads\n  - Owns memory (address space)\n  - Owns file descriptors, file system context, …\n  - Encapsulate one or more threads sharing process resources\n\n- Why processes?\n  - Protected from each other!\n  - OS Protected from them\n  - Processes provides memory protection\n  - Threads more efficient than processes (later)\n\n- Fundamental tradeoff between protection and efficiency\n\n- Communication easier *within* a process\n\n- Communication harder *between* processes\n\n- Application instance consists of one or more processes \n\n![image.png](https://i.loli.net/2020/07/21/BZK72hIqeuPV3MO.png)\n\n- Threads encapsulate concurrency: “Active” component\n\n- Address spaces encapsulate protection: “Passive” part\n  - Keeps buggy program from trashing the system\n\n# Fourth OS Concept: Dual Mode Operation\n\n- Hardware provides at least two modes:\n  - “Kernel” mode (or “supervisor” or “protected”)\n  - “User” mode: Normal programs executed \n\n- What is needed in the hardware to support “dual mode” operation?\n  - A bit of state (user/system mode bit)\n  - Certain operations / actions only permitted in system/kernel mode\n    - In user mode they fail or trap\n  - User à Kernel transition *sets* system mode AND saves the user PC\n    - Operating system code carefully puts aside user state then performs the necessary operations\n  - Kernel à User transition *clears* system mode AND restores appropriate user PC\n    - return-from-interrupt\n\n![image.png](https://i.loli.net/2020/07/21/C7tZ6zcrw2vPNyn.png)\n\n\n\n## Process Control Block\n\n- Kernel represents each process as a process control block (PCB)\n  - Status (running, ready, blocked, …)\n  - Register state (when not ready)\n  - Process ID (PID), User, Executable, Priority, …\n  - Execution time, …\n  - Memory space, translation, …\n\n- Kernel Scheduler maintains a data structure containing the PCBs\n\n- Scheduling algorithm selects the next one to run","categories":["OS"]},{"title":"UC Berkeley CS162 Operating Systems Lecture 1","url":"/2020/07/20/OS162_Lecture1/","content":"\n[**Course Link(including Slides and Materials)**](https://inst.eecs.berkeley.edu/~cs162/sp19/)\n\n[**Online Video Link**](https://www.bilibili.com/video/BV1e7411B7Ja?from=search&seid=15268514960428356326)\n\n# What is an operating system?\n\n## What does operating system do?\n\nProvide abstractions to apps\n\n- File systems \n- Processes, threads \n- VM, containers\n- Naming system\n\nManage resources: \n\n- Memory, CPU, storage, ...\n\nAchieves the above by implementing specific algorithms and techniques:\n\n- Scheduling\n- Concurrency\n- Transactions \n- Security\n\n## What is an operating system?\n\nSpecial layer of software that provides application software access to hardware resources \n\n- Convenient abstraction of complex hardware devices \n- Protected access to shared resources \n- Security and authentication \n- Communication amongst logical entities\n\n- Referee\n  - Manage sharing of resources, Protection, Isolation\n    - Resource allocation, isolation, communication\n\n- Illusionist\n  - Provide clean, easy to use abstractions of physical resources\n    - Infinite memory, dedicated machine\n    - Higher level objects: files, users, messages\n    - Masking limitations, virtualization\n\n- Glue\n  - Common services\n    - Storage, Window system, Networking\n    - Sharing, Authorization\n    - Look and feel\n\n- No universally accepted definition \n- “Everything a vendor ships when you order an operating system” is good approximation \n  - But varies wildly \n- “The one program running at all times on the computer” is the kernel –Everything else is either a system program (ships with the operating system) or an application program\n\n# In conclusion\n\n- Operating systems provide a virtual machine abstraction to handle diverse hardware \n  - Operating systems simplify application development by providing standard services \n- Operating systems coordinate resources and protect users from each other\n  - Operating systems can provide an array of fault containment, fault tolerance, and fault recovery","categories":["OS"]},{"title":"Elements of the Theory of Computation - Chapter 1 Sets, Relations, and Languages","url":"/2020/07/19/Computation Theory1/","content":"\n# Set\n\n## set\n\n- an unordered collection of elements\n\n## subsets and proper subsets\n\n- Subset notation: $\\subseteq$ \n  $$\n  S \\subseteq T \\Leftrightarrow (\\forall x \\in S \\Rightarrow x\\in T)\n  $$\n\n- Proper subset $\\subset $ \n\n## Set Operations and Its Identities\n\n- Union, Intersection, Difference, Symmetric difference, complement\n- Commutative Law, Associative Law, Distributive law, Absorption, DeMorgan's Law, Idempotent law\n\n## Power Set\n\n- $2^S$ = set of all subset of $S$\n  $$\n  2^S = \\{T|T\\subseteq S\\}\n  $$\n\n## Partition\n\nA **partition** of nonempty set $A$ is a subset $\\sqcap$ of $2^A$ such that\n\n1. $\\phi\\notin \\sqcap $\n2. $\\forall S,T \\in \\sqcap.$ and $S\\neq T, S \\cap T = \\phi$\n3. $\\cup \\sqcap = A$\n\n# Relations and Functions\n\n## Ordered Pair and Binary Relation\n\n- Ordered Pair: $(a,b)$\n  $$\n  (a,b)=(c,d) \\Leftrightarrow (a=c) \\land (b=d)\n  $$\n\n- Cartesian Product: $A \\times B$\n  $$\n  A \\times B = \\{ (a,b)|a\\in A \\land b\\in B\\}\n  $$\n\n- Binary Relation $A$ and $B$\n  $$\n  R \\subseteq A\\times B\n  $$\n\n## Ordered Tuple and n-ary Relation \n\n## Operations of Relations\n\n- Inverse\n  $$\n  R \\subseteq A\\times B \\Rightarrow R^{-1}\\subseteq B\\times A\n  $$\n\n- Composition\n\n## Function\n\nDefinition: A function $f: A\\rightarrow B$ must satisfy:\n\n- $f \\subseteq A \\times B$\n- $\\forall a \\in A, \\exists$ exactly one $b \\in B$ with $(a,b) \\in f$\n\nNote: We write $(a,b)\\in f$ as $f(a) = b$\n\n- one-to-one function: $\\forall a,b \\in A \\land a \\neq b \\Rightarrow f(a) \\neq f(b)$\n- onto function: $\\forall b \\in B, \\exists a\\in A$ such that $f(a)=b$\n- bijection function: one-to-one + onto \n\n# Special Types of Binary Relations\n\n## Representation of Relations\n\n- Directed graph: node, edge, path\n- Matrix: Adjacency matrix\n\n## Properties of Relations ($R \\subseteq A\\times A$)\n\n- reflexive: $\\forall a\\in A \\Rightarrow (a,a) \\in R$\n- symmetric: $(a,b) \\in R \\land a \\neq b \\Rightarrow (b,a)\\in R$, antisymmetric: $(a,b) \\in R \\Rightarrow (b,a) \\notin R$\n- transitive: $(a.b)\\in R, (b,c)\\in R \\Rightarrow (a,c)\\in R$\n\n## Equivalence Relation\n\n- reflexive, symmetric, transitive\n\n- equivalence classes\n  $$\n  [a]=\\{ b|(a,b)\\in R\\}\n  $$\n\n**Theorem** Let $R$ be an equivalence relation on a nonempty set $A$. Then the equivalence classes of $R$ constitute a partition of $A$.\n\n## Partial Order\n\n- reflexive, symmetric, transitive\n- total order\n- minimal element and maximal element\n\n# Finite and Infinite sets\n\n## Equinumerous\n\n- Sets $A$ and $B$ equinumerous $\\Leftrightarrow \\exists$ bijection $f:A\\rightarrow B$ \n- Cardinality and generalized Cardinality\n- Finite and Infinite sets\n\n## Countable and Uncountable Infinite \n\n- A set is said to be **countably infinite** $\\Leftrightarrow$ it is equinumerous with $\\mathbb{N}$\n- S is an uncountable set $\\Leftrightarrow |S|>|\\mathbb{N}|$\n- The union of a countably infinite collection of countably infinite sets is countably infinite. \n\nExample: Show that $\\mathbb{N} \\times \\mathbb{N}$ is countably infinite.\n\nTheorem: $|\\mathbb{R}|>|\\mathbb{N}|$ (diagonalization)\n\nQuestion: Is $|\\mathbb{R}| > |(0,1)|$? \n$$\nf(x)=\\frac{1}{\\pi}\\arctan (x) + \\frac12\n$$\n\n## Continuum Hypothesis\n\n$|\\mathbb{N}|=\\aleph_0, |\\mathbb{R}|=\\aleph_1$\n\n$\\aleph_0<\\aleph_1$\n\n","categories":["ComputationTheory"]},{"title":"Hulu AI Class - Recommender Systems 1","url":"/2020/07/19/Hulu AI class/","content":"\n# The aim of this class\n\n- Basic concept of recommender systems\n- Simple formula and theory\n- Underlying intuition of each recommendation model\n- Pros and cons\n\n# What is recommender system?\n\nBasic assumption of recommender systems\n\n- Information overload\n- Users are unsure about what they are looking for(different from information retrieval)\n\nTarget of (traditional) recommender systems\n\n- Develop a **mathematical model** of an **objective function**($\\mathcal{F}$) to predict how much a **user** will like an **item** in a given **context**.\n\n$$\nR=\\mathcal{F}(u, i ; c)\n$$\n\n![rs1.png](https://i.loli.net/2020/07/19/cRID18HTiMgsLCn.png)\n\n# Basic Concept\n\n- Ratings\n\n- - Explicit ratings: 5 star, like/dislike*(additional effort from users)*\n  - Implicit ratings: page views, click, effective playback, purchase records, whether or not listen to a music track*(easier to collect, less precise)*\n\n- Interaction matrix\n\n- - Matrix modelling user's rating on item\n  - User $u$'s rating towards item $i$ as $r_{u,i}$\n\n![rs1.2.png](https://i.loli.net/2020/07/19/zPJmU6hX3jLNYBK.png)\n\n# Collaborative Filtering \"物以类聚，人以群分\"\n\n- **Context-free** recommender system\n\n- Intuition: **the users who have agreed in the past tend to also agree in the future**\n\n- F: aggregate from nearest neighbor\n  $$\n  r_{u, i}=\\frac{\\sum_{v} w_{u v} \\cdot r_{v, i}}{\\# \\text { of neighbors }}\n  $$\n  \n- User-based CF: user neighbors (green block)\n\n- Item-based CF: item neighbors (blue block)\n\n- Hybrid: use both\n\n![image-20200719113651705.png](https://i.loli.net/2020/07/19/i4CtNJbxlD39rLM.png)\n\n![image-20200719113855912.png](https://i.loli.net/2020/07/19/o7qUYWhifSOZcXy.png)\n\n## Similarity Calculation\n\n- Cosine similarity\n- Pearson correlation\n- SimRank\n\n## Improvement of Similarity Calculation\n\n- Not all neighbors are equally \"valuable\" i.e. agreement on commonly liked items not so informative as agreement on controversial items\n  - Give more weight to items that have a higher variance\n- Low number of co-rated items may introduce bias\n  - Reduce \"confidence\" when the number of co-rated items is low\n- All neighbors are not very \"similar\"\n  - Set similarity threshold\n  - Set maximum number of neighbors\n\n## Pros and Cons\n\n- Pros \n\n  - Easy to implement \n  - Good explanation \n\n- Cons\n\n  - Large Memory needed \n  - Sparsity issue\n\n  \n\n# Matrix Factorization\n\n- **Context-free** recommender system\n- Intuition: express **higher-level attributes**\n- Generate user and item **embeddings** \n  - Latent vectors\n  - (A group of) similar users/items have similar embeddings\n\n   $\\mathcal{F}$  : dot product of embeddings $r_{u,i}=p_u^T q_i$\n$$\n\\min_{p,q} \\sum_{(u,i)\\in K} (r_{u,i}-p_u^T q_i)^2~+\\lambda(||p||+||q||)^2\n$$\n\n## Improvement of Matrix Factorization\n\n- $\\mathcal{F}$  could be more complicated\n  $$\n  r_{u, i}={\\mu}+b_{i}+{b_{u}}+p_{u}^{T} q_{i}\n  $$\n\n-  Global bias \n  \n  -  Average ratings in the whole catalog \n-  Item bias \n  \n  - Users rate diﬀerent categories in diﬀerent ways \n- User bias\n  \n  - Some users tend to give high ratings while others are not\n\n## Pros and Cons\n\n-  Pros: \n  - Better generalization capability: even though two users haven’t rated any same movies, it’s still possible to ﬁnd the similarity between them through embeddings \n  - Low memory: only need to store low-dimensional latent vectors, no need to store large user behaviors \n- Cons\n  - Hard to explain \n  - Sparsity\n\n# Logistic Regression\n\n- Intuition: add **context** information to our model \n\n- Recommendation as **classiﬁcation**: \n\n- $\\mathcal{F}$ \n  $$\n  r_{u, i}=\\frac{1}{1+e^{-(W x+b)}}\n  $$\n  \n- User, item, context => **categorical** feature vector\n\n  - Example: gender, time diﬀerence within the day, show genre as feature \n  - $[0, 1, 0, 0, 0, 1, 0]$ => female, view on morning, target show is *Horror Movie* \n\n- Diﬀerent weight of features, **learned by gradient descent** \n\n- Sigmoid projection\n\n## Logistic Regression —— why?\n\n- Why categorical feature vector? \n  - Some non-numerical features, e.g. device: “roku”, “web”, “android”…\n  - Absolute value of feature does not make sense \n  - Feature crossing \n  - Fast computation of sparse vector\n\n- Why sigmoid projection?\n  - Output between 0 and 1 \n  - Good mathematical form \n  - Maximum entropy model\n\n## Pros and Cons\n\n- Pros\n  - Good explanation (feature importance) \n  - Good parallelism, fast model training \n  - Low training cost \n  - Online training \n- Cons\n  - Manually craft features\n  - Limited model expressiveness (linear model)\n\n## Problems of Logistic Regression\n\nSimpson's Paradox\n\n# Factorization Machine (FM)\n\n-  Intuition: **feature crossing**\n\n- $\\mathcal{F}$\n  $$\n  y(x)=\\operatorname{sigmoid}\\left(w_{0}+\\sum_{i=1}^{n} w_{i} x_{i}+\\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} w_{i j} x_{i} x_{j}\\right)\n  $$\n  \n- Let $w_{ij}=<v_i, v_j>$ \n  $$\n  y(x)=sigmoid(w_0 + \\sum_{i=1}^n w_ix_i + \\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n} <v_i,v_j> x_ix_j)\n  $$\n  \n- Deal with parameter explosion ($n^2 \\geq nk$)\n- Equivalent to low rank parameter matrix factorization $W=VV^T$\n\n## Pros and Cons\n\n- Pros\n  -  Good expressiveness \n  - Good generalization \n  - Relatively low training cost\n- Cons\n  - Hard to make higher level feature crossing\n\n# Gradient Boosting Decision Tree (GBDT)\n\n- $\\mathcal{F}$\n  $$\n  F_m(x)=\\sum_{m=1}^M T(x;\\theta_m)\n  $$\n\n- Training samples $(x_1, y_1), (x_2, y_2), ... , (x_n,y_n)$\n\n- Boosting:\n\n- $f$ : Regression tre -> Boosting Regression(Decision) Tree\n\n- Loss function -> Gradient Boosting Decision Tree, GBDT\n  $$\n  L=\\sum_i (f(x_i)-y_i)^2 \\\\\n  T(x_i;\\theta_m) \\approx -[\\frac{\\partial L(y_i, f(x_i))}{\\partial f(x_i)}]_{f(x)=F_{m-1}(x)}\n  $$\n  \n\n## XGBoost & LightGBM\n\n- $\\mathcal{F}$\n  $$\n  \\hat{y}=\\sum_{k=1}^K f_k(x)\n  $$\n\n- Change optimization goal\n  $$\n  O b j^{(t)}=\\sum_{i=1}^{m} L\\left(y_{i}, \\hat{y}_{i}^{(t-1)}+f_{t}\\left(x_{i}\\right)\\right)+\\sqrt{\\gamma T}+\\sqrt{\\frac{\\lambda}{2} \\sum_{j=1}^{T} w_{j}^{2}}\n  $$\n\n- Loss function: Taylor expansion, keep second order terms \n- Regularization\n  - Prevent too complicated trees \n  - Prevent extreme parameters \n- LightGBM (engineering optimization)\n\n## Pros and Cons\n\n- Pros\n  - Good expressiveness \n  - Low eﬀort of feature engineering \n- Cons\n  - Hard parallelism\n  - Unable to do incremental training\n\n# GBDT + LR\n\n- Intuition: **model-based feature engineering**\n\n![image.png](https://i.loli.net/2020/07/19/bCaOt84o3Aji2pW.png)","categories":["RecommenderSystems"]},{"title":"The Element of Statistical Learning Chapter 16","url":"/2020/07/18/ESL Chapter 16/","content":"\n# Chapter 16. Ensemble Learning\n\n## What is the idea of Ensemble Learning?\n\nThe **idea** of ensemble learning is **to build a prediction model by combining the strengths of a collection of simpler base models**.\n\n[Zhou Zhihua](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf) Ensemble Learning: **to boost** **weak learner**s which are slightly better than random guess to **strong learners** which can make very accurate predictions.\n\n \n\nEnsemble learning can be broken down into **two** tasks:\n\n**First**, developing a population of base learners from the training data,\n\n**then** combining them to form the composite predictor. \n\n \n\nZhou Zhihua:\n\n**First**, a number of base learners are produced, which can be generated in a *parallel* style or in a *sequential* style where the generation of a base learner has influence on the generation of subsequent learners. \n\n**Then**, the base learners are combined to use, where among the most popular combination schemes are *majority votin*g for classification and *weighted averagin*g for regression.\n\n \n\n## List some **methods** of Ensemble Learning.\n\n- **Bagging**     \n\n- - trains a number of base learners each from a different *bootstrap* sample by calling a base      learning algorithm.\n  - After obtaining the base learners, Bagging combines them by majority voting and the most-voted      class is predicted.\n  - Sample: Random Forest\n  - Reduce **variance**\n\n- **Boosting**     \n\n- - Is a family of algorithms since there are many variants.\n  - Sample: Adaboost\n  - Reduce **bias**\n\n- **Stacking**     \n\n- - A number of first-level individual learners are generated from the training data set      by employing different learning algorithms. \n  - Those individual learners are then combined by a second-level learner which is      called as *meta-learner*. \n\n> *Bayesian methods for nonparametric regression can also be viewed as ensemble methods* \n\nGenerally speaking, there is no ensemble method which outperforms other ensemble methods consistently.\n\n \n\n## List some **Penalized  Regression** and how they works\n\n**Lasso regression** and **ridge regression**.\n\nConsider the dictionary of all possible J-terminal node regression trees $T=\\{T_k\\}$ that could be realized on the training data as basis functions in  $R^p$. The linear model is\n$$\nf(x)=\\sum_{k=1}^K \\alpha_k T_k(x)\n$$\nSuppose the coefficients are to be estimated by **least squares**. Since the number of such trees is likely to be much larger than even the largest training data sets, some form of regularization is required. Let $\\hat{\\alpha}(\\lambda)$ solve\n$$\n\\min_{\\alpha}\\{\\sum_{i=1}^{N}(y_i-\\sum_{k=1}^K\\alpha_kT_k(x_i))^2+\\lambda\\cdot J(\\alpha)\\}\n$$\n$J(\\alpha)$ is a function of the coefficients that generally penalizes larger values. \n$$\nJ(\\alpha)=\\sum_{k=1}^K |\\alpha_k|^2 - ridge  \\\\\nJ(\\alpha)=\\sum_{k=1}^K |\\alpha_k| - lasso\n$$\n\n\n## Why ensemble superior to Singles - **generalization**\n\n- the training data might not provide sufficient information for choosing a single best learner\n- the search processes of the learning algorithms might be imperfect\n- the hypothesis space being searched might not contain the true target function, while ensembles can give some good approximation. \n\n**The bias-variance decomposition** is often used in studying the performance of ensemble methods.","categories":["ML Concepts"]},{"title":"The Element of Statistical Learning Chapter 2","url":"/2020/07/18/ESL Chapter 2/","content":"\n# Chapter 2. Overview of Supervised Learning\n\nIn stats, **predictors/independent variables** = **input**, **responses/dependent variables** = **output**.\n\n## Difference between **regression** and **classification**\n\nRegression when we predict quantitative outputs, and classification when we predict qualitative outputs. \n\n## Some **notations** on dataset\n\nWe will typically denote an **input** variable by the symbol **X**. **Quantitative outputs** will be denoted by **Y** , and **qualitative outputs** by **G** (for group). \n\n**Observed values** are written in lowercase; hence the i-th observed value of **X** is written as .\n\n**Matrices** are represented by bold uppercase letters; for example, a set of N input p-vectors , i = 1,...,N would be represented by the N×p matrix **X**. \n\nIn general, vectors will not be bold, except when they have N components.\n\n## What is **Nearest Neighbors**?\n\nNearest-neighbor methods use those observations in the training set  closest in input space to  to form  . Specifically, the k-nearest neighbor fit for  is defined as follows:\n$$\n\\hat{Y}(x)=\\frac{1}{k}\\sum_{x_i\\in N_k(x)}y_i\n$$\nwhere  is the neighborhood of  defined by the  closest points  in the training sample. \n\nIt has **high variance and low bias**.","categories":["ML Concepts"]},{"title":"Intermediate R","url":"/2020/06/16/Intermediate R/","content":"\n# Equality\n\n```R\n== / identical()\n!=\n< and > # greater and less than\n& # and\n| # or\n! # reverse the result\n```\n\n# Compare\n\n## Compare vectors\n\nlinkedin > facebook\n\n## Compare matrices\n\nviews <= 14\n\n# The if statement\n\n```R\nif (condition) {\n  expr\n} else if (condition2) {\n  expr2\n} else if (condition3) {\n  expr3\n} else {\n  expr4\n}\n\n\n```\n\n# Write a while loop\n\n```R\nwhile (condition) {\n  expr\n}\n```\n\n## Stop the while loop: \n\n```react\nbreak\n```\n\n## Loop over a vector\n\n```R\nprimes <- c(2, 3, 5, 7, 11, 13)\n```\n\n### loop version 1\n\n```R\nfor (p in primes) {\n  print(p)\n}\n```\n\n### loop version 2\n```R\nfor (i in 1:length(primes)) {\n  print(primes[i])\n}\n```\n\n## Loop over a list\n\n```R\nprimes_list <- list(2, 3, 5, 7, 11, 13)\n```\n\n### loop version 1\n\n```R\nfor (p in primes_list) {\n  print(p)\n}\n```\n\n### loop version 2\n```R\nfor (i in 1:length(primes_list)) {\n  print(primes_list[[i]])\n}\n```\n\n## Loop over a matrix\n\n```R\nfor (var1 in seq1) {\n  for (var2 in seq2) {\n    expr\n  }\n}\n```\n\n```R\nstrsplit() \n```\n\n# Function documentation\n\n```\nhelp(sample)\n?sample\nargs(sample)\n```\n\n## Write your own function\n\n```\nmy_fun <- function(arg1, arg2=TRUE) {\n  body\n  return(result)\n}\n```\n\n## Load an R Package\n\n• install.packages(), which as you can expect, installs a given package.\n• library() which loads packages, i.e. attaches them to the search list on your R workspace. /  require()\n• search(), to look at the currently attached packages and\n• qplot(mtcars$wt, mtcars$hp), to build a plot of two variables of the mtcars data frame.\n\n```\nlibrary(ggplot2) \n```\n\n\n# a powerful package for data visualization\n\nUse lapply with a built-in R function\n\n```\nlapply(X, FUN, ...)\n```\n\nTo put it generally, lapply takes a vector or list X, and applies the function FUN to each of its members. \n\nTo put it generally, lapply takes a vector or list X, and applies the function FUN to each of its members. \n\nlapply and anonymous functions\n# Named function\n```\ntriple <- function(x) { 3 * x }\n```\n\n\n## Anonymous function with same implementation\n```\nfunction(x) { 3 * x }\n```\n\n\n# Use anonymous function inside lapply()\n```\nlapply(list(1,2,3), function(x) { 3 * x })\n```\n\n## Use lapply with additional arguments\n\n```\nmultiply <- function(x, factor) {\n  x * factor\n}\nlapply(list(1,2,3), multiply, factor = 3)\n```\n\n## How to use sapply\n\n```\nsapply(X, FUN, ...)\n```\n\nsapply() simplifies the list that lapply() would return by turning it into a vector. \n\nsapply() simplifies the list that lapply() would return by turning it into a vector. \n\n## Use vapply\n\n```\nvapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE)\n```\n\nOver the elements inside X, the function FUN is applied. The FUN.VALUE argument expects a template for the return argument of this function FUN. USE.NAMES is TRUE by default; in this case vapply() tries to generate a named array, if possible.\n\n# Data Utilities\n\nR features a bunch of functions to juggle around with data structures::\n\t• seq(): Generate sequences, by specifying the from, to, and by arguments.\n\t• rep(): Replicate elements of vectors and lists.\n\t• sort(): Sort a vector in ascending order. Works on numerics, but also on character strings and logicals.\n\t• rev(): Reverse the elements in a data structures for which reversal is defined.\n\t• str(): Display the structure of any R object.\n\t• append(): Merge vectors or lists.\n\t• is.*(): Check for the class of an R object.\n\t• as.*(): Convert an R object from one class to another.\n\t• unlist(): Flatten (possibly embedded) lists to produce a vector.\n\n```\ngrepl & grep\n```\n\n\nIn their most basic form, regular expressions can be used to see whether a pattern exists inside a character string or a vector of character strings. For this purpose, you can use:\n\t• grepl(), which returns TRUE when a pattern is found in the corresponding character string.\n\t• grep(), which returns a vector of indices of the character strings that contains the pattern.\n\nYou can use the caret, ^, and the dollar sign, \\$ to match the content located in the start and end of a string, respectively. This could take us one step closer to a correct pattern for matching only the \".edu\" email addresses from our list of emails. But there's more that can be added to make the pattern more robust:\n\t• @, because a valid email must contain an at-sign.\n\t• .*, which matches any character (.) zero or more times (*). Both the dot and the asterisk are metacharacters. You can use them to match any character between the at-sign and the \".edu\" portion of an email address.\n\t• \\\\.edu​\\$, to match the \".edu\" part of the email at the end of the string. The \\\\ part escapes the dot: it tells R that you want to use the . as an actual character.\n\nWhile grep() and grepl() were used to simply check whether a regular expression could be matched with a character vector, sub() and gsub() take it one step further: you can specify a replacement argument. If inside the character vector x, the regular expression pattern is found, the matching element(s) will be replaced with replacement.sub() only replaces the first match, whereas gsub() replaces all matches.\n\nCreate and format dates\nTo create a Date object from a simple character string in R, you can use the as.Date() function. The character string has to obey a format that can be defined using a set of symbols (the examples correspond to 13 January, 1982):\n\t• %Y: 4-digit year (1982)\n\t• %y: 2-digit year (82)\n\t• %m: 2-digit month (01)\n\t• %d: 2-digit day of the month (13)\n\t• %A: weekday (Wednesday)\n\t• %a: abbreviated weekday (Wed)\n\t• %B: month (January)\n\t• %b: abbreviated month (Jan)","categories":["R"]},{"title":"Introduction to R","url":"/2020/06/12/Introduction to R/","content":"\n# Introduction to R\n\nThis guide is a note for datacamp, introduction to R.\n\nI usually use Python, thus in this note, I used Python to contrast R on data collection to data cleaning to data frame...\n\n| R                                                            | Python                                                       |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| **Arithmetic with R ** <br />Addition: + <br />Subtraction: - <br />Multiplication: * <br />Division: / <br />Exponentiation: ^ <br />Modulo: %% | <br />Addition: + <br />Subtraction: - <br />Multiplication: * <br />Division: / <br />Exponentiation: pow() <br />Modulo: % |\n| **Variable assignment**<br />my_var <- 4                     | **Variable assignment**<br />my_var = 4                      |\n| **What's that data type?**<br />class()                      | **Data Type**<br />type()                                    |\n| **Create a vector** <br />the combine function c() <br />numeric_vector <- c(1, 2, 3) <br />character_vector <- c(\"a\", \"b\", \"c\") | **Create a vector?**<br />In python, we have List: []        |\n| **Naming a vector** <br />names() <br />some_vector <- c(\"John Doe\", \"poker player\") <br />names(some_vector) <- c(\"Name\", \"Profession\") | **Naming a vector?**<br />I think we normally use another list And we use zip() function to put two lists together. Or we use dict() |\n| One unique feature: vectors can be added together. If a <- c(1, 2, 3), b <- c(4, 5, 6), then a + b would be c(5, 7, 9) | Lists added together would be like stick together, instead of adding each element one by one. For example, a = [1, 2, 3], b = [4, 5, 6], a + b would be [1, 2, 3, 4, 5, 6] |\n| **Some functions** <br />sum() <br />max() <br />min() <br />mean(na.rm = FALSE) # na.rm: drop NA data <br />abs() <br />round() | **Some functions**<br />sum() <br />max() <br />min() <br />np.mean() |\n| **Index of vectors** <br />In R, we start from 1.  <br />First element, vector_sample[1]. <br />First to third element, vector_sample[c(1,2,3)] or vector_sample[1:3] | **Index of vectors** <br />In python, we start from 0.       |\n| **What's a matrix?** <br />matrix(1:9, byrow = TRUE, nrow = 3) <br />using `byrow`, we would fill the matrix by rows! | Python doesn't have this function. We use pandas.DataFrame or numpy to make a matrix, and we can use numpy.reshape. |\n\n","categories":["R"]},{"title":"Stanford CS224n Natural Language Processing Course10","url":"/2020/03/30/CS224n Classnotes Course9/","content":"\n# Course 10 - Question Answering\n\n## Motivation/History\n\nWith massive collections of full-text documents, return relevant documents.\n\n2 parts\n\n1. find documents that might contain an answer\n2. find an answer in a paragraph or a document\n\nMCTest Reading Comprehension: Passage+Question=Answer\n\n![image-20200330135135377.png](https://i.loli.net/2020/04/04/xpvqj1TbEcJkyOL.png)\n\n## The SQuAD dataset\n\nEvalution \n\n- Systems are scored on two metrics\n\n  - exact match\n  - f1:  Precision = tp/(tp+fp), Recall = tp/(tp+tn), F1=2PR/(P+R) - taken as primary\n\n  Both metrics ignore punctuation and articles (a, an, the only)\n\nLimiations\n\n- Only span-based answers\n- Questions were constructed looking at passages\n- Barely any multi-facts/sentence inference beyonce coreference\n\nBut still, well-targeted, well-structured, clean dataset\n\n## The Stanford Attentive Reader model\n\n## BiDAF\n\n![image-20200331105339872.png](https://i.loli.net/2020/04/04/wpZ5DlvJ7AK4xVY.png)\n\ncentral idea: the Attention Flow layer\n\nIdea: attention should flow both ways - from the context to the question and from the question to the context\n\nMake the similarity matrix:\n$$\nS_{ij} = w_{sim}^{T}[c_i;q_j;c_i \\cdot q_j]\n$$\nContext-to-Question attention:\n$$\n\\alpha^{i} = softmax(S_i,:) \\in \\mathbb{R}^M\\\\\n\\alpha_i=\\sum_{j=1}^M \\alpha_j^i q_j \\in \\mathbb{R}^{2h}\n$$\n\nAttention Flow Idea: attention should flow both ways\n\nQuestion-to-Context attention:\n$$\nm_i=max_j S_{ij} \\in \\mathbb{R}\\\\\n\\beta = softmax(m) \\in \\mathbb{R}^N\\\\\nc' = \\sum_{i=1}^{N} \\beta_i c_i \\in \\mathbb{R}^{2h}\n$$\n\n## Recent, more advanced architectures\n\nFusionNet\n\n## ELMo and BERT preview\n\n","categories":["NLP"]},{"title":"Stanford CS224n Natural Language Processing Course7","url":"/2020/03/16/CS224n Classnotes Course7/","content":"\n# Course 7 - Vanishing Gradients, Fancy RNN\n\n## Vanishing Gradients\n\nGradient can be viewed as a measure of *the effect of the past on the future*\n\n- There is no dependency between step $t$ and $t+n$ in the data\n- We have wrong parameters to capture the true dependency between  $t$ and $t+n$ \n\n## Effect of vanishing gradient on RNN-LM\n\nLM task - unable to predict similar long-distance dependencies\n\nSyntactic recency: The *writer* of the books **is**\n\nSequential recency: The writer of *books* **are**\n\nDue to vanishing gradient, RNN-LMs are better at learning from sequential recency than syntactic \n\n## Why is exploding gradient a problem?\n\n$$\n\\theta^{new} = \\theta^{old}-\\alpha \\nabla_{\\theta} J(\\theta)\n$$\n\n### Solution: gradient clipping\n\nAlgorithm 1: Pseudo-code for norm clipping\n\n$\\hat{g} \\leftarrow \\frac{\\partial \\epsilon}{\\partial \\theta}$\n\nif $||\\hat{g}|| \\geq threshold $ then\n\n​\t$\\hat{g} \\leftarrow \\frac{threshold}{||\\hat{g}||} \\hat{g}$\n\nend if \n\n## Long Short-Term Memory(LSTM)\n\nOn step $t$, there is a hidden state $h^{(t)}$ and a cell state $c^{(t)}$\n\n- Both are vectors length $n$\n- The cell stores long-term information\n- The LSTM can erase, write and read information from the cell\n\nThe selection is controlled by 3 corresponding **gates**\n\n- vector length $n$\n- each element of the gates can be open(1), closed(0), or in between\n- dynamic: their value is computed based on the current context\n\nWe have a sequence of input $x^{(t)}$, and we will compute a sequence of hidden states $h^{(t)}$ and cell states $c^{(t)}$. On timestep $t$:\n\nForget gate - controls what is kept vs forgotten, from previous cell state\n$$\n\\boldsymbol{f}^{(t)}={\\sigma}\\left(\\boldsymbol{W}_{f} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{U}_{f} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{f}\\right)\n$$\nInput gate - controls what parts of the new cell content are written to cell\n$$\n\\boldsymbol{i}^{(t)}=\\sigma\\left(\\boldsymbol{W}_{i} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{U}_{i} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{i}\\right)\n$$\nOutput gate - controls what parts of cell are output to hidden state\n$$\n\\boldsymbol{o}^{(t)}={\\sigma}\\left(\\boldsymbol{W}_{o} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{U}_{o} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{o}\\right)\n$$\nNew cell content - this is the new content to be written to the cell\n$$\n\\tilde{\\boldsymbol{c}}^{(t)}=\\tanh \\left(\\boldsymbol{W}_{c} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{U}_{c} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{c}\\right)\n$$\nCell state - erase(\"forget\") some content from last cell state, and write(\"input\") some new cell content\n$$\n\\boldsymbol{c}^{(t)}=\\boldsymbol{f}^{(t)} \\circ \\boldsymbol{c}^{(t-1)}+\\boldsymbol{i}^{(t)} \\circ \\tilde{\\boldsymbol{c}}^{(t)}\n$$\nHidden state: read(\"output\") some content from the cell\n$$\n\\boldsymbol{h}^{(t)}=\\boldsymbol{o}^{(t)} \\circ \\tanh \\boldsymbol{c}^{(t)}\n$$\n![image-20200316220913640.png](https://i.loli.net/2020/04/04/pW7mMj9aDN53h8C.png)\n\n## Gated Recurrent Units(GRU)\n\na simpler alternative to the LSTM\n\nOn each timestep $t$ we have input $x^{(t)}$ and hidden state $h^{(t)}$ (no cell state)\n\nUpdate gate - controls what parts of hidden state are updated vs preserved\n$$\n\\boldsymbol{u}^{(t)}=\\sigma\\left(\\boldsymbol{W}_{u} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{U}_{u} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{u}\\right)\n$$\nReset gate - controls what parts of previous hidden state are used to compute new content\n$$\n\\boldsymbol{r}^{(t)}=\\sigma\\left(\\boldsymbol{W}_{r} \\boldsymbol{h}^{(t-1)}+\\boldsymbol{U}_{r} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{r}\\right)\n$$\nNew hidden state content - reset gate selects useful parts of prev hidden state. Use this and current input to compute new hidden content.\n$$\n\\tilde{\\boldsymbol{h}}^{(t)}=\\tanh \\left(\\boldsymbol{W}_{h}\\left(\\boldsymbol{r}^{(t)} \\circ \\boldsymbol{h}^{(t-1)}\\right)+\\boldsymbol{U}_{h} \\boldsymbol{x}^{(t)}+\\boldsymbol{b}_{h}\\right)\n$$\nHidden state - update gate simultaneously controls what is kept from previous hidden state, and what is updated to new hidden state content\n$$\n\\boldsymbol{h}^{(t)}=\\left(1-\\boldsymbol{u}^{(t)}\\right) \\circ \\boldsymbol{h}^{(t-1)}+\\boldsymbol{u}^{(t)} \\circ \\tilde{\\boldsymbol{h}}^{(t)}\n$$\n\n## LSTM vs GRU\n\nLSTM is a good default choice.\n\nRule of thumb: start with LSTM, but switch to GRU if you want something more efficient\n\n## Vanishing/exploding gradient\n\nadd more direct connections.\n\ne.g.: Residual connections, \"ResNet\". Skip-connections. The identity connection preserves information by default.\n\ne.g.: Dense connections, \"DenseNet\". \n\ne.g.:Highway connections, \"HighwayNet\".\n\n## Bidirectional RNNs\n\nmotivation - task: Sentiment Classification\n\n![image-20200317001951410.png](https://i.loli.net/2020/04/04/on9a2s1Y5jJiADb.png)\n\nNote: bidirection RNNs are only applicable if you have access to the entire input sentence. \n\nBERT is built on bidirectionality.\n\n## Multi-layer RNNs = stacked RNNs\n\nHigh-performing RNNs are often multi-layer(but aren't as deep as cn)\n\n","categories":["NLP"]},{"title":"Stanford CS224n Natural Language Processing Course8","url":"/2020/03/16/CS224n Classnotes Course8/","content":"\n# Course 8 - Translation, Seq2Seq, Attention\n\n## Machine Translation\n\nfrom the source language to the target language\n\n### Statistical Machine Translation\n\nCore idea: learn a probabilistic model from data.\n\nbest  English sentence $y$, given French sentence $x$\n$$\nargmax_y P(y|x)\\\\\n=argmax_y P(x|y) P(y)\n$$\nTranslation model($P(x|y)$) - Models how words and phrases should be translated. Learnt from **parallel data**\n\nnote: parallel data - pairs of human-translated french/english sentences.\n\nLanguage model($P(y)$) - Models how to write good English. Learnt from monolingual data.\n\n#### Learning alignment for SMT\n\nalignment is the *correspondence between particular words* in the translated sentence pair.\n\nAlignment can be **many-to-one** & **one-to-many** & **many-to-many**\n\nWe learn $P(x, a|y)$ as a combination of many factors.\n\n#### Decoding for SMT\n\nUse a **heuristic search algorithm** to search for the best translation, discarding hypotheses that are too low-probability. This process is called *decoding*\n\n### Neural Machine Translation\n\n**sequence-to-sequence(Seq2seq)** involves 2 RNNs. a **conditional language model**\n\n- conditional - its predictions are conditioned on the source sentence $x$\n\n![image-20200328233749255.png](https://i.loli.net/2020/04/04/gixM6LmGa5Ce7Js.png)\n\nOther tasks \n\n- summarization\n- dialogue\n- parsing\n- code generation (natural language -> Python code)\n\nNMT directly calculates $P(y|x)$\n$$\nP(y | x)=P\\left(y_{1} | x\\right) P\\left(y_{2} | y_{1}, x\\right) P\\left(y_{3} | y_{1}, y_{2}, x\\right) \\ldots P\\left(y_{T} | y_{1}, \\ldots, y_{T-1}, x\\right)\n$$\nQuestion: How to train?\n\nAnswer: Get a big parallel corpus...\n\nGreedy decoding has many problems, it's an **exhaustive search decoding**, instead we use **beam search decoding**\n\nCore idea: On each step of decoder, keep track of the *k most probable* partial translations (*hypotheses*)\n\n- *k* is the **beam size**\n\n$$\nscore(y_1,\\cdots,y_t)=logP_{LM}(y_1,\\cdots,y_t|x)=\\sum_{i=1}^t logP_{LM}(y_i|y_1,\\cdots,y_{i-1}, x)\n$$\n\n- Scores are all negative, and higher score is betterr\n- We search for high-scoring hypotheses, tracking top *k* on each step\n\nBeam search is *not guaranteed* to find optimal solution, but efficient.\n\nIn greedy decoding, usually we decode until the model produces a <END> token. For example, <START> he hit me with a pie <END>\n\nProblem：How to select top one with highest score? longer hypotheses have lower scores\n\nFix: Normalize by length. \n$$\n\\frac{1}{t}\\sum_{i=1}^t logP_{LM}(y_i|y_1,\\cdots,y_{i-1}, x)\n$$\n\n#### Advantages of NMT\n\n- better performance.\n  - more fluent\n  - better use of context\n  - better use of phrase similarities\n- A single neural network to be optimized end-to-end\n  - No subcomponents to be individually optimized\n- Requires much less human engineering effort\n  - No feature engineering\n\n#### Disadvantages of NMT\n\nCompared to SMT:\n\n- NMT is less interpretable\n  - Hard to debug\n- NMT is difficult to control\n\n#### How to evaluate?\n\nBLEU(Bilingual Evaluation Understudy)\n\nBLEU compares the machine-written translation to one or several human-written translations, and computes similarity-score based on:\n\n- n-gram precision\n- Plus a penalty for too-short system tranlations\n\n#### Difficulties remain\n\nOut-of-vocabulary words\n\nDomain mismatch between train and test data\n\nMaintaining context over longer text\n\nLow-resource language pairs\n\nNMT picks up **biases** in training data\n\nUninterpretable system do strange things.\n\n#### Attention\n\nBottleneck problem.\n\ndefinition - Given a set of vector *values*, and a vector *query*, **attention** is a technique to compute a weighted sum of the values, dependent on the query. \n\nvariants\n\n- Basic dot-product attention\n- Multiplicative attention\n- Additive attention","categories":["NLP"]},{"title":"How powerful is Graph Neural Networks?","url":"/2020/03/10/How_powerful_is_gnn/","content":"\n# How powerful is Graph Neural Networks?\n\nGNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. \n\nThere are two tasks of interest: \n\n(1) Node classiﬁcation\n\n(2) Graph classiﬁcation\n\nx Formally, the k-th layer of a GNN is\n$$\na_{v}^{(k)}=\\operatorname{AGGREGATE}^{(k)}\\left(\\left\\{h_{u}^{(k-1)}: u \\in \\mathcal{N}(v)\\right\\}\\right), \\quad h_{v}^{(k)}=\\mathrm{COMBINE}^{(k)}\\left(h_{v}^{(k-1)}, a_{v}^{(k)}\\right)\n$$\n\n\n In the pooling variant of GraphSAGE, AGGREGATE has been formulated as \n$$\na_{v}^{(k)}=\\operatorname{MAX}\\left(\\left\\{\\operatorname{ReLU}\\left(W \\cdot h_{u}^{(k-1)}\\right), \\forall u \\in \\mathcal{N}(v)\\right\\}\\right)\n$$\nand COMBINE could be concatenation followed by a linear mapping \n$$\nW \\cdot [h_{v}^{(k-1)}, a_{v}^{(k)}]\n$$\nGraph Convolutional Networks (GCN) - the element-wise *mean* pooling is used. AGGREGATE and COMBINE step\n$$\nh_{v}^{(k)}=\\operatorname{ReLU}\\left(W \\cdot \\operatorname{MEAN}\\left\\{h_{u}^{(k-1)}, \\forall u \\in \\mathcal{N}(v) \\cup\\{v\\}\\right\\}\\right)\n$$\nthe READOUT function aggregates node features from the ﬁnal iteration to obtain the entire graph’s representation $h_G$\n$$\nh_{G}=\\operatorname{READOUT}\\left(\\left\\{h_{v}^{(K)} | v \\in G\\right\\}\\right)\n$$\n\n**Deﬁnition1 (Multiset).** A multiset is a generalized concept of a set that allows multiple instances for its elements. More formally, a multiset is a 2-tuple $X = (S,m)$ where $S$ is the underlying set of $X$ that is formed from its distinct elements, and $m : S \\rightarrow \\mathbb{N}_{\\geq 1}$ gives the multiplicity of the elements. \n\n**Lemma2**. Let $G_1$ and $G_2$ be any two non-isomorphic graphs. If a graph neural network $A : G→\\mathbb{R}^d $ maps $G_1$ and $G_2$ to different embeddings, the Weisfeiler-Lehman graph isomorphism test also decides $G_1$ and $G_2$ are not isomorphic. \n\n**Theorem 3**. Let $A : G → \\mathbb{R}^d$ be a GNN. With a sufﬁcient number of GNN layers, A maps any graphs $G_1$ and $G_2$ that the Weisfeiler-Lehman test of isomorphism decides as non-isomorphic, to different embeddings if the following conditions hold: \n\na) $A$ aggregates and updates node features iteratively with \n$$\nh_{v}^{(k)}=\\phi\\left(h_{v}^{(k-1)}, f\\left(\\left\\{h_{u}^{(k-1)}: u \\in \\mathcal{N}(v)\\right\\}\\right)\\right)\n$$\nwhere the functions $f$, which operates on multisets, and $φ$ are injective. \n\nb) $A$’s graph-level readout, which operates on the multiset of node features ${h_{v}^{(k)}}$, is injective. \n\n**Lemma4**. Assume the input feature space $\\mathcal{X}$ is *countable*. Let $g^{(k)}$ be the function parameterized by a GNN’s k-th layer for $k = 1,...,L$, where $g^{(1)}$ is deﬁned on multisets $X \\subset \\mathcal{X}$ of bounded size. The range of $g^{(k)}$, i.e., the space of node hidden features ${h_{v}^{(k)}}$, is also countable for all  $k = 1,...,L$.\n\n*countable*: If a set A has the same cardinality as $\\mathbb{N}$, then we say that A is *countable*.\n\n**Lemma5**. Assume $\\mathcal{X}$ is countable. There exists a function $f : \\mathcal{X} →\\mathbb{R}^n$ so that  $h(X) =\\sum _{x\\in X} f(x)$ is unique for each multiset  $X \\subset \\mathcal{X}$  of bounded size. Moreover, any multiset function $g$ can be decomposed as $g (X) = \\varphi(\\sum _{x\\in X} f(x))$ for some function $\\varphi$.\n\n**Corollary 6**. Assume $\\mathcal{X}$ is countable. There exists a function $f : \\mathcal{X} →\\mathbb{R}^n$ so that for inﬁnitely many choices of $\\epsilon$, including all irrational numbers, $h(c,X) = (1 + \\epsilon) · f(c) + \\sum_{x \\in X} f(x)$ is unique for each pair $(c,X)$, where $c \\in \\mathcal{X}$ and $X \\subset \\mathcal{X}$ is a multiset of bounded size. Moreover, any function $g$ over such pairs can be decomposed as $g (c,X) = \\varphi((1 + \\epsilon)·f(c) + \\sum_{x\\in X} f(x))$ for some function $\\varphi$. \n\n## GIN - GRAPH ISOMORPHISM NETWORK \n\nTo update node representation - \n$$\nh_{v}^{(k)}=\\operatorname{MLP}^{(k)}\\left(\\left(1+\\epsilon^{(k)}\\right) \\cdot h_{v}^{(k-1)}+\\sum_{u \\in \\mathcal{N}(v)} h_{u}^{(k-1)}\\right)\n$$\nnode learnt can be directly used for tasks like **node classification and link prediction**.\n\nReadout function for graph classiﬁcation tasks. Given embeddings of individual nodes, readout function produces the embedding of the entire graph. \n$$\nh_{G}=\\operatorname{CONCAT}\\left(\\operatorname{READOUT}\\left(\\left\\{h_{v}^{(k)} | v \\in G\\right\\}\\right) | k=0,1, \\ldots, K\\right)\n$$\n\n## GNN - GRAPH NEURAL NETWORK\n\nGNN do not satisfy the conditions in Theorem 3, and we conduct ablation studies in \n\n*An ablation study typically refers to removing some “feature” of the model or algorithm, and seeing how that affects performance.\n\n(1) 1-layer perceptrons instead of MLPs\n\nWe are interested in understanding whether 1-layer perceptrons are enough for graph learning.\n\n**Lemma 7.** There exist ﬁnite multisets $X1 \\neq X2$ so that for any linear mapping $W$, $\\sum_{x\\in X_1} ReLU(Wx) =\\sum_{x\\in X_2} ReLU(Wx). $\n\n(2) mean or max-pooling instead of the sum.\n\nMean learns distributions, and max-pooling learns sets with distinct elements.\n\nConsider $X_1 = (S,m)$ and $X_2 = (S,k ·m)$, where $X_1$ and $X_2$ have the same set of distinct elements, but $X_2$ contains $k$ copies of each element of $X_1$.\n\n**Corollary 8.** Assume $X$ is countable. There exists a function $f : \\mathcal{X} → \\mathbb{R^n}$ so that for $h(X) = \\frac{1}{|X|}\\sum{x\\in X} f(x), h(X_1) = h(X_2)$ if and only if multisets $X_1$ and $X_2$ have the same distribution. That is, assuming $|X_2|\\geq|X_1|$, we have $X_1 = (S,m)$ and $X_2 = (S,k\\cdot m)$ for some $k \\in \\mathbb{N}_{\\geq 1}.$\n\n**Corollary 9**. Assume $X$  is countable. Then there exists a function $f : X → \\mathbb{R}^{\\infty}$ so that for $h(X) = max_{x\\in X} f(x), h(X_1) = h(X_2)$ if and only if $X_1$ and $X_2$ have the same underlying set. ","categories":["NLP"]},{"title":"GNN survey","url":"/2020/03/02/GNN-survey/","content":"\n# Background and Definitions\n\n## Background\n\nEarly studies fall into the category of recurrent graph neural networks(**RecGNNs**). With the success of CNNs,  new approach developed. (**ConvGNNs**)  ConvGNNs are divided into two main streams, the **spectral-based** approaches and the **spatial-based** approaches. \n\nNetwork embedding aims at representing network nodes as low-dimensional vector representations, preserving both network topology structure and node content information.\n\nGraph kernel methods employ a kernel function to measure the similarity between pairs of graphs.  Graph kernels can embed graphs or nodes into vector spaces by a mapping function. The difference between GNN and graph kernels is that this mapping function is deterministic rather than learnable.\n\n## Definitions\n\n**Deﬁnition 1 (Graph):** A graph is represented as $G = (V,E)$ where $V$ is the set of vertices or nodes, and $E$ is the set of edges. Let $v_i \\in V$ to denote a node and $e_{ij} = (v_i,v_j) \\in E$ to denote an edge pointing from $v_j$ to $v_i$. The neighborhood of a node $v$ is deﬁned as $N(v) = {u \\in V|(v,u) \\in E}$. The adjacency matrix $A$ is a $n \\times n$ matrix with $A_{ij} = 1$ if $e_{ij} \\in E$ and $A_{ij} = 0$ if $e_{ij} \\notin E$. A graph may have node attributes $ x_{v}$ , where $X \\in R^{n×d}$ is a node feature matrix with $x_{v} \\in R^d$  representing the feature vector of a node $v$. Meanwhile, a graph may have edge attributes $X^e$, where $X^e \\in R^{m×c}$ is an edge feature matrix with $x^{e}_{v,u} \\in R^c$ representing the feature vector of an edge $(v,u)$.\n\n**Deﬁnition 2 (Directed Graph):** A directed graph is a graph with all edges directed from one node to another. An undirected graph is considered as a special case of directed graphs where there is a pair of edges with inverse directions if two nodes are connected. A graph is undirected if and only if the adjacency matrix is symmetric. \n\n**Deﬁnition 3 (Spatial-Temporal Graph)**: A spatial-temporal graph is an attributed graph where the node attributes change dynamically over time. The spatial-temporal graph is deﬁned as $G^{(t)} = (V,E,X^{(t)})$ with $X^{(t)} \\in R^{n×d}$. \n\n# Categorization and Frameworks\n\nRecurrent Neural Network - aim to learn node representations with recurrent neural architectures.\n\nConvolutional graph neural networks - generate a node $v$’s representation by aggregating its own features $x_v$ and neighbors’ features $x_u$, where $u \\in N(v)$ \n\nGraph autoencoders(GAE)  learn network embeddings and graph generative distributions\n\n","tags":["NLP"]},{"title":"ubuntu18.04+nvidia driver+cuda+cudnn install","url":"/2020/02/29/ubuntu18-04-nvidia-driver-cuda-cudnn爬坑全记录/","content":"\n# Install Ubuntu18.04 \n\naccording to this blog: https://blog.csdn.net/silver1225/article/details/100393719\n\n# install nvidia driver\n\n## First, disable original nvidia driver, nouveau\n\n1. delete the original nvidia driver\n\n```\nsudo apt-get remove nvidia-*\n```\n\n2. forbid nouveau\n\n```\nsudo gedit /etc/modprobe.d/blacklist.conf\n```\n\nand add this in the bottom\n\n```\nblacklist nouveau\noptions nouveau modeset=0\n```\n\n3. execute\n\n```\nsudo update-initramfs -u\nreboot \n```\n\n4. see the version of nouveau, if nothing returns, you succeed.\n\n```\nlsmod | grep nouveau\n```\n\n## Second, install your nvidia driver.\n\n1. search your nvidia driver according to your version at https://www.nvidia.cn/Download/index.aspx?lang=cn\n\n2. update all\n\n```\nsudo apt-get update\n```\n\n3. install gcc\n\n```\nsudo apt install build-essential\n```\n\n4. show the version of your nvidia\n\n```\nlshw -numeric -C display\n```\n\n5. install\n\n```\nsudo chmod a+x NVIDIA-Linux-x86_64-418.43.run\nsudo ./NVIDIA-Linux-x86_64-418.43.run --no-opengl-files --no-x-check --no-nouveau-check\n```\n\n6. check whether you succeed or not\n\n```\nnvidia-smi\n```\n\n## Third, download cuda\n\ndownload cuda at https://developer.nvidia.com/cuda-downloads\n\n```\nsudo sh cuda_10.0.130_410.48_linux.run\n```\n\nand select \n\n```\n1 accept #同意安装\n2 n #不安装Driver，因为已安装最新驱动\n3 y #安装CUDA Toolkit\n4 <Enter> #安装到默认目录\n5 y #创建安装目录的软链接\n6 y #复制Samples一份到家目录\n```\n\nThen\n\n```\nsudo vim ~/.bashrc\n```\n\nAdd this to the file\n\n```\nexport PATH=/usr/local/cuda-10.1/bin${PATH:+:$PATH}} \nexport LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}\n```\n\nGo back to command line\n\n```\nsource ~/.bashrc\nsudo vim /etc/profile\nexport PATH=/usr/local/cuda/bin:$PATH\nsudo vim /etc/ld.so.conf.d/cuda.conf\n```\n\nAdd this to the file.\n\n# Last\n\n```\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable, grad\n\nSIZE=[1, 1, 171*21, 171*21]\ninput = Variable(torch.cuda.FloatTensor(*SIZE).uniform_(), requires_grad=True)\nconv1 = nn.Conv2d(1, 1, kernel_size=3, stride=1, dilation=1, padding=1,bias=False).cuda()\noutput = conv1(input)\nloss = output.sum()\nloss.backward()\n```\n\nrun this, if no error, you SUCCEED!","categories":["Ubuntu18.04"]},{"title":"Weibo - Delete all your followings and posts","url":"/2020/02/11/weibo_delete_all/","content":"\n## Delete all your followings \n\n1. open Google Chrome\n2. Press F12\n3. click `console`\n4. paste the codes\n\n```\n/* 点击批量管理 */\n$(\".btn_link.S_txt1\").click();\n/* 勾选全部 */\n$$('.member_li').forEach(l => l.click());\n/* 点击取消关注 */\n$('.W_btn_a[node-type=\"cancelFollowBtn\"]').click();\n/* 点击确认按钮 */\n$('[node-type=\"ok\"]').click();\n```\n\n## Delete all your posts\n\n```\n// ==UserScript==\n// @name         Weibored.js\n// @namespace    https://vito.sdf.org\n// @version      0.2.0\n// @description  删除所有微博\n// @author       Vito Van\n// @match        https://weibo.com/p/*\n// @grant        none\n// ==/UserScript==\n'use strict';\nvar s = document.createElement('script');\ns.setAttribute(\n'src',\n'https://lib.sinaapp.com/js/jquery/2.0.3/jquery-2.0.3.min.js'\n);\ns.onload = function() {\nsetInterval(function() {\nif (!$('a[action-type=\"feed_list_delete\"]')) {\n$('a.next').click();\n} else {\n$('a[action-type=\"feed_list_delete\"]')[0].click();\n$('a[action-type=\"ok\"]')[0].click();\n}\n// scroll bottom let auto load\n$('html, body').animate({ scrollTop: $(document).height() }, 'slow');\n}, 800);\n};\ndocument.head.appendChild(s);\n```\n\n","categories":["Others"]},{"title":"Stanford CS224n Natural Language Processing Course6","url":"/2020/02/10/CS224n Classnotes Course6/","content":"\n# Course 6 - Language Models and RNNs\n\n## Language Modeling\n\nis the task of predicting what word comes next. \n$$\nP\\left(\\boldsymbol{x}^{(t+1)} | \\boldsymbol{x}^{(t)}, \\ldots, \\boldsymbol{x}^{(1)}\\right)\n$$\nalso, **assigns probability to a piece of text**\n$$\n\\begin{aligned} P\\left(\\boldsymbol{x}^{(1)}, \\ldots, \\boldsymbol{x}^{(T)}\\right) &=P\\left(\\boldsymbol{x}^{(1)}\\right) \\times P\\left(\\boldsymbol{x}^{(2)} | \\boldsymbol{x}^{(1)}\\right) \\times \\cdots \\times P\\left(\\boldsymbol{x}^{(T)} | \\boldsymbol{x}^{(T-1)}, \\ldots, \\boldsymbol{x}^{(1)}\\right) \\\\ &=\\prod_{t=1}^{T} P\\left(\\boldsymbol{x}^{(t)} | \\boldsymbol{x}^{(t-1)}, \\ldots, \\boldsymbol{x}^{(1)}\\right) \\end{aligned}\n$$\n\n### n-gram Language Models\n\n#### Definition\n\nA **n-gram** is a chunk of n consecutive words\n\n#### Idea\n\nCollect statistics about how frequent different n-grams are, and use these to predict next word.\n\n#### Sparity Problem 1: what if \"students opened their *w*\" never occurred in data\n\nSolution-Smoothing: Add small delta to the count for every *w* \n\n#### Sparity Problem2: what if \"students opened their\" never occurred in data? \n\nSolution-backoff: Just condition on \"opened their\" instead\n\n## How to build a *neural* Language Model - Recurrent Neural Networks\n\n### A fixed-window neural Language Model\n\n### Recurrent Neural Networks(RNN)\n\n![image-20200210211740128.png](https://i.loli.net/2020/03/04/VGcU3SeqL7JhZoM.png)\n\n\n\n**Core idea**: Apply the same weight W repeatedly. \n\n![image-20200210220449695.png](https://i.loli.net/2020/03/04/2vle5KkWsRcUJ8F.png)\n\n\n\n**Advantages**\n\n- Can process any length input\n- use information from many steps back in theory\n- Model size doesn't increase for longer input\n- Same weights applied on every steps. \n\n**Disadvantages**\n\n- slow computation\n- difficult to access information from many steps back practically \n\n#### Training a RNN Language Model\n\n- Get a **big corpus of text** which is a sequence of word $x^{1}, \\cdots, x^{T}$\n\n- Feed into RNN-LM\n\n- Loss function on step t is **cross-entropy**\n  $$\n  J^{(t)}(\\theta)=C E\\left(\\boldsymbol{y}^{(t)}, \\hat{\\boldsymbol{y}}^{(t)}\\right)=-\\sum_{w \\in V} \\boldsymbol{y}_{w}^{(t)} \\log \\hat{\\boldsymbol{y}}_{w}^{(t)}=-\\log \\hat{\\boldsymbol{y}}_{\\boldsymbol{x}_{t+1}}^{(t)}\n  $$\n\n- Average this to get **overall loss **for entire training set\n  $$\n  J(\\theta)=\\frac{1}{T} \\sum_{t=1}^{T} J^{(t)}(\\theta)=\\frac{1}{T} \\sum_{t=1}^{T}-\\log \\hat{y}_{x_{t+1}}^{(t)}\n  $$\n\n- However, computing loss and gradients across **entire corpus** is too expensive. In practice, consider $x^{1}, \\cdots, x^{T}$ as a **sentence** (or a **document**)\n- Instead, using **SGD** to compute loss $J(\\theta)$ for a sentence, compute gradients and update weights. Repeat.\n\n#### Backpropagation for RNNs\n\nQuestions: derivative of $J^{(t)}(\\theta)$ w.r.t the **repeated** weight matrix \n\nAnswer:\n$$\n\\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{W}_{\\boldsymbol{h}}}=\\left.\\sum_{i=1}^{t} \\frac{\\partial J^{(t)}}{\\partial \\boldsymbol{W}_{\\boldsymbol{h}}}\\right|_{(i)}\n$$\n\n#### Generating text with a RNN Language Model.\n\nrepeated sampling \n\n#### Evaluating Language Models: Perplexity\n\n$$\nperplexity =\\prod_{t=1}^{T}\\left(\\frac{1}{P_{\\mathrm{LM}}\\left(x^{(t+1)} | \\boldsymbol{x}^{(t)}, \\ldots, \\boldsymbol{x}^{(1)}\\right)}\\right)^{1 / T} = \\exp(J(\\theta))\n$$\n\nlower perplexity is better!\n\n#### Why should we care about Language Modeling?\n\nLanguage Modeling is a **benchmark task** that helps us **measure our progress** on understanding language.\n\n**subcomponent** of many NLP tasks.\n\nRNNs can be used for tagging, NER, part-of-speech tagging.\n\nRNNs can be used for sentence classification, sentiment classification.\n\nRNNs can be used as an encoder module, question answering, machine translation.\n\nRNN described is called **vanilla RNN**\n\nGRU, LSTM(chocolate), multi-layer RNNs","categories":["NLP"]},{"title":"Weibo Hot Topic Web Scrawler - to monitor the public sentiment in China","url":"/2020/02/10/weibo hottopics crawler/","content":"\n### Web Scrawler is dangerous\n\nFirst and foremost, do not touch web scrawler if you are 100% sure that you wanna do this.\n\n### Similar topic:\n\nhttps://nobugs.dev/2019/07/14/webscrawler/\n\nwebsite: enlightent\n\n### This time\n\nWe choose to use the package of `requests` and package `json` to get the result of hot topics  in weibo, similar website to twitter which has real-time hot topics, a great way to monitor the public sentiment in China.\n\n### Code Implementation\n\n\n\n```python\nimport requests\nimport json\n\n\nheaders = {\n    'charset': \"utf-8\",\n    'Accept-Encoding': \"gzip\",\n    'referer': \"https://servicewechat.com/wx90ae92bbd13ec629/11/page-frame.html\",\n    'content-type': \"application/x-www-form-urlencoded\",\n    'User-Agent': \"Mozilla/5.0 (Linux; Android 9; Redmi Note 7 Build/PKQ1.180904.001; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/68.0.3440.91 Mobile Safari/537.36 MicroMessenger/7.0.3.1400(0x2700033B) Process/appbrand0 NetType/WIFI Language/zh_CN\",\n    'Host': \"www.eecso.com\",\n    'Connection': \"keep-alive\",\n    'cache-control': \"no-cache\",\n    'Origin': 'https://www.weibotop.cn',\n}\n\n\nwith open('微博热搜.csv', 'w', encoding='gbk') as f:\n    f.write('时间,排名,热搜内容,上榜时间,最后时间\\n')\n\ntimeid = 77594\ndateUrl = \"https://www.eecso.com/test/weibo/apis/getlatest.php?timeid={}\"\ncontentUrl = \"https://www.eecso.com/test/weibo/apis/currentitems.php?timeid={}\"\nn = 1\ndays = 42 #需获取2.10往前多少天的数据\ninterval = 720 #改为1则是爬所有数据（该网站2分钟记录一次） 24*30 = 720\nwhile True:\n    dateResponse = requests.request(\"GET\", dateUrl.format(timeid), headers=headers,verify=False)\n    contentResponse = requests.request(\"GET\", contentUrl.format(timeid), headers=headers,verify=False)\n    timeid = 77594-interval*n #77594为2020/2/10 12:00的timeid，720为一天timeid的间隔\n    print(timeid)\n    n += 1\n    dateJson = json.loads(dateResponse.text)\n    json_obj = json.loads(contentResponse.text)\n    #print(dateJson)\n    \n    for index,item in enumerate(json_obj):\n        date = dateJson[1]\n        rank = str(index+1)\n        hotTopic = item[0]\n        onTime = item[1]\n        lastTime = item[2]\n        save_res = date+\",\"+rank+\",\"+hotTopic+','+onTime+','+lastTime+'\\n'\n        with open('微博热搜.csv','a',encoding='gbk',errors='ignore') as f:\n            f.write(save_res)\n    if n > days:\n        break\n\n```\n\n","categories":["Web Crawler"]},{"title":"Walter Rudin, Functional Analysis brief theorems(Chapter General Theory) - ongoing","url":"/2020/02/10/Functional Analysis brief theorems/","content":"\n[Booklink](https://59clc.files.wordpress.com/2012/08/functional-analysis-_-rudin-2th.pdf)\n\n# Part I - General Theory\n\n## Chapter 1 -  Topological Vector Spaces\n\n**1.2 Normed spaces** A vector space X is said to be a normed space if to every x in X there is associated a nonnegative real number llxll, called the norm of x, in such a way that \n\n(a) $\\quad\\|x+y\\| \\leq\\|x\\|+\\|y\\|$ for all $x$ and $y$ in $X$\n(b) $\\quad\\|\\alpha x\\|=|\\alpha|\\|x\\|$ if $x \\in X$ and $\\alpha$ is a scalar,\n(c) $\\quad\\|x\\|>0$ if $x \\neq 0$\n\nIn any metric space, the open ball with center at x and radius r is the set \n$$\nB_r(x) = \\{y:d(x,y)<r\\}\n$$\n\n\n\n\n","categories":["Mathematics"]},{"title":"Stanford CS224n Natural Language Processing Course5","url":"/2020/02/04/CS224n Classnotes Course5/","content":"\n# Course 5 - Dependency Parsing\n\n## Syntactic Structure: Consistency and Dependency\n\n### 1. Two views of linguistic structure: Constituency  = phrase stucture grammar = context-free grammars (CFGs)\n\n**Phrase structure** organizes words into nested constituents\n\n## Dependency Grammar and Treebanks\n\nDependency structure shows which words depend on which other words.\n\nDependency syntax postulates that syntactic structure consists of relations between lexical items, normally binary asymmetric relations(\"arrows\") called **dependencies**\n\nThe arrows are commonly **typed** with the name of grammatical relations(subject, prepositional object, apposition, etc.)\n\nUsually, dependencies form a tree(connected, acyclic, single-head)\n\n## Transition-based dependency parsing\n\nA simple form of greedy discriminative dependency parser.\n$$\nStart: \\sigma=[\\mathrm{ROOT}], \\beta=w_{1}, \\ldots, w_{n}, A=\\emptyset\\\\\n1. Shift\\qquad \\sigma, w_{i}|\\beta, A \\rightarrow \\sigma| w_{i}, \\beta, A\\\\\n2. Left-Arc_r\\qquad \\sigma\\left|w_{i}\\right| w_{j}, \\beta, A \\rightarrow\n\\sigma | w_{j}, \\beta, \\operatorname{A}\\mathop{\\cup} \\left\\{r\\left(w_{j}, w_{i}\\right)\\right\\}\n\\\\\n3. Right-Arc_r \\qquad \\sigma\\left[w_{i} | w_{j}, \\beta, A \\rightarrow\\right.\n\n\\sigma | w_{i}, \\beta, \\operatorname{A}\\mathop{\\cup}\\left\\{r\\left(w_{i}, w_{j}\\right)\\right\\}\n\\\\\nFinish: \\beta=\\varnothing\n$$\n\n### 2005 MaltParser, Nivre and Hall 2005\n\nEach action is predicted by a discriminative classifier over each legal move(like softmax classifier)\n\n- Max of 3 untyped choices, max of |R| * 2 + 1 when typed\n- Features: top of stack word, POS; first in the buffer word, POS\n\nIt provides very **fast linear time parsing** with fractionally lower accuracy\n\n### Evaluation of Dependency Parsing: (labeled) dependency accuracy\n\n![image-20200205111717409.png](https://i.loli.net/2020/04/04/DRoBKOlQ2tEhA9W.png)\n\n## Neural dependency parsing\n\n### Why train a neural dependency parsing?\n\nProblem #1: sparse\n\nProblem #2: incomplete\n\nProblem #3: expensive computation(95%+ of parsing time is consumed by feature computation)\n\n### 2014, chen and manning\n\nEnglish parsing to Stanford Dependencies:\n\n- Unlabeled attachment score = head\n- Labeled attachment score = head and label\n\n### Google \n\nbigger, deeper networks + beam search + CRF-style inference\n\n### Graph-based neural dependency parsing\n\nover 95%\n\n","categories":["NLP"]},{"title":"Stanford CS224n Natural Language Processing Course4","url":"/2020/02/03/CS224n Classnotes Course4/","content":"\n# Course 4 - Backpropagation and computation graphs\n\n## Matrix gradients for our simple neural net and some tips\n\nChain rule:\n$$\n\\begin{aligned}\n\\boldsymbol{s}=&\\boldsymbol{u}^{T} \\boldsymbol{h} \\\\\n\\boldsymbol{h}=& f(\\boldsymbol{z}) \\\\ \\boldsymbol{z}=& W \\boldsymbol{x}+\\boldsymbol{b} \\end{aligned}\n$$\n\n$$\n\\begin{aligned} \n\\frac{\\partial s}{\\partial \\boldsymbol{W}}=&\\frac{\\partial s}{\\partial \\boldsymbol{h}} \\frac{\\partial \\boldsymbol{h}}{\\partial \\boldsymbol{z}} \\frac{\\partial \\boldsymbol{z}}{\\partial \\boldsymbol{W}} \\\\\n=& \\delta\\frac{\\partial z}{\\partial W}\\\\\n=& \\delta \\frac{\\partial}{\\partial W} Wx + b \\\\\n=& \\delta^{T}x+b\n\\end{aligned}\n$$\n\n**Question** Should I use available \"pre-trained\" word vectors?\n\n**Answer** almost always\n\n**Question** Should I update(\"fine tune\") my own word vectors?\n\n**Answer** \n\n- If you only have a **small** training data set, **don't** train the word vectors\n- If you have a **large** dataset, it probably will work better to **train = update = fine-tune** word vectors to the task\n\n## Computation graphs and backpropagation\n\n```python\nclass MultiplyGate(object):\n    def forward(x,y):\n        z = x*y\n        self.x = x\n        self.y = y\n        return z\n    def backward(dz):\n        dx = self.y * dz\n        dy = self.x * dz\n        return [dx,dy]\n```\n\n## Stuff you should know\n\n### Regularization to prevent overfitting\n\n$$\nJ(\\theta)=\\frac{1}{N} \\sum_{i=1}^{N}-\\log \\left(\\frac{e^{f_{y_{i}}}}{\\sum_{c=1}^{C} e^{f_{c}}}\\right)\\left[+\\lambda \\sum_{k} \\theta_{k}^{2}\\right]\n$$\n\nPrevents **overfitting** when we have a lot of features\n\n### Vectorization\n\n### Nonlinearities\n\nlogistic(\"sigmoid\")\n$$\nf(z)=\\frac{1}{1+\\exp (-z)}\n$$\ntanh\n$$\nf(z)=\\tanh (z)=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \\\\\n\\tanh (z) = 2\\operatorname{logistic}(2z) - 1\n$$\nhard tanh\n$$\n\\operatorname{HardTanh} (x)=\\left\\{\\begin{array}{cl}{-1} & {\\text { if } x<-1} \\\\ {x} & {\\text { if }-1<=x<=1} \\\\ {1} & {\\text { if } x>1}\\end{array}\\right.\n$$\nReLU: \n$$\nrect(z) = \\max(z,0)\n$$\nLeaky ReLU/Parametric ReLU\n\n### Initialization\n\nXavier initialzation has variance inversely proportional to fan-in n_in and fan-out n_out:\n$$\n\\operatorname{Var}\\left(W_{i}\\right)=\\frac{2}{n_{\\mathrm{in}}+n_{\\mathrm{out}}}\n$$\n\n### Optimizers\n\nSGD/Adagrad/RMSprop/Adak/SparseAdam\n\n### Learning rates\n\n- You can just use a constant learning rate\n\n- Better results can generally be obtained by allowing learning rates to decrease as you train\n\n  - by hand: halve the learning rate every k epochs\n\n  - By a formula, for epoch t\n\n  - $$\n    lr = lr_0 e^{-kt}\n    $$\n\n  - Fancier method like cyclic learning rates(q.v.)\n\n- Fancier optimizers still use a learning rate but it may be an initial rate that the optimizer shrinks - so may be able to start high","categories":["NLP"]},{"title":"Stanford CS224n Natural Language Processing Course3","url":"/2020/02/02/CS224n Classnotes Course3/","content":"\n# Course 3 - Neural Networks\n\n## Classification review/introduction\n\nGenerally we have a **training dataset** consisting of **samples**\n$$\n\\left\\{x_{\\mathrm{i}}, y_{\\mathrm{i}}\\right\\}_{\\mathrm{i}=1}^{\\mathrm{N}}\n$$\nx_i are inputs, eg words, sentences, documents, etc\n\ny_j are labels we try to predict. \n\n- classes: sentiment, named entities, buy/sell decision\n- other words\n- later: multi-word sequences\n\nTraditional ML/Stats approach: assume x_i are fixed, train softmax/logistic regression weights *W in R(C,d)* to determine a decision boundary as in the picture. \n\n### Method: Training with softmax and cross-entropy loss\n\nFor each *x*, predict:\n$$\np(y | x)=\\frac{\\exp \\left(W_{y} \\cdot x\\right)}{\\sum_{c=1}^{C} \\exp \\left(W_{c} \\cdot x\\right)}\n$$\nFor each training example (x,y), to **maximize the probability of the correct class** *y*, or **minimize the negative log probability**\n$$\n-\\log p(y | x)=-\\log \\left(\\frac{\\exp \\left(f_{y}\\right)}{\\sum_{c=1}^{C} \\exp \\left(f_{c}\\right)}\\right)\n$$\nFYI: Cross entropy in the full dataset is \n$$\nJ(\\theta)=\\frac{1}{N} \\sum_{i=1}^{N}-\\log \\left(\\frac{e^{f_{y_{i}}}}{\\sum_{c=1}^{C} e^{f_{c}}}\\right)\n$$\n\n\n## Neural networks introduction\n\nCommonly in NLP deep learning:\n\n- We learn **both** W **and** word vectors X\n- We learn **both** conventional parameters **and** representations\n\n$$\n\\nabla_{\\theta} J(\\theta)=\\left[\\begin{array}{c}{\\nabla_{W_{1}}} \\\\ {\\vdots} \\\\ {\\nabla_{W_{d}}} \\\\ {\\nabla_{x_{a a r d v a r k}}} \\\\ {\\vdots} \\\\ {\\nabla_{x_{z e b r a}}}\\end{array}\\right] \\in \\mathbb{R}^{C d+V d}\n$$\n\n## Named Entity Recognition\n\nThe task: **find** and **classify** name in text.\n\n### NER on word sequences\n\nWe predict entities by classifying words in context and then extracting entities as word subsequences.\n\nBIAO encoding \n\n### Why might NER be hard?\n\n- hard to work out boundaries of entity\n- hard to know if something is an entity\n- hard to know class of unknown/novel entity\n- entity class is ambiguous and depends on context\n\n## Binary true vs. corrupted word window classification\n\n### Binary word window classification\n\nProblem: ambiguity\n\nIdea: **classify a word in its context window** of neighboring words.\n\nSolution: train softmax classifer to classify a center word by taking **concatenation of word vectors surrounding it** in a window\n$$\n(x_1, x_2, \\cdots, x_n)\n$$\n","categories":["NLP"]},{"title":"Stanford CS224n Natural Language Processing Course2","url":"/2020/02/01/CS224n Classnotes Course2/","content":"\n# Course 2 - Word Vectors and Word Senses\n\nFirstly, some analogy by calculating the linear space similarity by word vectors.\n\n![image-20200201062227420.png](https://i.loli.net/2020/04/04/CSBItLwc53PdYuX.png)\n\nDefinition of Code:\n\n```python\ndef analogy(x1, x2, y1):\n    result = model.most_similar(positive=[y1,x2], negative=[x1])\n    return result[0][0]\n```\n\nSome Implementations:\n\n```python\nanalogy('japan','japanese','australia') # australian\nanalogy('man','king','woman') # queen\nanalogy('australia','beer','france') # champagne\n```\n\nNext thing: **PCA Scatterplot**\n\n```python\ndef display_pca_scatterplot(model, words=None, sample=0):\n    if words == None:\n        if sample > 0:\n            words = np.random.choice(list(model.vocab.keys()), sample)\n        else:\n            words = [ word for word in model.vocab ]\n        \n        word_vectors = np.array([model[w] for w in words])\n        \n        twodim = PCA().fit_transform(word_vectors)[:,:2]\n        \n        plt.figure(figsize=(6,6))\n        plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n        for word, (x,y) in zip(words, twodim):\n            plt.text(x+0.05, y+0.05, word)\n```\n\n##  Word Vectors and word2vec\n\n### 1. Review: Main idea of word2vec\n\n- Iterate through each word of the whole corpus\n- Predict surrounding words using word vectors\n\n$$\nP(o | c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\n$$\n\n## Optimization Basics: Gradient Descent(cs229 Optimization)\n\nIdea: for current value of theta, calculate gradient of J(theta), then take **small step in the direction of negative gradient**. \n\n**Update equation**(in matrix notation)\n$$\n\\theta^{n e w}=\\theta^{o l d}-\\alpha \\nabla_{\\theta} J(\\theta)\n$$\n**Update equation**(in a single parameter)\n$$\n\\theta_{j}^{n e w}=\\theta_{j}^{o l d}-\\alpha \\frac{\\partial}{\\partial \\theta_{j}^{o l d}} J(\\theta)\n$$\nAlgorithm:\n\n```python\nwhile True:\n    theta_grad = evaluate_gradient(J, corpus, theta)\n    theta = theta - alpha * theta_grad\n```\n\n### Stochastic Gradient Descent(SGD)\n\nSolution: Repeatedly sample windows, and update after each one\n\nAlgorithm\n\n```python\nwhile True:\n    window = sample_window(corpus)\n    theta_grad = evaluate_gradient(J, window, theta)\n    theta = theta - alpha * theta_grad\n```\n\n### Stochastic Gradient with word vectors:\n\nIteratively take gradients at each such window for SGD\n\nBut in each window, we only have at most *2m+1* words, so it's very sparse!\n$$\n\\nabla_{\\theta} J_{t}(\\theta)=\\left[\\begin{array}{c}{0} \\\\ {\\vdots} \\\\ {\\nabla_{v_{v i k e}}} \\\\ {\\vdots} \\\\ {0} \\\\ {\\vdots} \\\\ {\\nabla_{u_{\\text {learning}}}} \\\\ {\\vdots}\\end{array}\\right] \\in \\mathbb{R}^{2 d V}\n$$\nSolution: \n\n- need sparse matrix update operations to only update certain rows of full embedding matrices *U* and *V*\n- need to keep around a hash for word vectors\n\n### Word2Vec: More details\n\nTwo model variants:\n\n- Skip-grams(SG): Predict context words given center word\n- Continuous Bag of Words(CBOW): Predict center word from context words\n\n=> **Skip-gram model**\n\nAdditional efficiency in training: \n\n- Negative sampling\n\nPaper: \"Distributed Representations of Words and Phrases and their Compositionality\"(Mikolov et al.2013)\n\n**Overall objective function**(maximize):\n$$\nJ(\\theta)=\\frac{1}{T} \\sum_{t=1}^{T} J_{t}(\\theta)\\\\\nJ_{t}(\\theta)=\\log \\sigma\\left(u_{o}^{T} v_{c}\\right)+\\sum_{i=1}^{k} \\mathbb{E}_{j \\sim P(w)}\\left[\\log \\sigma\\left(-u_{j}^{T} v_{c}\\right)\\right]\n$$\nThe sigmoid function:\n$$\n\\sigma(x)=\\frac{1}{1+e^{-x}}\n$$\nSo we maximize the probability of two words co-occurring in the first log\n$$\nJ_{n e g-s a m p l e}\\left(\\boldsymbol{o}, \\boldsymbol{v}_{c}, \\boldsymbol{U}\\right)=-\\log \\left(\\sigma\\left(\\boldsymbol{u}_{o}^{\\top} \\boldsymbol{v}_{c}\\right)\\right)-\\sum_{k=1}^{K} \\log \\left(\\sigma\\left(-\\boldsymbol{u}_{k}^{\\top} \\boldsymbol{v}_{c}\\right)\\right)\n$$\nMaximize probability that real outside word appears, minimize prob. that random word appear around center word.\n\nThe unigram distribution U(w)\n$$\nP(w)=U(w)^{3 / 4} / Z\n$$\nraised to the 3/4 power, which makes less frequent words be sampled more often.\n\n## Can we capture this essence more effectively by counting?\n\nWith a co-occurrence matrix *X*\n\n- 2 options: windows vs. full document\n- Window: Similar to word2vec, use window around each word -> captures both syntactic(POS) and semantic information\n- Word-document co-occurrence matrix will give general topic leading to *Latent Semantic Analysis*\n\nModels are less robust due to increase in size with vocabulary.\n\nSolution: Low dimensional vectors\n\nIdea: store most information in a fixed, small number of dimensions\n\n### Singular Value Decomposition of co-occurrence matrix *X* - SVD\n\n$$\nX = U \\Sigma V^{\\top}\n$$\n\n### Hacks to X (several used in Rohde et al. 2005)\n\nProblem: function words(*the, he, has*) are too frequent -> syntax has too much impact\n\nSolution\n\n-  min(X,t) t~= 100, Ignore them all\n-  Use Pearson correlations instead of counts, then set negative values to 0\n\n## The GloVe model of word vectors\n\n### Crucial insight\n\nRatios of co-occurrence probabilities can encode meaning components\n$$\nw_{i} \\cdot w_{j}=\\log P(i | j)\\\\\nJ=\\sum_{i, j=1}^{V} f\\left(X_{i j}\\right)\\left(w_{i}^{T} \\tilde{w}_{j}+b_{i}+\\tilde{b}_{j}-\\log X_{i j}\\right)^{2}\n$$\n**Merits**\n\n- Fast training\n- Scalable to hug corpora\n- Good performance even with small corpus and small vectors\n\n## Evaluating word vectors\n\n### Intrinsic\n\n- Evaluation on a specific/intermediate subtask\n- Fast to compute\n- Helps to understand that system\n- Not clear if helpful\n\nMethods: \n\n1. Cosine distance of word vectors\n2. Word vector distances and their correlation with human judgments\n\n### Extrinsic\n\n- Evaluation on a real task\n- Can take a long time to compute accuracy\n- Unclear if subsystem is the problem or its interaction\n- If replacing exactly one subsystem with another improves accuracy\n\nExample: NER\n\n## Word senses\n\n#### Word senses and word sense ambiguity\n\nMost words have lots of meanings, especially common words. ","categories":["NLP"]},{"title":"Stanford CS224n Natural Language Processing Course1","url":"/2020/01/31/CS224n Classnotes Course1/","content":"\n[**Course Link(including Slides and Materials)**](http://web.stanford.edu/class/cs224n/index.html#schedule)\n\n[**Online Video Link**](http://onlinehub.stanford.edu/cs224)\n\n[**Online Video Link in Chinese**](https://www.bilibili.com/video/av46216519)\n\n# Course 1 - Introduction and Word Vectors\n\nLanguage isn't a formal system. Language is glorious chaos. *by Chris Manning*\n\n**Target**\n\n- Modern methods: Recurrent networks, attentions\n- Big Picture\n- Build system in PyTorch: Word meaning, dependency parsing, machine translation, question answering.\n\n## 1. how do we represent the meaning of a word?\n\nsignifier(symbol) <=> signified(idea or thing) = denotational semantics\n\n## 2. how do we have usable meaning in a computer?\n\n**Common Solution** Use e.g. WordNet containing lists of **synonym sets** and **hypernyms**\n\n**Problems** \n\n- missing nuance: proficient = good in some contexts\n- new meanings of words: wicked, badass, ninja\n- Subjective\n- Requires human labor to create and adapt\n- Can't compute accurate and word similarity\n\n### Representing words as discrete symbols\n\n**one-hot vectors** => **encode similarity in the vector themselves**\n\n**Distributional semantics**: A word's meaning is given by the words that frequently appear close-by.\n\n**Word Vectors**: a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts.\n\nNote: Word Vectors = Word Embeddings = Word Representations. All distributed representation\n$$\nbanking = \\left[\n \\begin{matrix}\n   0.286 \\\\\n   0.792  \\\\\n   -0.177 \\\\\n   -0.107 \\\\\n   0.109 \\\\\n   -0.542\\\\\n   0.349\\\\\n   0.271\\\\\n   0.487\n  \\end{matrix}\n  \\right] \\tag{1}\n$$\n\n## 3. Word2Vec: Overview\n\nIdea:\n\n- a large corpus of text\n- every word in a fixed vocabulary is represented by a **vector**\n- go through each position *t* in the text, which has a center word *c* and context words *o*\n- Use the **similarity of the word vectors** for *c* and *o* to **calculate the probability** of *o* given *c*\n- **Keep Adjusting the word vectors** to maximize this probability\n\n$$\nlikelihood = L(\\theta) = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m \\atop j \\neq 0} P\\left(w_{t+j} | w_{t} ; \\theta\\right)\n$$\n\nNote: theta is all variables to be optimized\n\n### Objective function J\n\nsometimes called **cost / loss function**\n$$\nJ(\\theta)=-\\frac{1}{T} \\log L(\\theta)=-\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m} \\log P\\left(w_{t+j} | w_{t} ; \\theta\\right)\n$$\n**Minimizing objective function <=> Maximizing predictive accuracy**\n\n*Question: How to calculate P?*\n\n*Answer: We use two vectors per word w*\n\n- `v_w` when *w* is a center word\n- `u_w` when *w* is a context word\n\nThen for a center word *c* and a context word *o*:\n\n### Prediction function\n\n$$\nP(o | c)=\\frac{\\exp \\left(u_{o}^{T} v_{c}\\right)}{\\sum_{w \\in V} \\exp \\left(u_{w}^{T} v_{c}\\right)}\n$$\n\n### To train the model: Compute all vector gradients\n\n- Recall: theta represents **all** model parameters, in one long vector\n\n- In our cases\n\n$$\n\\theta=\\left[\\begin{array}{l}{v_{\\text {aardvark}}} \\\\ {v_{a}} \\\\ {\\vdots} \\\\ {v_{z e b r a}} \\\\ {u_{\\text {aardvark}}} \\\\ {u_{a}} \\\\ {\\vdots} \\\\ {u_{z e b r a}}\\end{array}\\right] \\in \\mathbb{R}^{2 d V}\n$$\n\n- Remember: every word has two vectors\n\n- We optimize these parameters by walking down the gradient\n\n\n\n\n","categories":["NLP"]},{"title":"Leetcode MySQL cases","url":"/2020/01/30/MYSQL_Leetcode/","content":"\n## Problem 175 - Combine Two Tables\n\n### Question:\n\nTable: `Person`\n\n```\n+-------------+---------+\n| Column Name | Type    |\n+-------------+---------+\n| PersonId    | int     |\n| FirstName   | varchar |\n| LastName    | varchar |\n+-------------+---------+\nPersonId is the primary key column for this table.\n```\n\nTable: `Address`\n\n```\n+-------------+---------+\n| Column Name | Type    |\n+-------------+---------+\n| AddressId   | int     |\n| PersonId    | int     |\n| City        | varchar |\n| State       | varchar |\n+-------------+---------+\nAddressId is the primary key column for this table.\n```\n\nWrite a SQL query for a report that provides the following information for each person in the Person table, regardless if there is an address for each of those people:\n\n```\nFirstName, LastName, City, State\n```\n\n\n\n### Solution\n\nThis is a easy question that interferes with **LEFT JOIN** technique, FYI, please notice the word **ON**\n\n```mysql\nSELECT FirstName, LastName, City, State\nFROM \nPerson a\nLEFT JOIN\nAddress b\nON a.PersonId = b.PersonId;\n```\n\n\n\n## Problem 176 - Second Highest Salary\n\n### Question:\n\nWrite a SQL query to get the second highest salary from the `Employee` table.\n\n```\n+----+--------+\n| Id | Salary |\n+----+--------+\n| 1  | 100    |\n| 2  | 200    |\n| 3  | 300    |\n+----+--------+\n```\n\nFor example, given the above Employee table, the query should return `200` as the second highest salary. If there is no second highest salary, then the query should return `null`.\n\n```\n+---------------------+\n| SecondHighestSalary |\n+---------------------+\n| 200                 |\n+---------------------+\n```\n\n### Solution:\n\nrecommended way.\n\nRuntime: faster than 92%\n\nMemory: Less than 100%\n\n```mysql\nSELECT MAX(salary) AS SecondHighestSalary\nFROM Employee\nWHERE salary < (SELECT MAX(salary) FROM Employee);\n```\n\nother solution which might be slower for using the **DISTINCT** \n\nRuntime: faster than 39%\n\nMemory: Less than 100%\n\n```mysql\nSELECT \nCASE WHEN COUNT(a.SecondHighestSalary) > 0 THEN a.SecondHighestSalary ELSE null END AS SecondHighestSalary\nFROM\n(SELECT \nDISTINCT Salary AS SecondHighestSalary \nFROM\nEmployee ORDER BY Salary DESC LIMIT 1,1) a;\n```\n\n## Problem 178 - Rank Scores\n\n### Question:\n\nWrite a SQL query to rank scores. If there is a tie between two scores, both should have the same ranking. Note that after a tie, the next ranking number should be the next consecutive integer value. In other words, there should be no \"holes\" between ranks.\n\n```\n+----+-------+\n| Id | Score |\n+----+-------+\n| 1  | 3.50  |\n| 2  | 3.65  |\n| 3  | 4.00  |\n| 4  | 3.85  |\n| 5  | 4.00  |\n| 6  | 3.65  |\n+----+-------+\n```\n\nFor example, given the above `Scores` table, your query should generate the following report (order by highest score):\n\n```\n+-------+------+\n| Score | Rank |\n+-------+------+\n| 4.00  | 1    |\n| 4.00  | 1    |\n| 3.85  | 2    |\n| 3.65  | 3    |\n| 3.65  | 3    |\n| 3.50  | 4    |\n+-------+------+\n```\n\n### Solution 1: With Variables\n\nWith variables: one for the current rank and one for the previous score.\n\n \n\n```\nSELECT  Score,  @rank := @rank + (@prev <> (@prev := Score)) as 'Rank' FROM  Scores,  (SELECT @rank := 0, @prev := -1) init ORDER BY Score desc\n \n```\n\n### Solution 2: Always Count\n\nThis one counts, for each score, the number of distinct greater or equal scores.\n\n```\nSELECT  Score,  (SELECT count(distinct Score) FROM Scores WHERE Score >= s.Score) as 'Rank' FROM Scores s ORDER BY Score desc\n \n```\n\n### Solution 3: Always Count, pre-uniqued\n\nSame as the previous one, but faster because I have a subquery that \"uniquifies\" the scores first. Not entirely sure *why* it's faster, I'm guessing MySQL makes tmp a temporary table and uses it for every outer Score.\n\n \n\n```\nSELECT  Score,  (SELECT count(*) FROM (SELECT distinct Score s FROM Scores) tmp WHERE s >= Score) as 'Rank' FROM Scores ORDER BY Score desc\n \n```\n\n### Solution 4: Filter/Count Scores^2\n\n```\nSELECT s.Score, count(distinct t.score) Rank FROM Scores s JOIN Scores t ON s.Score <= t.score GROUP BY s.Id ORDER BY s.Score desc\n```\n\n\n\n## Problem 181 - Employees Earning More Than Their Managers\n\n### Question:\n\nThe `Employee` table holds all employees including their managers. Every employee has an Id, and there is also a column for the manager Id.\n\n```\n+----+-------+--------+-----------+\n| Id | Name  | Salary | ManagerId |\n+----+-------+--------+-----------+\n| 1  | Joe   | 70000  | 3         |\n| 2  | Henry | 80000  | 4         |\n| 3  | Sam   | 60000  | NULL      |\n| 4  | Max   | 90000  | NULL      |\n+----+-------+--------+-----------+\n```\n\nGiven the `Employee` table, write a SQL query that finds out employees who earn more than their managers. For the above table, Joe is the only employee who earns more than his manager.\n\n```\n+----------+\n| Employee |\n+----------+\n| Joe      |\n+----------+\n```\n\n### Solution:\n\nRuntime: faster than 93.96%\n\nMemory: Less than 100%\n\n```mysql\nSELECT a.Name AS Employee\nFROM\n(SELECT * FROM Employee WHERE ManagerId IS NOT NULL) a\nLEFT JOIN\nEmployee b\nON a.ManagerId = b.Id\nWHERE a.Salary > b.Salary;\n```\n\n","categories":["SQL"]},{"title":"SQL cheetsheet","url":"/2019/08/29/SQL-cheetsheet/","content":"## SQL Cheetsheet\nIt's more convenient to store the data in your MySQL instead of Hive to improve the efficiency and facilitate retrieval of the data if the amount of data is not that large. Here are some tips about SQL, according to [IBM Databases and SQL for Data Science](https://www.coursera.org/learn/sql-data-science/)\n\n------\n\n### String Patterns, Ranges, Sorting and Grouping\n\n**using string pattern**: like '%%'\n\n**using a range**: between ... and ...\n\n**using a set of values**: in ('', '')\n\n**Sorting**: order by ... / order by ... desc / order by 2(column number)\n\n**Eliminating Duplicates**: distinct \n\nIt seems like you don't need to write distinct(), distinct also works.\n\n**Group by clause**\n\n**Restricting the result set - Having clause**: Having \n\nworks only with the GROUP BY clause.\n\n------\n\n### Functions, Sub-Queries, Multiple Tables\n\n**Aggregate Functions**: sum(), min(), max(), avg()\n\n**Scaler and String functions**: round(), length(), ucase, lcase\n\n**Date and Time functions**: year(), month(), day(), dayofmonth(), dayofweek(), dayofyear(), week(), hour(), minute(), second()\n\n**Date or time arithmetic**: + 1 Days, CURRENT_DATE, CURRENT_TIME\n\n**Accessing multiple tables with with Implicit join**: \n\nselect * from employees E, departments D where E.DEP_ID = D.DEPT_ID_DEP;","categories":["SQL"]},{"title":"Discovery towards Web Scrawler","url":"/2019/07/14/webscrawler/","content":"\n### Web Scrawler is dangerous\n\nFirst and foremost, do not touch web scrawler if you are 100% sure that you wanna do this.\n\n### Beginning\n\nThe target website is enlightent, a third-party data website which have data of my company and component. \n\nWe need to do some background research first. The problems I encountered are listed:\n\n- Need to log in with WeChat account by QR code\n- Simulate click (by package selenium)\n- It's a dynamic website, you need to wait for its information loaded (by package time)\n- Write into MySQL (by package Pymysql)\n\n### Process\n\n##### First problem cannot be solve due to the security of WeChat.\n\n##### Second problem\n\nStep 1: Find the pattern in html. Using chrome, just ctrl+u or ctrl+shift+i. It needs your patience to find the thing you want. If you mistaken the pattern, you cannot get the information you want\n\nStep 2: Choose the function: by_path or by_class. The tricky point is that if there is only one class, it's okay to use by_class, if there are more than two classes, selenium would choose the first class as your output. As a result, I choose by_path\n\nStep 3: Install chrome driver according to your chrome version. Be sure to download into /anaconda3/lib/site-packages.\n\n```python\n# Choose daily\ntime.sleep(5)\ndriver.find_elements_by_id(\"rank-date-btn\")[0].click()\n# Choose year\ntime.sleep(1)\ndriver.find_element_by_class_name(\"datepicker-switch\").click()\n# Choose target month\ntime.sleep(1)\n# driver.find_element_by_class_name(\"month\").click()\nmonth_url = \"/html/body/div[2]/div[3]/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[1]/div/div/div/div[2]/table/tbody/tr/td/span[%s]\" % (l+1)\ndriver.find_element_by_xpath(month_url).click()\n# Choose target Date\ntime.sleep(1)\n# day = driver.find_element_by_class_name(\"day\").click()\nxpath = \"/html/body/div[2]/div[3]/div[1]/div[1]/div[1]/div[2]/div[2]/div[2]/div[1]/div/div/div/div/table/tbody/tr[%d]/td[%d]\" % ((start_date) // 7 + 1, (start_date) % 7 + 1)\ndriver.find_element_by_xpath(xpath).click()\n# Press enter\ntime.sleep(1)\ndriver.find_element_by_id(\"choose-rank\").click()\ntime.sleep(1)\n\n```\n\n##### Third Problem: Data Processing\n\n```python\nalbum_separately = string_list[j][string_list[j].find('data-name='):string_list[j].find('data-channeltype=\"tv\"')]\nif j != 0: \n    album.append(album_separately.replace('data-name=','').replace('\"',''))\npercentage_separately = string_list[j][string_list[j].find('<td class=\"sort rank-playTimesPredicted active\" style=\"\"><span>'):string_list[j].find('</span></td><td class=\"rank-playTimes\" style=\"\">')]\npercentage.append(percentage_separately.replace('<td class=\"sort rank-playTimesPredicted active\" style=\"\"><span>',''))\nclick_separately = string_list[j][string_list[j].find('</td><td class=\"rank-playTimes\" style=\"\"><span>'):string_list[j].find('</span><span class=\"star-playtimes\">')]\nif len(click_separately) >= 10:\n    click_separately = string_list[j][string_list[j].find('</td><td class=\"rank-playTimes\" style=\"\"><span>'):string_list[j].find('</span></td><td class=\"rank-average m-change\" style=\"\">')]\nclick.append(click_separately.replace('</td><td class=\"rank-playTimes\" style=\"\"><span>','').replace('</span><span class=\"star-playtimes\">',''))\n```\n\n##### Fourth problem: Log into your MySQL and use python like MySQL!\n\n```python\ndb = DB('your database')\ndb.insert(dataset)\n```\n\n \n\n### Take Care! Be sure to add time.sleep() when you do it!","categories":["Web Crawler"]},{"title":"Random Forest Classification from the bottom layer","url":"/2019/05/05/RandomForest/","content":"import packages we need\n\n    import random\n    import numpy as np\n    import sys\n\nThe feature list is an array of the possible feature indicies to use. This prevents splitting on the same feature multiple times and runs a rough C45 algorithm.\n\nIn pseudocode, the general algorithm for building decision trees is:\n\n1.Check for base cases\n2.For each attribute a\n3.Find the normalized information gain ratio from splitting on a\n4.Let a_best be the attribute with the highest normalized information gain\n5.Create a decision node that splits on a_best\n6.Recur on the sublists obtained by splitting on a_best, and add those nodes as children of node\n\n    class TreeNode:\n        def __init__(self, dataSet, featureList, parent=None):\n            self.featureNumber = None  #This is the trained index of the feature to split on\n            self.featureList = featureList \n            self.threshold = None     #This is the trained threshold of the feature to split on\n            self.leftChild = None\n            self.rightChild = None\n            self.dataSet = dataSet\n            self.parent = parent\n    \n        def c45Train(self):\n            if(self.dataSet.isPure()):\n                #gets the label of the first data instance and makes a leaf node\n                #classifying it. \n                label = self.dataSet.getData()[0].getLabel()\n                leaf = LeafNode(label)\n                return leaf\n            #If there are no more features in the feature list\n            if len(self.featureList) == 0:\n                labels = self.dataSet.getLabelStatistics()\n                bestLabel = None\n                mostTimes = 0\n    \n                for key in labels:\n                    if labels[key] > mostTimes:\n                        bestLabel = key\n                        mostTimes = labels[key]\n                #Make the leaf node with the best label\n                leaf = LeafNode(bestLabel)\n                return leaf\n    \n            #Check all of the features for the split with the most \n            #information gain. Use that split.\n            currentEntropy = self.dataSet.getEntropy()\n            currentLength = self.dataSet.getLength()\n            infoGain = -1 * sys.maxsize\n            bestFeature = 0\n            bestLeft = None\n            bestRight = None\n            bestThreshold = 0\n    \n            #Feature Bagging, Random subspace\n            num = int(np.ceil(np.sqrt(len(self.featureList))))\n            featureSubset = random.sample(self.featureList, num)\n    \n            for featureIndex in featureSubset:\n                #Calculate the threshold to use for that feature\n                threshold = self.dataSet.betterThreshold(featureIndex)\n    \n                (leftSet, rightSet) = self.dataSet.splitOn(featureIndex, threshold)\n    \n                leftEntropy = leftSet.getEntropy()\n                rightEntropy = rightSet.getEntropy()\n                #Weighted entropy for this split\n                newEntropy = (leftSet.getLength() / currentLength) * leftEntropy + (rightSet.getLength() / currentLength) * rightEntropy\n                #Calculate the gain for this test\n                newIG = currentEntropy - newEntropy\n    \n                if(newIG > infoGain):\n                    #Update the best stuff\n                    infoGain = newIG\n                    bestLeft = leftSet\n                    bestRight = rightSet\n                    bestFeature = featureIndex\n                    bestThreshold = threshold\n    \n            newFeatureList = list(self.featureList)\n            newFeatureList.remove(bestFeature)\n    \n            #Another base case, if there are no good features to split on\n            if bestLeft.getLength() == 0 or bestRight.getLength() == 0:\n                labels = self.dataSet.getLabelStatistics()\n                bestLabel = None\n                mostTimes = 0\n    \n                for key in labels:\n                    if labels[key] > mostTimes:\n                        bestLabel = key\n                        mostTimes = labels[key]\n                #Make the leaf node with the best label\n                leaf = LeafNode(bestLabel)\n                return leaf\n    \n            self.threshold = bestThreshold\n            self.featureNumber = bestFeature\n    \n            leftChild = TreeNode(bestLeft, newFeatureList, self)\n            rightChild = TreeNode(bestRight, newFeatureList, self)\n    \n            self.leftChild = leftChild.c45Train()\n            self.rightChild = rightChild.c45Train()\n    \n            return self\n            \n        def __str__(self):\n            return str(self.featureList)\n    \n        def __repr__(self):\n            return self.__str__()\n                    \n        def classify(self, sample):\n            '''\n            Recursivly traverse the tree to classify the sample that is passed in. \n            '''\n    \n            value = sample.getFeatures()[self.featureNumber]\n    \n            if(value < self.threshold):\n                #Continue down the left child    \n                return self.leftChild.classify(sample)\n    \n            else:\n                #continue down the right child\n                return self.rightChild.classify(sample)\n\n\n    class LeafNode:\n        '''\n        A leaf node is a node that just has a classification \n        and is used to cap off a tree.\n        '''\n    \n        def __init__(self, classification):\n            self.classification = classification\n    \n        def classify(self, sample):\n            #A leaf node simply is a classification, return that\n            #This is the base case of the classify recursive function for TreeNodes\n            return self.classification\n\n\n    class C45Tree:\n        '''\n        A tree contains a root node and from here\n        does the training and classification. Tree objects also\n        are responsible for having the data that they use to train.\n        '''\n    \n        def __init__(self, data):\n            self.rootNode = None\n            self.data = data\n    \n        def train(self):\n            '''\n            Trains a decision tree classifier on data set passed in. \n            The data set should contain a good mix of each class to be\n            classified.\n            '''\n            length = self.data.getFeatureLength()\n            featureIndices = range(length)\n            self.rootNode = TreeNode(self.data, featureIndices)\n            self.rootNode.c45Train()\n    \n        def classify(self, sample):\n            '''\n            Classify a sample based off of this trained tree.\n            '''\n    \n            return self.rootNode.classify(sample)\n\nThen we introduce the RandomForest algorithm\n\n\n    from C45Tree import C45Tree\n\n\n    class RandomForest(object):\n        \"\"\"A random forest object with the default of using a C45Tree \n        for each of the trees in the random forest. To train the forest\n        create an instance of it then call train on a TraingData object\"\"\"\n        def __init__(self, data, numberOfTrees=100):\n            '''\n            Initialize the random forest. \n            Each tree has a bag of the data associated with it.\n            '''\n            self.data = data    #The data that the trees will be trained on\n            self.numberOfTrees = numberOfTrees\n            self.forest = []\n    \n            for i in range(numberOfTrees):\n                bag = data.getBag()\n                self.forest.append(C45Tree(bag))\n    \n        def train(self):\n            '''\n            Train the random forest trees.\n            '''\n            for tree in self.forest:\n                tree.train()\n    \n        def classify(self, sample):\n            '''\n            Classify a data sample by polling the trees.\n            '''\n    \n            #Create an empty dictionary\n            votes = {}\n            #Tally the votes, for each tree classify the sample\n            for tree in self.forest:\n                label = tree.classify(sample)\n                if label in votes:\n                    votes[label] += 1\n                else:\n                    votes[label] = 1\n    \n            bestLabel = None\n            mostTimes = 0\n            #Find the label with the most votes\n            for key in votes:\n                if votes[key] > mostTimes:\n                    bestLabel = key\n                    mostTimes = votes[key]\n    \n            #Return the most popular label\n            return bestLabel      ","categories":["ML Concepts"]},{"title":"Deep dive into KAGGLE","url":"/2019/04/16/kaggle_1/","content":"[click here to kaggle official website for this dataset](https://www.kaggle.com/ronitf/heart-disease-uci)\n\n## Context\nThis database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.\n\n## Content\n\n### Attribute Information: \n> 1. age \n> 2. sex \n> 3. chest pain type (4 values) \n> 4. resting blood pressure \n> 5. serum cholestoral in mg/dl \n> 6. fasting blood sugar > 120 mg/dl\n> 7. resting electrocardiographic results (values 0,1,2)\n> 8. maximum heart rate achieved \n> 9. exercise induced angina \n> 10. oldpeak = ST depression induced by exercise relative to rest \n> 11. the slope of the peak exercise ST segment \n> 12. number of major vessels (0-3) colored by flourosopy \n> 13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\nThe names and social security numbers of the patients were recently removed from the database, replaced with dummy values. One file has been \"processed\", that one containing the Cleveland database. All four unprocessed files also exist in this directory.\n\nTo see Test Costs (donated by Peter Turney), please see the folder \"Costs\"\n\n# Introduction\nTo solve this problem, here we introduce jupyter notebook by python, implementing random forest to give us a brief scope of this dataset.\nFirstly, we import the packages needed.\n\n\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    import seaborn as sns #for plotting\n    from sklearn.ensemble import RandomForestClassifier #for the model\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.tree import export_graphviz #plot tree\n    from sklearn.metrics import roc_curve, auc #for model evaluation\n    from sklearn.metrics import classification_report #for model evaluation\n    from sklearn.metrics import confusion_matrix #for model evaluation\n    from sklearn.model_selection import train_test_split #for data splitting\n    import eli5 #for purmutation importance\n    from eli5.sklearn import PermutationImportance\n    import shap #for SHAP values\n    from pdpbox import pdp, info_plots #for partial plots\n    np.random.seed(123) #ensure reproducibility\n    pd.options.mode.chained_assignment = None  #hide any pandas warnings\n\n# The data\nNext, we load the data.\n\n    dt = pd.read_csv(\"../input/heart.csv\")\n\nIt's a clean, easy to understand set of data. However, the meaning of some of the column headers are not obvious. Here's what they mean,\n\n**age**: The person's age in years\n**sex**: The person's sex (1 = male, 0 = female)\n**cp**: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n**trestbps**: The person's resting blood pressure (mm Hg on admission to the hospital)\n**chol**: The person's cholesterol measurement in mg/dl\n**fbs**: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n**restecg**: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n**thalach**: The person's maximum heart rate achieved\n**exang**: Exercise induced angina (1 = yes; 0 = no)\n**oldpeak**: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n**slope**: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n**ca**: The number of major vessels (0-3)\n**thal**: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n**target**: Heart disease (0 = no, 1 = yes)\n\n**Diagnosis**: The diagnosis of heart disease is done on a combination of clinical signs and test results. The types of tests run will be chosen on the basis of what the physician thinks is going on 1, ranging from electrocardiograms and cardiac computerized tomography (CT) scans, to blood tests and exercise stress tests 2.\n\nLooking at information of heart disease risk factors led me to the following: high cholesterol, high blood pressure, diabetes, weight, family history and smoking 3. According to another source 4, the major factors that can't be changed are: increasing age, male gender and heredity. Note that thalassemia, one of the variables in this dataset, is heredity. Major factors that can be modified are: Smoking, high cholesterol, high blood pressure, physical inactivity, and being overweight and having diabetes. Other factors include stress, alcohol and poor diet/nutrition.\n\nI can see no reference to the 'number of major vessels', but given that the definition of heart disease is \"...what happens when your heart's blood supply is blocked or interrupted by a build-up of fatty substances in the coronary arteries\", it seems logical the more major vessels is a good thing, and therefore will reduce the probability of heart disease.\n\nGiven the above, I would hypothesis that, if the model has some predictive ability, we'll see these factors standing out as the most important.\n\nDeclare some of the columns to make the indication clear\n\n    dt.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']\n\n\n    dt['sex'][dt['sex'] == 0] = 'female'\n    dt['sex'][dt['sex'] == 1] = 'male'\n    \n    dt['chest_pain_type'][dt['chest_pain_type'] == 1] = 'typical angina'\n    dt['chest_pain_type'][dt['chest_pain_type'] == 2] = 'atypical angina'\n    dt['chest_pain_type'][dt['chest_pain_type'] == 3] = 'non-anginal pain'\n    dt['chest_pain_type'][dt['chest_pain_type'] == 4] = 'asymptomatic'\n    \n    dt['fasting_blood_sugar'][dt['fasting_blood_sugar'] == 0] = 'lower than 120mg/ml'\n    dt['fasting_blood_sugar'][dt['fasting_blood_sugar'] == 1] = 'greater than 120mg/ml'\n    \n    dt['rest_ecg'][dt['rest_ecg'] == 0] = 'normal'\n    dt['rest_ecg'][dt['rest_ecg'] == 1] = 'ST-T wave abnormality'\n    dt['rest_ecg'][dt['rest_ecg'] == 2] = 'left ventricular hypertrophy'\n    \n    dt['exercise_induced_angina'][dt['exercise_induced_angina'] == 0] = 'no'\n    dt['exercise_induced_angina'][dt['exercise_induced_angina'] == 1] = 'yes'\n    \n    dt['st_slope'][dt['st_slope'] == 1] = 'upsloping'\n    dt['st_slope'][dt['st_slope'] == 2] = 'flat'\n    dt['st_slope'][dt['st_slope'] == 3] = 'downsloping'\n    \n    dt['thalassemia'][dt['thalassemia'] == 1] = 'normal'\n    dt['thalassemia'][dt['thalassemia'] == 2] = 'fixed defect'\n    dt['thalassemia'][dt['thalassemia'] == 3] = 'reversable defect'\n\n# The Model\nDepending on package sklearn, we split the data - test data 70%, training data 30%\n\n    X_train, X_test, y_train, y_test = train_test_split(dt.drop('target', 1), dt['target'], test_size = .2, random_state=10) #split the data\n\nUsing random forest methodology\n\n    model = RandomForestClassifier(max_depth=5)\n    model.fit(X_train, y_train)\n\nplot the result of random forest\n\n    estimator = model.estimators_[1]\n    feature_names = [i for i in X_train.columns]\n    \n    y_train_str = y_train.astype('str')\n    y_train_str[y_train_str == '0'] = 'no disease'\n    y_train_str[y_train_str == '1'] = 'disease'\n    y_train_str = y_train_str.values\n\ncode from https://towardsdatascience.com/how-to-visualize-a-decision-tree-from-a-random-forest-in-python-using-scikit-learn-38ad2d75f21c\n\n    export_graphviz(estimator, out_file='tree.dot', \n                    feature_names = feature_names,\n                    class_names = y_train_str,\n                    rounded = True, proportion = True, \n                    label='root',\n                    precision = 2, filled = True)\n    \n    from subprocess import call\n    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n    \n    from IPython.display import Image\n    Image(filename = 'tree.png')\n\nEvaluate the model\n\n    y_predict = model.predict(X_test)\n    y_pred_quant = model.predict_proba(X_test)[:, 1]\n    y_pred_bin = model.predict(X_test)\n\nFit with Cnfusion model\n\n    confusion_matrix = confusion_matrix(y_test, y_pred_bin)\n\nHere we propose the sensitvity and specificity\n$$sensitivity = \\frac{TF}{TP+FN}$$\n$$specificity = \\frac{TN}{TN+FP}$$\n\n    total=sum(sum(confusion_matrix))\n    \n    sensitivity = confusion_matrix[0,0]/(confusion_matrix[0,0]+confusion_matrix[1,0])\n    print('Sensitivity : ', sensitivity )\n    \n    specificity = confusion_matrix[1,1]/(confusion_matrix[1,1]+confusion_matrix[0,1])\n    print('Specificity : ', specificity)\n\nNext, we check ROC curve.\n\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\n    \n    fig, ax = plt.subplots()\n    ax.plot(fpr, tpr)\n    ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for diabetes classifier')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.grid(True)\n\nAnother common metric is the Area Under the Curve, or AUC.\n\n    auc(fpr, tpr)\n\n# Explanation\nHere we implement the shap value to explain our model.\n\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(X_test)\n    \n    shap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")\n\n[for more](https://www.kaggle.com/tentotheminus9/what-causes-heart-disease-explaining-the-model)","categories":["Kaggle"]}]