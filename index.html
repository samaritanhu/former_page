<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Xinyi&#39;s gadget">
<meta property="og:url" content="http://nobugs.dev/index.html">
<meta property="og:site_name" content="Xinyi&#39;s gadget">
<meta property="article:author" content="Xinyi HU">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://nobugs.dev/"/>





  <title>Xinyi's gadget</title>
  








<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

<!-- ҳ����С���� -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>


    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Xinyi's gadget</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-knowme">
          <a href="/KnowMe" rel="section">
            
            KnowMe
          </a>
        </li>
      
        
        <li class="menu-item menu-item-resume">
          <a href="/about" rel="section">
            
            Resume
          </a>
        </li>
      
        
        <li class="menu-item menu-item-blogs">
          <a href="/archives" rel="section">
            
            Blogs
          </a>
        </li>
      
        
        <li class="menu-item menu-item-projects">
          <a href="/projects" rel="section">
            
            Projects
          </a>
        </li>
      
        
        <li class="menu-item menu-item-friends">
          <a href="/group" rel="section">
            
            Friends
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://nobugs.dev/2020/07/19/Hulu%20AI%20class/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xinyi HU">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xinyi's gadget">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/07/19/Hulu%20AI%20class/" itemprop="url">Hulu AI Class - Recommender Systems 1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-19T12:56:21+08:00">
                2020-07-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  881
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  5
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="The-aim-of-this-class"><a href="#The-aim-of-this-class" class="headerlink" title="The aim of this class"></a>The aim of this class</h1><ul>
<li>Basic concept of recommender systems</li>
<li>Simple formula and theory</li>
<li>Underlying intuition of each recommendation model</li>
<li>Pros and cons</li>
</ul>
<h1 id="What-is-recommender-system"><a href="#What-is-recommender-system" class="headerlink" title="What is recommender system?"></a>What is recommender system?</h1><p>Basic assumption of recommender systems</p>
<ul>
<li>Information overload</li>
<li>Users are unsure about what they are looking for(different from information retrieval)</li>
</ul>
<p>Target of (traditional) recommender systems</p>
<ul>
<li>Develop a <strong>mathematical model</strong> of an <strong>objective function</strong>($\mathcal{F}$) to predict how much a <strong>user</strong> will like an <strong>item</strong> in a given <strong>context</strong>.</li>
</ul>
<script type="math/tex; mode=display">
R=\mathcal{F}(u, i ; c)</script><p><img src="https://i.loli.net/2020/07/19/cRID18HTiMgsLCn.png" alt="rs1.png"></p>
<h1 id="Basic-Concept"><a href="#Basic-Concept" class="headerlink" title="Basic Concept"></a>Basic Concept</h1><ul>
<li><p>Ratings</p>
</li>
<li><ul>
<li>Explicit ratings: 5 star, like/dislike<em>(additional effort from users)</em></li>
<li>Implicit ratings: page views, click, effective playback, purchase records, whether or not listen to a music track<em>(easier to collect, less precise)</em></li>
</ul>
</li>
<li><p>Interaction matrix</p>
</li>
<li><ul>
<li>Matrix modelling user’s rating on item</li>
<li>User $u$’s rating towards item $i$ as $r_{u,i}$</li>
</ul>
</li>
</ul>
<p><img src="https://i.loli.net/2020/07/19/zPJmU6hX3jLNYBK.png" alt="rs1.2.png"></p>
<h1 id="Collaborative-Filtering-“物以类聚，人以群分”"><a href="#Collaborative-Filtering-“物以类聚，人以群分”" class="headerlink" title="Collaborative Filtering “物以类聚，人以群分”"></a>Collaborative Filtering “物以类聚，人以群分”</h1><ul>
<li><p><strong>Context-free</strong> recommender system</p>
</li>
<li><p>Intuition: <strong>the users who have agreed in the past tend to also agree in the future</strong></p>
</li>
<li><p>F: aggregate from nearest neighbor</p>
<script type="math/tex; mode=display">
r_{u, i}=\frac{\sum_{v} w_{u v} \cdot r_{v, i}}{\# \text { of neighbors }}</script></li>
</ul>
<ul>
<li><p>User-based CF: user neighbors (green block)</p>
</li>
<li><p>Item-based CF: item neighbors (blue block)</p>
</li>
<li><p>Hybrid: use both</p>
</li>
</ul>
<p><img src="https://i.loli.net/2020/07/19/i4CtNJbxlD39rLM.png" alt="image-20200719113651705.png"></p>
<p><img src="https://i.loli.net/2020/07/19/o7qUYWhifSOZcXy.png" alt="image-20200719113855912.png"></p>
<h2 id="Similarity-Calculation"><a href="#Similarity-Calculation" class="headerlink" title="Similarity Calculation"></a>Similarity Calculation</h2><ul>
<li>Cosine similarity</li>
<li>Pearson correlation</li>
<li>SimRank</li>
</ul>
<h2 id="Improvement-of-Similarity-Calculation"><a href="#Improvement-of-Similarity-Calculation" class="headerlink" title="Improvement of Similarity Calculation"></a>Improvement of Similarity Calculation</h2><ul>
<li>Not all neighbors are equally “valuable” i.e. agreement on commonly liked items not so informative as agreement on controversial items<ul>
<li>Give more weight to items that have a higher variance</li>
</ul>
</li>
<li>Low number of co-rated items may introduce bias<ul>
<li>Reduce “confidence” when the number of co-rated items is low</li>
</ul>
</li>
<li>All neighbors are not very “similar”<ul>
<li>Set similarity threshold</li>
<li>Set maximum number of neighbors</li>
</ul>
</li>
</ul>
<h2 id="Pros-and-Cons"><a href="#Pros-and-Cons" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h2><ul>
<li><p>Pros </p>
<ul>
<li>Easy to implement </li>
<li>Good explanation </li>
</ul>
</li>
<li><p>Cons</p>
<ul>
<li>Large Memory needed </li>
<li>Sparsity issue</li>
</ul>
</li>
</ul>
<h1 id="Matrix-Factorization"><a href="#Matrix-Factorization" class="headerlink" title="Matrix Factorization"></a>Matrix Factorization</h1><ul>
<li><strong>Context-free</strong> recommender system</li>
<li>Intuition: express <strong>higher-level attributes</strong></li>
<li><p>Generate user and item <strong>embeddings</strong> </p>
<ul>
<li>Latent vectors</li>
<li><p>(A group of) similar users/items have similar embeddings</p>
<p>$\mathcal{F}$  : dot product of embeddings $r_{u,i}=p_u^T q_i$</p>
<script type="math/tex; mode=display">
\min_{p,q} \sum_{(u,i)\in K} (r_{u,i}-p_u^T q_i)^2~+\lambda(||p||+||q||)^2</script></li>
</ul>
</li>
</ul>
<h2 id="Improvement-of-Matrix-Factorization"><a href="#Improvement-of-Matrix-Factorization" class="headerlink" title="Improvement of Matrix Factorization"></a>Improvement of Matrix Factorization</h2><ul>
<li><p>$\mathcal{F}$  could be more complicated</p>
<script type="math/tex; mode=display">
r_{u, i}={\mu}+b_{i}+{b_{u}}+p_{u}^{T} q_{i}</script></li>
<li><p>Global bias </p>
<ul>
<li>Average ratings in the whole catalog </li>
</ul>
</li>
<li>Item bias <ul>
<li>Users rate diﬀerent categories in diﬀerent ways </li>
</ul>
</li>
<li>User bias<ul>
<li>Some users tend to give high ratings while others are not</li>
</ul>
</li>
</ul>
<h2 id="Pros-and-Cons-1"><a href="#Pros-and-Cons-1" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h2><ul>
<li>Pros: <ul>
<li>Better generalization capability: even though two users haven’t rated any same movies, it’s still possible to ﬁnd the similarity between them through embeddings </li>
<li>Low memory: only need to store low-dimensional latent vectors, no need to store large user behaviors </li>
</ul>
</li>
<li>Cons<ul>
<li>Hard to explain </li>
<li>Sparsity</li>
</ul>
</li>
</ul>
<h1 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h1><ul>
<li><p>Intuition: add <strong>context</strong> information to our model </p>
</li>
<li><p>Recommendation as <strong>classiﬁcation</strong>: </p>
</li>
<li><p>$\mathcal{F}$ </p>
<script type="math/tex; mode=display">
r_{u, i}=\frac{1}{1+e^{-(W x+b)}}</script></li>
</ul>
<ul>
<li><p>User, item, context =&gt; <strong>categorical</strong> feature vector</p>
<ul>
<li>Example: gender, time diﬀerence within the day, show genre as feature </li>
<li>$[0, 1, 0, 0, 0, 1, 0]$ =&gt; female, view on morning, target show is <em>Horror Movie</em> </li>
</ul>
</li>
<li><p>Diﬀerent weight of features, <strong>learned by gradient descent</strong> </p>
</li>
<li><p>Sigmoid projection</p>
</li>
</ul>
<h2 id="Logistic-Regression-——-why"><a href="#Logistic-Regression-——-why" class="headerlink" title="Logistic Regression —— why?"></a>Logistic Regression —— why?</h2><ul>
<li><p>Why categorical feature vector? </p>
<ul>
<li>Some non-numerical features, e.g. device: “roku”, “web”, “android”…</li>
<li>Absolute value of feature does not make sense </li>
<li>Feature crossing </li>
<li>Fast computation of sparse vector</li>
</ul>
</li>
<li><p>Why sigmoid projection?</p>
<ul>
<li>Output between 0 and 1 </li>
<li>Good mathematical form </li>
<li>Maximum entropy model</li>
</ul>
</li>
</ul>
<h2 id="Pros-and-Cons-2"><a href="#Pros-and-Cons-2" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h2><ul>
<li>Pros<ul>
<li>Good explanation (feature importance) </li>
<li>Good parallelism, fast model training </li>
<li>Low training cost </li>
<li>Online training </li>
</ul>
</li>
<li>Cons<ul>
<li>Manually craft features</li>
<li>Limited model expressiveness (linear model)</li>
</ul>
</li>
</ul>
<h2 id="Problems-of-Logistic-Regression"><a href="#Problems-of-Logistic-Regression" class="headerlink" title="Problems of Logistic Regression"></a>Problems of Logistic Regression</h2><p>Simpson’s Paradox</p>
<h1 id="Factorization-Machine-FM"><a href="#Factorization-Machine-FM" class="headerlink" title="Factorization Machine (FM)"></a>Factorization Machine (FM)</h1><ul>
<li><p>Intuition: <strong>feature crossing</strong></p>
</li>
<li><p>$\mathcal{F}$</p>
<script type="math/tex; mode=display">
y(x)=\operatorname{sigmoid}\left(w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} w_{i j} x_{i} x_{j}\right)</script></li>
</ul>
<ul>
<li>Let $w_{ij}=<v_i, v_j="">$ <script type="math/tex; mode=display">
y(x)=sigmoid(w_0 + \sum_{i=1}^n w_ix_i + \sum_{i=1}^{n-1}\sum_{j=i+1}^{n} <v_i,v_j> x_ix_j)</script></v_i,></li>
</ul>
<ul>
<li>Deal with parameter explosion ($n^2 \geq nk$)</li>
<li>Equivalent to low rank parameter matrix factorization $W=VV^T$</li>
</ul>
<h2 id="Pros-and-Cons-3"><a href="#Pros-and-Cons-3" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h2><ul>
<li>Pros<ul>
<li>Good expressiveness </li>
<li>Good generalization </li>
<li>Relatively low training cost</li>
</ul>
</li>
<li>Cons<ul>
<li>Hard to make higher level feature crossing</li>
</ul>
</li>
</ul>
<h1 id="Gradient-Boosting-Decision-Tree-GBDT"><a href="#Gradient-Boosting-Decision-Tree-GBDT" class="headerlink" title="Gradient Boosting Decision Tree (GBDT)"></a>Gradient Boosting Decision Tree (GBDT)</h1><ul>
<li><p>$\mathcal{F}$</p>
<script type="math/tex; mode=display">
F_m(x)=\sum_{m=1}^M T(x;\theta_m)</script></li>
<li><p>Training samples $(x_1, y_1), (x_2, y_2), … , (x_n,y_n)$</p>
</li>
<li><p>Boosting:</p>
</li>
<li><p>$f$ : Regression tre -&gt; Boosting Regression(Decision) Tree</p>
</li>
<li><p>Loss function -&gt; Gradient Boosting Decision Tree, GBDT</p>
<script type="math/tex; mode=display">
L=\sum_i (f(x_i)-y_i)^2 \\
T(x_i;\theta_m) \approx -[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}]_{f(x)=F_{m-1}(x)}</script></li>
</ul>
<h2 id="XGBoost-amp-LightGBM"><a href="#XGBoost-amp-LightGBM" class="headerlink" title="XGBoost &amp; LightGBM"></a>XGBoost &amp; LightGBM</h2><ul>
<li><p>$\mathcal{F}$</p>
<script type="math/tex; mode=display">
\hat{y}=\sum_{k=1}^K f_k(x)</script></li>
<li><p>Change optimization goal</p>
<script type="math/tex; mode=display">
O b j^{(t)}=\sum_{i=1}^{m} L\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(x_{i}\right)\right)+\sqrt{\gamma T}+\sqrt{\frac{\lambda}{2} \sum_{j=1}^{T} w_{j}^{2}}</script></li>
<li><p>Loss function: Taylor expansion, keep second order terms </p>
</li>
<li>Regularization<ul>
<li>Prevent too complicated trees </li>
<li>Prevent extreme parameters </li>
</ul>
</li>
<li>LightGBM (engineering optimization)</li>
</ul>
<h2 id="Pros-and-Cons-4"><a href="#Pros-and-Cons-4" class="headerlink" title="Pros and Cons"></a>Pros and Cons</h2><ul>
<li>Pros<ul>
<li>Good expressiveness </li>
<li>Low eﬀort of feature engineering </li>
</ul>
</li>
<li>Cons<ul>
<li>Hard parallelism</li>
<li>Unable to do incremental training</li>
</ul>
</li>
</ul>
<h1 id="GBDT-LR"><a href="#GBDT-LR" class="headerlink" title="GBDT + LR"></a>GBDT + LR</h1><ul>
<li>Intuition: <strong>model-based feature engineering</strong></li>
</ul>
<p><img src="https://i.loli.net/2020/07/19/bCaOt84o3Aji2pW.png" alt="image.png"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://nobugs.dev/2020/07/18/ESL%20Chapter%2016/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xinyi HU">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xinyi's gadget">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/07/18/ESL%20Chapter%2016/" itemprop="url">The Element of Statistical Learning Chapter 16</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-18T19:37:23+08:00">
                2020-07-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ESL/" itemprop="url" rel="index">
                    <span itemprop="name">ESL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  474
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  2
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Chapter-16-Ensemble-Learning"><a href="#Chapter-16-Ensemble-Learning" class="headerlink" title="Chapter 16. Ensemble Learning"></a>Chapter 16. Ensemble Learning</h1><h2 id="What-is-the-idea-of-Ensemble-Learning"><a href="#What-is-the-idea-of-Ensemble-Learning" class="headerlink" title="What is the idea of Ensemble Learning?"></a>What is the idea of Ensemble Learning?</h2><p>The <strong>idea</strong> of ensemble learning is <strong>to build a prediction model by combining the strengths of a collection of simpler base models</strong>.</p>
<p><a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf" target="_blank" rel="noopener">Zhou Zhihua</a> Ensemble Learning: <strong>to boost</strong> <strong>weak learner</strong>s which are slightly better than random guess to <strong>strong learners</strong> which can make very accurate predictions.</p>
<p>Ensemble learning can be broken down into <strong>two</strong> tasks:</p>
<p><strong>First</strong>, developing a population of base learners from the training data,</p>
<p><strong>then</strong> combining them to form the composite predictor. </p>
<p>Zhou Zhihua:</p>
<p><strong>First</strong>, a number of base learners are produced, which can be generated in a <em>parallel</em> style or in a <em>sequential</em> style where the generation of a base learner has influence on the generation of subsequent learners. </p>
<p><strong>Then</strong>, the base learners are combined to use, where among the most popular combination schemes are <em>majority votin</em>g for classification and <em>weighted averagin</em>g for regression.</p>
<h2 id="List-some-methods-of-Ensemble-Learning"><a href="#List-some-methods-of-Ensemble-Learning" class="headerlink" title="List some methods of Ensemble Learning."></a>List some <strong>methods</strong> of Ensemble Learning.</h2><ul>
<li><p><strong>Bagging</strong>     </p>
</li>
<li><ul>
<li>trains a number of base learners each from a different <em>bootstrap</em> sample by calling a base      learning algorithm.</li>
<li>After obtaining the base learners, Bagging combines them by majority voting and the most-voted      class is predicted.</li>
<li>Sample: Random Forest</li>
<li>Reduce <strong>variance</strong></li>
</ul>
</li>
<li><p><strong>Boosting</strong>     </p>
</li>
<li><ul>
<li>Is a family of algorithms since there are many variants.</li>
<li>Sample: Adaboost</li>
<li>Reduce <strong>bias</strong></li>
</ul>
</li>
<li><p><strong>Stacking</strong>     </p>
</li>
<li><ul>
<li>A number of first-level individual learners are generated from the training data set      by employing different learning algorithms. </li>
<li>Those individual learners are then combined by a second-level learner which is      called as <em>meta-learner</em>. </li>
</ul>
</li>
</ul>
<blockquote>
<p><em>Bayesian methods for nonparametric regression can also be viewed as ensemble methods</em> </p>
</blockquote>
<p>Generally speaking, there is no ensemble method which outperforms other ensemble methods consistently.</p>
<h2 id="List-some-Penalized-Regression-and-how-they-works"><a href="#List-some-Penalized-Regression-and-how-they-works" class="headerlink" title="List some Penalized  Regression and how they works"></a>List some <strong>Penalized  Regression</strong> and how they works</h2><p><strong>Lasso regression</strong> and <strong>ridge regression</strong>.</p>
<p>Consider the dictionary of all possible J-terminal node regression trees $T=\{T_k\}$ that could be realized on the training data as basis functions in  $R^p$. The linear model is</p>
<script type="math/tex; mode=display">
f(x)=\sum_{k=1}^K \alpha_k T_k(x)</script><p>Suppose the coefficients are to be estimated by <strong>least squares</strong>. Since the number of such trees is likely to be much larger than even the largest training data sets, some form of regularization is required. Let $\hat{\alpha}(\lambda)$ solve</p>
<script type="math/tex; mode=display">
\min_{\alpha}\{\sum_{i=1}^{N}(y_i-\sum_{k=1}^K\alpha_kT_k(x_i))^2+\lambda\cdot J(\alpha)\}</script><p>$J(\alpha)$ is a function of the coefficients that generally penalizes larger values. </p>
<script type="math/tex; mode=display">
J(\alpha)=\sum_{k=1}^K |\alpha_k|^2 - ridge  \\
J(\alpha)=\sum_{k=1}^K |\alpha_k| - lasso</script><h2 id="Why-ensemble-superior-to-Singles-generalization"><a href="#Why-ensemble-superior-to-Singles-generalization" class="headerlink" title="Why ensemble superior to Singles - generalization"></a>Why ensemble superior to Singles - <strong>generalization</strong></h2><ul>
<li>the training data might not provide sufficient information for choosing a single best learner</li>
<li>the search processes of the learning algorithms might be imperfect</li>
<li>the hypothesis space being searched might not contain the true target function, while ensembles can give some good approximation. </li>
</ul>
<p><strong>The bias-variance decomposition</strong> is often used in studying the performance of ensemble methods.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://nobugs.dev/2020/07/18/ESL%20Chapter%202/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xinyi HU">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xinyi's gadget">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/07/18/ESL%20Chapter%202/" itemprop="url">The Element of Statistical Learning Chapter 2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-07-18T18:37:23+08:00">
                2020-07-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ESL/" itemprop="url" rel="index">
                    <span itemprop="name">ESL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  182
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Chapter-2-Overview-of-Supervised-Learning"><a href="#Chapter-2-Overview-of-Supervised-Learning" class="headerlink" title="Chapter 2. Overview of Supervised Learning"></a>Chapter 2. Overview of Supervised Learning</h1><p>In stats, <strong>predictors/independent variables</strong> = <strong>input</strong>, <strong>responses/dependent variables</strong> = <strong>output</strong>.</p>
<h2 id="Difference-between-regression-and-classification"><a href="#Difference-between-regression-and-classification" class="headerlink" title="Difference between regression and classification"></a>Difference between <strong>regression</strong> and <strong>classification</strong></h2><p>Regression when we predict quantitative outputs, and classification when we predict qualitative outputs. </p>
<h2 id="Some-notations-on-dataset"><a href="#Some-notations-on-dataset" class="headerlink" title="Some notations on dataset"></a>Some <strong>notations</strong> on dataset</h2><p>We will typically denote an <strong>input</strong> variable by the symbol <strong>X</strong>. <strong>Quantitative outputs</strong> will be denoted by <strong>Y</strong> , and <strong>qualitative outputs</strong> by <strong>G</strong> (for group). </p>
<p><strong>Observed values</strong> are written in lowercase; hence the i-th observed value of <strong>X</strong> is written as .</p>
<p><strong>Matrices</strong> are represented by bold uppercase letters; for example, a set of N input p-vectors , i = 1,…,N would be represented by the N×p matrix <strong>X</strong>. </p>
<p>In general, vectors will not be bold, except when they have N components.</p>
<h2 id="What-is-Nearest-Neighbors"><a href="#What-is-Nearest-Neighbors" class="headerlink" title="What is Nearest Neighbors?"></a>What is <strong>Nearest Neighbors</strong>?</h2><p>Nearest-neighbor methods use those observations in the training set  closest in input space to  to form  . Specifically, the k-nearest neighbor fit for  is defined as follows:</p>
<script type="math/tex; mode=display">
\hat{Y}(x)=\frac{1}{k}\sum_{x_i\in N_k(x)}y_i</script><p>where  is the neighborhood of  defined by the  closest points  in the training sample. </p>
<p>It has <strong>high variance and low bias</strong>.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://nobugs.dev/2020/06/16/Intermediate%20R/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xinyi HU">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xinyi's gadget">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/16/Intermediate%20R/" itemprop="url">Intermediate R</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-16T21:39:02+08:00">
                2020-06-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/R/" itemprop="url" rel="index">
                    <span itemprop="name">R</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  885
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  5
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Equality"><a href="#Equality" class="headerlink" title="Equality"></a>Equality</h1><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">== / identical()</span><br><span class="line">!=</span><br><span class="line">&lt; and &gt; <span class="comment"># greater and less than</span></span><br><span class="line">&amp; <span class="comment"># and</span></span><br><span class="line">| <span class="comment"># or</span></span><br><span class="line">! <span class="comment"># reverse the result</span></span><br></pre></td></tr></table></figure>
<h1 id="Compare"><a href="#Compare" class="headerlink" title="Compare"></a>Compare</h1><h2 id="Compare-vectors"><a href="#Compare-vectors" class="headerlink" title="Compare vectors"></a>Compare vectors</h2><p>linkedin &gt; facebook</p>
<h2 id="Compare-matrices"><a href="#Compare-matrices" class="headerlink" title="Compare matrices"></a>Compare matrices</h2><p>views &lt;= 14</p>
<h1 id="The-if-statement"><a href="#The-if-statement" class="headerlink" title="The if statement"></a>The if statement</h1><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (condition) &#123;</span><br><span class="line">  expr</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (condition2) &#123;</span><br><span class="line">  expr2</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (condition3) &#123;</span><br><span class="line">  expr3</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  expr4</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="Write-a-while-loop"><a href="#Write-a-while-loop" class="headerlink" title="Write a while loop"></a>Write a while loop</h1><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (condition) &#123;</span><br><span class="line">  expr</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Stop-the-while-loop"><a href="#Stop-the-while-loop" class="headerlink" title="Stop the while loop:"></a>Stop the while loop:</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">break</span><br></pre></td></tr></table></figure>
<h2 id="Loop-over-a-vector"><a href="#Loop-over-a-vector" class="headerlink" title="Loop over a vector"></a>Loop over a vector</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">primes &lt;- c(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>)</span><br></pre></td></tr></table></figure>
<h3 id="loop-version-1"><a href="#loop-version-1" class="headerlink" title="loop version 1"></a>loop version 1</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (p <span class="keyword">in</span> primes) &#123;</span><br><span class="line">  print(p)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="loop-version-2"><a href="#loop-version-2" class="headerlink" title="loop version 2"></a>loop version 2</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:length(primes)) &#123;</span><br><span class="line">  print(primes[i])</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Loop-over-a-list"><a href="#Loop-over-a-list" class="headerlink" title="Loop over a list"></a>Loop over a list</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">primes_list &lt;- list(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">11</span>, <span class="number">13</span>)</span><br></pre></td></tr></table></figure>
<h3 id="loop-version-1-1"><a href="#loop-version-1-1" class="headerlink" title="loop version 1"></a>loop version 1</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (p <span class="keyword">in</span> primes_list) &#123;</span><br><span class="line">  print(p)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="loop-version-2-1"><a href="#loop-version-2-1" class="headerlink" title="loop version 2"></a>loop version 2</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:length(primes_list)) &#123;</span><br><span class="line">  print(primes_list[[i]])</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Loop-over-a-matrix"><a href="#Loop-over-a-matrix" class="headerlink" title="Loop over a matrix"></a>Loop over a matrix</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (var1 <span class="keyword">in</span> seq1) &#123;</span><br><span class="line">  <span class="keyword">for</span> (var2 <span class="keyword">in</span> seq2) &#123;</span><br><span class="line">    expr</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">strsplit()</span><br></pre></td></tr></table></figure>
<h1 id="Function-documentation"><a href="#Function-documentation" class="headerlink" title="Function documentation"></a>Function documentation</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">help(sample)</span><br><span class="line">?sample</span><br><span class="line">args(sample)</span><br></pre></td></tr></table></figure>
<h2 id="Write-your-own-function"><a href="#Write-your-own-function" class="headerlink" title="Write your own function"></a>Write your own function</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">my_fun &lt;- function(arg1, arg2&#x3D;TRUE) &#123;</span><br><span class="line">  body</span><br><span class="line">  return(result)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Load-an-R-Package"><a href="#Load-an-R-Package" class="headerlink" title="Load an R Package"></a>Load an R Package</h2><p>• install.packages(), which as you can expect, installs a given package.<br>• library() which loads packages, i.e. attaches them to the search list on your R workspace. /  require()<br>• search(), to look at the currently attached packages and<br>• qplot(mtcars$wt, mtcars$hp), to build a plot of two variables of the mtcars data frame.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">library(ggplot2)</span><br></pre></td></tr></table></figure>
<h1 id="a-powerful-package-for-data-visualization"><a href="#a-powerful-package-for-data-visualization" class="headerlink" title="a powerful package for data visualization"></a>a powerful package for data visualization</h1><p>Use lapply with a built-in R function</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lapply(X, FUN, ...)</span><br></pre></td></tr></table></figure>
<p>To put it generally, lapply takes a vector or list X, and applies the function FUN to each of its members. </p>
<p>To put it generally, lapply takes a vector or list X, and applies the function FUN to each of its members. </p>
<p>lapply and anonymous functions</p>
<h1 id="Named-function"><a href="#Named-function" class="headerlink" title="Named function"></a>Named function</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">triple &lt;- function(x) &#123; 3 * x &#125;</span><br></pre></td></tr></table></figure>
<h2 id="Anonymous-function-with-same-implementation"><a href="#Anonymous-function-with-same-implementation" class="headerlink" title="Anonymous function with same implementation"></a>Anonymous function with same implementation</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">function(x) &#123; 3 * x &#125;</span><br></pre></td></tr></table></figure>
<h1 id="Use-anonymous-function-inside-lapply"><a href="#Use-anonymous-function-inside-lapply" class="headerlink" title="Use anonymous function inside lapply()"></a>Use anonymous function inside lapply()</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lapply(list(1,2,3), function(x) &#123; 3 * x &#125;)</span><br></pre></td></tr></table></figure>
<h2 id="Use-lapply-with-additional-arguments"><a href="#Use-lapply-with-additional-arguments" class="headerlink" title="Use lapply with additional arguments"></a>Use lapply with additional arguments</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">multiply &lt;- function(x, factor) &#123;</span><br><span class="line">  x * factor</span><br><span class="line">&#125;</span><br><span class="line">lapply(list(1,2,3), multiply, factor &#x3D; 3)</span><br></pre></td></tr></table></figure>
<h2 id="How-to-use-sapply"><a href="#How-to-use-sapply" class="headerlink" title="How to use sapply"></a>How to use sapply</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sapply(X, FUN, ...)</span><br></pre></td></tr></table></figure>
<p>sapply() simplifies the list that lapply() would return by turning it into a vector. </p>
<p>sapply() simplifies the list that lapply() would return by turning it into a vector. </p>
<h2 id="Use-vapply"><a href="#Use-vapply" class="headerlink" title="Use vapply"></a>Use vapply</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vapply(X, FUN, FUN.VALUE, ..., USE.NAMES &#x3D; TRUE)</span><br></pre></td></tr></table></figure>
<p>Over the elements inside X, the function FUN is applied. The FUN.VALUE argument expects a template for the return argument of this function FUN. USE.NAMES is TRUE by default; in this case vapply() tries to generate a named array, if possible.</p>
<h1 id="Data-Utilities"><a href="#Data-Utilities" class="headerlink" title="Data Utilities"></a>Data Utilities</h1><p>R features a bunch of functions to juggle around with data structures::<br>    • seq(): Generate sequences, by specifying the from, to, and by arguments.<br>    • rep(): Replicate elements of vectors and lists.<br>    • sort(): Sort a vector in ascending order. Works on numerics, but also on character strings and logicals.<br>    • rev(): Reverse the elements in a data structures for which reversal is defined.<br>    • str(): Display the structure of any R object.<br>    • append(): Merge vectors or lists.<br>    • is.<em>(): Check for the class of an R object.<br>    • as.</em>(): Convert an R object from one class to another.<br>    • unlist(): Flatten (possibly embedded) lists to produce a vector.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grepl &amp; grep</span><br></pre></td></tr></table></figure>
<p>In their most basic form, regular expressions can be used to see whether a pattern exists inside a character string or a vector of character strings. For this purpose, you can use:<br>    • grepl(), which returns TRUE when a pattern is found in the corresponding character string.<br>    • grep(), which returns a vector of indices of the character strings that contains the pattern.</p>
<p>You can use the caret, ^, and the dollar sign, $ to match the content located in the start and end of a string, respectively. This could take us one step closer to a correct pattern for matching only the “.edu” email addresses from our list of emails. But there’s more that can be added to make the pattern more robust:<br>    • @, because a valid email must contain an at-sign.<br>    • .<em>, which matches any character (.) zero or more times (</em>). Both the dot and the asterisk are metacharacters. You can use them to match any character between the at-sign and the “.edu” portion of an email address.<br>    • \.edu​$, to match the “.edu” part of the email at the end of the string. The \\ part escapes the dot: it tells R that you want to use the . as an actual character.</p>
<p>While grep() and grepl() were used to simply check whether a regular expression could be matched with a character vector, sub() and gsub() take it one step further: you can specify a replacement argument. If inside the character vector x, the regular expression pattern is found, the matching element(s) will be replaced with replacement.sub() only replaces the first match, whereas gsub() replaces all matches.</p>
<p>Create and format dates<br>To create a Date object from a simple character string in R, you can use the as.Date() function. The character string has to obey a format that can be defined using a set of symbols (the examples correspond to 13 January, 1982):<br>    • %Y: 4-digit year (1982)<br>    • %y: 2-digit year (82)<br>    • %m: 2-digit month (01)<br>    • %d: 2-digit day of the month (13)<br>    • %A: weekday (Wednesday)<br>    • %a: abbreviated weekday (Wed)<br>    • %B: month (January)<br>    • %b: abbreviated month (Jan)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://nobugs.dev/2020/06/12/Introduction%20to%20R/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xinyi HU">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xinyi's gadget">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/12/Introduction%20to%20R/" itemprop="url">Introduction to R</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-12T21:39:02+08:00">
                2020-06-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/R/" itemprop="url" rel="index">
                    <span itemprop="name">R</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  301
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Introduction-to-R"><a href="#Introduction-to-R" class="headerlink" title="Introduction to R"></a>Introduction to R</h1><p>This guide is a note for datacamp, introduction to R.</p>
<p>I usually use Python, thus in this note, I used Python to contrast R on data collection to data cleaning to data frame…</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>R</th>
<th>Python</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Arithmetic with R </strong> <br>Addition: + <br>Subtraction: - <br>Multiplication: * <br>Division: / <br>Exponentiation: ^ <br>Modulo: %%</td>
<td><br>Addition: + <br>Subtraction: - <br>Multiplication: * <br>Division: / <br>Exponentiation: pow() <br>Modulo: %</td>
</tr>
<tr>
<td><strong>Variable assignment</strong><br>my_var &lt;- 4</td>
<td><strong>Variable assignment</strong><br>my_var = 4</td>
</tr>
<tr>
<td><strong>What’s that data type?</strong><br>class()</td>
<td><strong>Data Type</strong><br>type()</td>
</tr>
<tr>
<td><strong>Create a vector</strong> <br>the combine function c() <br>numeric_vector &lt;- c(1, 2, 3) <br>character_vector &lt;- c(“a”, “b”, “c”)</td>
<td><strong>Create a vector?</strong><br>In python, we have List: []</td>
</tr>
<tr>
<td><strong>Naming a vector</strong> <br>names() <br>some_vector &lt;- c(“John Doe”, “poker player”) <br>names(some_vector) &lt;- c(“Name”, “Profession”)</td>
<td><strong>Naming a vector?</strong><br>I think we normally use another list And we use zip() function to put two lists together. Or we use dict()</td>
</tr>
<tr>
<td>One unique feature: vectors can be added together. If a &lt;- c(1, 2, 3), b &lt;- c(4, 5, 6), then a + b would be c(5, 7, 9)</td>
<td>Lists added together would be like stick together, instead of adding each element one by one. For example, a = [1, 2, 3], b = [4, 5, 6], a + b would be [1, 2, 3, 4, 5, 6]</td>
</tr>
<tr>
<td><strong>Some functions</strong> <br>sum() <br>max() <br>min() <br>mean(na.rm = FALSE) # na.rm: drop NA data <br>abs() <br>round()</td>
<td><strong>Some functions</strong><br>sum() <br>max() <br>min() <br>np.mean()</td>
</tr>
<tr>
<td><strong>Index of vectors</strong> <br>In R, we start from 1.  <br>First element, vector_sample[1]. <br>First to third element, vector_sample[c(1,2,3)] or vector_sample[1:3]</td>
<td><strong>Index of vectors</strong> <br>In python, we start from 0.</td>
</tr>
<tr>
<td><strong>What’s a matrix?</strong> <br>matrix(1:9, byrow = TRUE, nrow = 3) <br>using <code>byrow</code>, we would fill the matrix by rows!</td>
<td>Python doesn’t have this function. We use pandas.DataFrame or numpy to make a matrix, and we can use numpy.reshape.</td>
</tr>
</tbody>
</table>
</div>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://nobugs.dev/2020/03/30/CS224n%20Classnotes%20Course9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xinyi HU">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xinyi's gadget">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/30/CS224n%20Classnotes%20Course9/" itemprop="url">Stanford CS224n Natural Language Processing Course10</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-30T20:25:23+08:00">
                2020-03-30
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  215
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Course-10-Question-Answering"><a href="#Course-10-Question-Answering" class="headerlink" title="Course 10 - Question Answering"></a>Course 10 - Question Answering</h1><h2 id="Motivation-History"><a href="#Motivation-History" class="headerlink" title="Motivation/History"></a>Motivation/History</h2><p>With massive collections of full-text documents, return relevant documents.</p>
<p>2 parts</p>
<ol>
<li>find documents that might contain an answer</li>
<li>find an answer in a paragraph or a document</li>
</ol>
<p>MCTest Reading Comprehension: Passage+Question=Answer</p>
<p><img src="https://i.loli.net/2020/04/04/xpvqj1TbEcJkyOL.png" alt="image-20200330135135377.png"></p>
<h2 id="The-SQuAD-dataset"><a href="#The-SQuAD-dataset" class="headerlink" title="The SQuAD dataset"></a>The SQuAD dataset</h2><p>Evalution </p>
<ul>
<li><p>Systems are scored on two metrics</p>
<ul>
<li>exact match</li>
<li>f1:  Precision = tp/(tp+fp), Recall = tp/(tp+tn), F1=2PR/(P+R) - taken as primary</li>
</ul>
<p>Both metrics ignore punctuation and articles (a, an, the only)</p>
</li>
</ul>
<p>Limiations</p>
<ul>
<li>Only span-based answers</li>
<li>Questions were constructed looking at passages</li>
<li>Barely any multi-facts/sentence inference beyonce coreference</li>
</ul>
<p>But still, well-targeted, well-structured, clean dataset</p>
<h2 id="The-Stanford-Attentive-Reader-model"><a href="#The-Stanford-Attentive-Reader-model" class="headerlink" title="The Stanford Attentive Reader model"></a>The Stanford Attentive Reader model</h2><h2 id="BiDAF"><a href="#BiDAF" class="headerlink" title="BiDAF"></a>BiDAF</h2><p><img src="https://i.loli.net/2020/04/04/wpZ5DlvJ7AK4xVY.png" alt="image-20200331105339872.png"></p>
<p>central idea: the Attention Flow layer</p>
<p>Idea: attention should flow both ways - from the context to the question and from the question to the context</p>
<p>Make the similarity matrix:</p>
<script type="math/tex; mode=display">
S_{ij} = w_{sim}^{T}[c_i;q_j;c_i \cdot q_j]</script><p>Context-to-Question attention:</p>
<script type="math/tex; mode=display">
\alpha^{i} = softmax(S_i,:) \in \mathbb{R}^M\\
\alpha_i=\sum_{j=1}^M \alpha_j^i q_j \in \mathbb{R}^{2h}</script><p>Attention Flow Idea: attention should flow both ways</p>
<p>Question-to-Context attention:</p>
<script type="math/tex; mode=display">
m_i=max_j S_{ij} \in \mathbb{R}\\
\beta = softmax(m) \in \mathbb{R}^N\\
c' = \sum_{i=1}^{N} \beta_i c_i \in \mathbb{R}^{2h}</script><h2 id="Recent-more-advanced-architectures"><a href="#Recent-more-advanced-architectures" class="headerlink" title="Recent, more advanced architectures"></a>Recent, more advanced architectures</h2><p>FusionNet</p>
<h2 id="ELMo-and-BERT-preview"><a href="#ELMo-and-BERT-preview" class="headerlink" title="ELMo and BERT preview"></a>ELMo and BERT preview</h2>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://nobugs.dev/2020/03/16/CS224n%20Classnotes%20Course7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xinyi HU">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xinyi's gadget">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/16/CS224n%20Classnotes%20Course7/" itemprop="url">Stanford CS224n Natural Language Processing Course7</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-16T20:25:23+08:00">
                2020-03-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  757
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  4
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Course-7-Vanishing-Gradients-Fancy-RNN"><a href="#Course-7-Vanishing-Gradients-Fancy-RNN" class="headerlink" title="Course 7 - Vanishing Gradients, Fancy RNN"></a>Course 7 - Vanishing Gradients, Fancy RNN</h1><h2 id="Vanishing-Gradients"><a href="#Vanishing-Gradients" class="headerlink" title="Vanishing Gradients"></a>Vanishing Gradients</h2><p>Gradient can be viewed as a measure of <em>the effect of the past on the future</em></p>
<ul>
<li>There is no dependency between step $t$ and $t+n$ in the data</li>
<li>We have wrong parameters to capture the true dependency between  $t$ and $t+n$ </li>
</ul>
<h2 id="Effect-of-vanishing-gradient-on-RNN-LM"><a href="#Effect-of-vanishing-gradient-on-RNN-LM" class="headerlink" title="Effect of vanishing gradient on RNN-LM"></a>Effect of vanishing gradient on RNN-LM</h2><p>LM task - unable to predict similar long-distance dependencies</p>
<p>Syntactic recency: The <em>writer</em> of the books <strong>is</strong></p>
<p>Sequential recency: The writer of <em>books</em> <strong>are</strong></p>
<p>Due to vanishing gradient, RNN-LMs are better at learning from sequential recency than syntactic </p>
<h2 id="Why-is-exploding-gradient-a-problem"><a href="#Why-is-exploding-gradient-a-problem" class="headerlink" title="Why is exploding gradient a problem?"></a>Why is exploding gradient a problem?</h2><script type="math/tex; mode=display">
\theta^{new} = \theta^{old}-\alpha \nabla_{\theta} J(\theta)</script><h3 id="Solution-gradient-clipping"><a href="#Solution-gradient-clipping" class="headerlink" title="Solution: gradient clipping"></a>Solution: gradient clipping</h3><p>Algorithm 1: Pseudo-code for norm clipping</p>
<p>$\hat{g} \leftarrow \frac{\partial \epsilon}{\partial \theta}$</p>
<p>if $||\hat{g}|| \geq threshold $ then</p>
<p>​    $\hat{g} \leftarrow \frac{threshold}{||\hat{g}||} \hat{g}$</p>
<p>end if </p>
<h2 id="Long-Short-Term-Memory-LSTM"><a href="#Long-Short-Term-Memory-LSTM" class="headerlink" title="Long Short-Term Memory(LSTM)"></a>Long Short-Term Memory(LSTM)</h2><p>On step $t$, there is a hidden state $h^{(t)}$ and a cell state $c^{(t)}$</p>
<ul>
<li>Both are vectors length $n$</li>
<li>The cell stores long-term information</li>
<li>The LSTM can erase, write and read information from the cell</li>
</ul>
<p>The selection is controlled by 3 corresponding <strong>gates</strong></p>
<ul>
<li>vector length $n$</li>
<li>each element of the gates can be open(1), closed(0), or in between</li>
<li>dynamic: their value is computed based on the current context</li>
</ul>
<p>We have a sequence of input $x^{(t)}$, and we will compute a sequence of hidden states $h^{(t)}$ and cell states $c^{(t)}$. On timestep $t$:</p>
<p>Forget gate - controls what is kept vs forgotten, from previous cell state</p>
<script type="math/tex; mode=display">
\boldsymbol{f}^{(t)}={\sigma}\left(\boldsymbol{W}_{f} \boldsymbol{h}^{(t-1)}+\boldsymbol{U}_{f} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{f}\right)</script><p>Input gate - controls what parts of the new cell content are written to cell</p>
<script type="math/tex; mode=display">
\boldsymbol{i}^{(t)}=\sigma\left(\boldsymbol{W}_{i} \boldsymbol{h}^{(t-1)}+\boldsymbol{U}_{i} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{i}\right)</script><p>Output gate - controls what parts of cell are output to hidden state</p>
<script type="math/tex; mode=display">
\boldsymbol{o}^{(t)}={\sigma}\left(\boldsymbol{W}_{o} \boldsymbol{h}^{(t-1)}+\boldsymbol{U}_{o} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{o}\right)</script><p>New cell content - this is the new content to be written to the cell</p>
<script type="math/tex; mode=display">
\tilde{\boldsymbol{c}}^{(t)}=\tanh \left(\boldsymbol{W}_{c} \boldsymbol{h}^{(t-1)}+\boldsymbol{U}_{c} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{c}\right)</script><p>Cell state - erase(“forget”) some content from last cell state, and write(“input”) some new cell content</p>
<script type="math/tex; mode=display">
\boldsymbol{c}^{(t)}=\boldsymbol{f}^{(t)} \circ \boldsymbol{c}^{(t-1)}+\boldsymbol{i}^{(t)} \circ \tilde{\boldsymbol{c}}^{(t)}</script><p>Hidden state: read(“output”) some content from the cell</p>
<script type="math/tex; mode=display">
\boldsymbol{h}^{(t)}=\boldsymbol{o}^{(t)} \circ \tanh \boldsymbol{c}^{(t)}</script><p><img src="https://i.loli.net/2020/04/04/pW7mMj9aDN53h8C.png" alt="image-20200316220913640.png"></p>
<h2 id="Gated-Recurrent-Units-GRU"><a href="#Gated-Recurrent-Units-GRU" class="headerlink" title="Gated Recurrent Units(GRU)"></a>Gated Recurrent Units(GRU)</h2><p>a simpler alternative to the LSTM</p>
<p>On each timestep $t$ we have input $x^{(t)}$ and hidden state $h^{(t)}$ (no cell state)</p>
<p>Update gate - controls what parts of hidden state are updated vs preserved</p>
<script type="math/tex; mode=display">
\boldsymbol{u}^{(t)}=\sigma\left(\boldsymbol{W}_{u} \boldsymbol{h}^{(t-1)}+\boldsymbol{U}_{u} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{u}\right)</script><p>Reset gate - controls what parts of previous hidden state are used to compute new content</p>
<script type="math/tex; mode=display">
\boldsymbol{r}^{(t)}=\sigma\left(\boldsymbol{W}_{r} \boldsymbol{h}^{(t-1)}+\boldsymbol{U}_{r} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{r}\right)</script><p>New hidden state content - reset gate selects useful parts of prev hidden state. Use this and current input to compute new hidden content.</p>
<script type="math/tex; mode=display">
\tilde{\boldsymbol{h}}^{(t)}=\tanh \left(\boldsymbol{W}_{h}\left(\boldsymbol{r}^{(t)} \circ \boldsymbol{h}^{(t-1)}\right)+\boldsymbol{U}_{h} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{h}\right)</script><p>Hidden state - update gate simultaneously controls what is kept from previous hidden state, and what is updated to new hidden state content</p>
<script type="math/tex; mode=display">
\boldsymbol{h}^{(t)}=\left(1-\boldsymbol{u}^{(t)}\right) \circ \boldsymbol{h}^{(t-1)}+\boldsymbol{u}^{(t)} \circ \tilde{\boldsymbol{h}}^{(t)}</script><h2 id="LSTM-vs-GRU"><a href="#LSTM-vs-GRU" class="headerlink" title="LSTM vs GRU"></a>LSTM vs GRU</h2><p>LSTM is a good default choice.</p>
<p>Rule of thumb: start with LSTM, but switch to GRU if you want something more efficient</p>
<h2 id="Vanishing-exploding-gradient"><a href="#Vanishing-exploding-gradient" class="headerlink" title="Vanishing/exploding gradient"></a>Vanishing/exploding gradient</h2><p>add more direct connections.</p>
<p>e.g.: Residual connections, “ResNet”. Skip-connections. The identity connection preserves information by default.</p>
<p>e.g.: Dense connections, “DenseNet”. </p>
<p>e.g.:Highway connections, “HighwayNet”.</p>
<h2 id="Bidirectional-RNNs"><a href="#Bidirectional-RNNs" class="headerlink" title="Bidirectional RNNs"></a>Bidirectional RNNs</h2><p>motivation - task: Sentiment Classification</p>
<p><img src="https://i.loli.net/2020/04/04/on9a2s1Y5jJiADb.png" alt="image-20200317001951410.png"></p>
<p>Note: bidirection RNNs are only applicable if you have access to the entire input sentence. </p>
<p>BERT is built on bidirectionality.</p>
<h2 id="Multi-layer-RNNs-stacked-RNNs"><a href="#Multi-layer-RNNs-stacked-RNNs" class="headerlink" title="Multi-layer RNNs = stacked RNNs"></a>Multi-layer RNNs = stacked RNNs</h2><p>High-performing RNNs are often multi-layer(but aren’t as deep as cn)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://nobugs.dev/2020/03/16/CS224n%20Classnotes%20Course8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xinyi HU">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xinyi's gadget">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/16/CS224n%20Classnotes%20Course8/" itemprop="url">Stanford CS224n Natural Language Processing Course8</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-16T20:25:23+08:00">
                2020-03-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  553
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  3
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Course-8-Translation-Seq2Seq-Attention"><a href="#Course-8-Translation-Seq2Seq-Attention" class="headerlink" title="Course 8 - Translation, Seq2Seq, Attention"></a>Course 8 - Translation, Seq2Seq, Attention</h1><h2 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h2><p>from the source language to the target language</p>
<h3 id="Statistical-Machine-Translation"><a href="#Statistical-Machine-Translation" class="headerlink" title="Statistical Machine Translation"></a>Statistical Machine Translation</h3><p>Core idea: learn a probabilistic model from data.</p>
<p>best  English sentence $y$, given French sentence $x$</p>
<script type="math/tex; mode=display">
argmax_y P(y|x)\\
=argmax_y P(x|y) P(y)</script><p>Translation model($P(x|y)$) - Models how words and phrases should be translated. Learnt from <strong>parallel data</strong></p>
<p>note: parallel data - pairs of human-translated french/english sentences.</p>
<p>Language model($P(y)$) - Models how to write good English. Learnt from monolingual data.</p>
<h4 id="Learning-alignment-for-SMT"><a href="#Learning-alignment-for-SMT" class="headerlink" title="Learning alignment for SMT"></a>Learning alignment for SMT</h4><p>alignment is the <em>correspondence between particular words</em> in the translated sentence pair.</p>
<p>Alignment can be <strong>many-to-one</strong> &amp; <strong>one-to-many</strong> &amp; <strong>many-to-many</strong></p>
<p>We learn $P(x, a|y)$ as a combination of many factors.</p>
<h4 id="Decoding-for-SMT"><a href="#Decoding-for-SMT" class="headerlink" title="Decoding for SMT"></a>Decoding for SMT</h4><p>Use a <strong>heuristic search algorithm</strong> to search for the best translation, discarding hypotheses that are too low-probability. This process is called <em>decoding</em></p>
<h3 id="Neural-Machine-Translation"><a href="#Neural-Machine-Translation" class="headerlink" title="Neural Machine Translation"></a>Neural Machine Translation</h3><p><strong>sequence-to-sequence(Seq2seq)</strong> involves 2 RNNs. a <strong>conditional language model</strong></p>
<ul>
<li>conditional - its predictions are conditioned on the source sentence $x$</li>
</ul>
<p><img src="https://i.loli.net/2020/04/04/gixM6LmGa5Ce7Js.png" alt="image-20200328233749255.png"></p>
<p>Other tasks </p>
<ul>
<li>summarization</li>
<li>dialogue</li>
<li>parsing</li>
<li>code generation (natural language -&gt; Python code)</li>
</ul>
<p>NMT directly calculates $P(y|x)$</p>
<script type="math/tex; mode=display">
P(y | x)=P\left(y_{1} | x\right) P\left(y_{2} | y_{1}, x\right) P\left(y_{3} | y_{1}, y_{2}, x\right) \ldots P\left(y_{T} | y_{1}, \ldots, y_{T-1}, x\right)</script><p>Question: How to train?</p>
<p>Answer: Get a big parallel corpus…</p>
<p>Greedy decoding has many problems, it’s an <strong>exhaustive search decoding</strong>, instead we use <strong>beam search decoding</strong></p>
<p>Core idea: On each step of decoder, keep track of the <em>k most probable</em> partial translations (<em>hypotheses</em>)</p>
<ul>
<li><em>k</em> is the <strong>beam size</strong></li>
</ul>
<script type="math/tex; mode=display">
score(y_1,\cdots,y_t)=logP_{LM}(y_1,\cdots,y_t|x)=\sum_{i=1}^t logP_{LM}(y_i|y_1,\cdots,y_{i-1}, x)</script><ul>
<li>Scores are all negative, and higher score is betterr</li>
<li>We search for high-scoring hypotheses, tracking top <em>k</em> on each step</li>
</ul>
<p>Beam search is <em>not guaranteed</em> to find optimal solution, but efficient.</p>
<p>In greedy decoding, usually we decode until the model produces a <END> token. For example, <START> he hit me with a pie <END></END></START></END></p>
<p>Problem：How to select top one with highest score? longer hypotheses have lower scores</p>
<p>Fix: Normalize by length. </p>
<script type="math/tex; mode=display">
\frac{1}{t}\sum_{i=1}^t logP_{LM}(y_i|y_1,\cdots,y_{i-1}, x)</script><h4 id="Advantages-of-NMT"><a href="#Advantages-of-NMT" class="headerlink" title="Advantages of NMT"></a>Advantages of NMT</h4><ul>
<li>better performance.<ul>
<li>more fluent</li>
<li>better use of context</li>
<li>better use of phrase similarities</li>
</ul>
</li>
<li>A single neural network to be optimized end-to-end<ul>
<li>No subcomponents to be individually optimized</li>
</ul>
</li>
<li>Requires much less human engineering effort<ul>
<li>No feature engineering</li>
</ul>
</li>
</ul>
<h4 id="Disadvantages-of-NMT"><a href="#Disadvantages-of-NMT" class="headerlink" title="Disadvantages of NMT"></a>Disadvantages of NMT</h4><p>Compared to SMT:</p>
<ul>
<li>NMT is less interpretable<ul>
<li>Hard to debug</li>
</ul>
</li>
<li>NMT is difficult to control</li>
</ul>
<h4 id="How-to-evaluate"><a href="#How-to-evaluate" class="headerlink" title="How to evaluate?"></a>How to evaluate?</h4><p>BLEU(Bilingual Evaluation Understudy)</p>
<p>BLEU compares the machine-written translation to one or several human-written translations, and computes similarity-score based on:</p>
<ul>
<li>n-gram precision</li>
<li>Plus a penalty for too-short system tranlations</li>
</ul>
<h4 id="Difficulties-remain"><a href="#Difficulties-remain" class="headerlink" title="Difficulties remain"></a>Difficulties remain</h4><p>Out-of-vocabulary words</p>
<p>Domain mismatch between train and test data</p>
<p>Maintaining context over longer text</p>
<p>Low-resource language pairs</p>
<p>NMT picks up <strong>biases</strong> in training data</p>
<p>Uninterpretable system do strange things.</p>
<h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p>Bottleneck problem.</p>
<p>definition - Given a set of vector <em>values</em>, and a vector <em>query</em>, <strong>attention</strong> is a technique to compute a weighted sum of the values, dependent on the query. </p>
<p>variants</p>
<ul>
<li>Basic dot-product attention</li>
<li>Multiplicative attention</li>
<li>Additive attention</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://nobugs.dev/2020/03/10/How_powerful_is_gnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xinyi HU">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xinyi's gadget">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/10/How_powerful_is_gnn/" itemprop="url">How powerful is Graph Neural Networks?</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-10T12:56:21+08:00">
                2020-03-10
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  6
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="How-powerful-is-Graph-Neural-Networks"><a href="#How-powerful-is-Graph-Neural-Networks" class="headerlink" title="How powerful is Graph Neural Networks?"></a>How powerful is Graph Neural Networks?</h1><p>GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. </p>
<p>There are two tasks of interest: </p>
<p>(1) Node classiﬁcation</p>
<p>(2) Graph classiﬁcation</p>
<p>x Formally, the k-th layer of a GNN is</p>
<script type="math/tex; mode=display">
a_{v}^{(k)}=\operatorname{AGGREGATE}^{(k)}\left(\left\{h_{u}^{(k-1)}: u \in \mathcal{N}(v)\right\}\right), \quad h_{v}^{(k)}=\mathrm{COMBINE}^{(k)}\left(h_{v}^{(k-1)}, a_{v}^{(k)}\right)</script><p> In the pooling variant of GraphSAGE, AGGREGATE has been formulated as </p>
<script type="math/tex; mode=display">
a_{v}^{(k)}=\operatorname{MAX}\left(\left\{\operatorname{ReLU}\left(W \cdot h_{u}^{(k-1)}\right), \forall u \in \mathcal{N}(v)\right\}\right)</script><p>and COMBINE could be concatenation followed by a linear mapping </p>
<script type="math/tex; mode=display">
W \cdot [h_{v}^{(k-1)}, a_{v}^{(k)}]</script><p>Graph Convolutional Networks (GCN) - the element-wise <em>mean</em> pooling is used. AGGREGATE and COMBINE step</p>
<script type="math/tex; mode=display">
h_{v}^{(k)}=\operatorname{ReLU}\left(W \cdot \operatorname{MEAN}\left\{h_{u}^{(k-1)}, \forall u \in \mathcal{N}(v) \cup\{v\}\right\}\right)</script><p>the READOUT function aggregates node features from the ﬁnal iteration to obtain the entire graph’s representation $h_G$</p>
<script type="math/tex; mode=display">
h_{G}=\operatorname{READOUT}\left(\left\{h_{v}^{(K)} | v \in G\right\}\right)</script><p><strong>Deﬁnition1 (Multiset).</strong> A multiset is a generalized concept of a set that allows multiple instances for its elements. More formally, a multiset is a 2-tuple $X = (S,m)$ where $S$ is the underlying set of $X$ that is formed from its distinct elements, and $m : S \rightarrow \mathbb{N}_{\geq 1}$ gives the multiplicity of the elements. </p>
<p><strong>Lemma2</strong>. Let $G_1$ and $G_2$ be any two non-isomorphic graphs. If a graph neural network $A : G→\mathbb{R}^d $ maps $G_1$ and $G_2$ to different embeddings, the Weisfeiler-Lehman graph isomorphism test also decides $G_1$ and $G_2$ are not isomorphic. </p>
<p><strong>Theorem 3</strong>. Let $A : G → \mathbb{R}^d$ be a GNN. With a sufﬁcient number of GNN layers, A maps any graphs $G_1$ and $G_2$ that the Weisfeiler-Lehman test of isomorphism decides as non-isomorphic, to different embeddings if the following conditions hold: </p>
<p>a) $A$ aggregates and updates node features iteratively with </p>
<script type="math/tex; mode=display">
h_{v}^{(k)}=\phi\left(h_{v}^{(k-1)}, f\left(\left\{h_{u}^{(k-1)}: u \in \mathcal{N}(v)\right\}\right)\right)</script><p>where the functions $f$, which operates on multisets, and $φ$ are injective. </p>
<p>b) $A$’s graph-level readout, which operates on the multiset of node features ${h_{v}^{(k)}}$, is injective. </p>
<p><strong>Lemma4</strong>. Assume the input feature space $\mathcal{X}$ is <em>countable</em>. Let $g^{(k)}$ be the function parameterized by a GNN’s k-th layer for $k = 1,…,L$, where $g^{(1)}$ is deﬁned on multisets $X \subset \mathcal{X}$ of bounded size. The range of $g^{(k)}$, i.e., the space of node hidden features ${h_{v}^{(k)}}$, is also countable for all  $k = 1,…,L$.</p>
<p><em>countable</em>: If a set A has the same cardinality as $\mathbb{N}$, then we say that A is <em>countable</em>.</p>
<p><strong>Lemma5</strong>. Assume $\mathcal{X}$ is countable. There exists a function $f : \mathcal{X} →\mathbb{R}^n$ so that  $h(X) =\sum _{x\in X} f(x)$ is unique for each multiset  $X \subset \mathcal{X}$  of bounded size. Moreover, any multiset function $g$ can be decomposed as $g (X) = \varphi(\sum _{x\in X} f(x))$ for some function $\varphi$.</p>
<p><strong>Corollary 6</strong>. Assume $\mathcal{X}$ is countable. There exists a function $f : \mathcal{X} →\mathbb{R}^n$ so that for inﬁnitely many choices of $\epsilon$, including all irrational numbers, $h(c,X) = (1 + \epsilon) · f(c) + \sum_{x \in X} f(x)$ is unique for each pair $(c,X)$, where $c \in \mathcal{X}$ and $X \subset \mathcal{X}$ is a multiset of bounded size. Moreover, any function $g$ over such pairs can be decomposed as $g (c,X) = \varphi((1 + \epsilon)·f(c) + \sum_{x\in X} f(x))$ for some function $\varphi$. </p>
<h2 id="GIN-GRAPH-ISOMORPHISM-NETWORK"><a href="#GIN-GRAPH-ISOMORPHISM-NETWORK" class="headerlink" title="GIN - GRAPH ISOMORPHISM NETWORK"></a>GIN - GRAPH ISOMORPHISM NETWORK</h2><p>To update node representation - </p>
<script type="math/tex; mode=display">
h_{v}^{(k)}=\operatorname{MLP}^{(k)}\left(\left(1+\epsilon^{(k)}\right) \cdot h_{v}^{(k-1)}+\sum_{u \in \mathcal{N}(v)} h_{u}^{(k-1)}\right)</script><p>node learnt can be directly used for tasks like <strong>node classification and link prediction</strong>.</p>
<p>Readout function for graph classiﬁcation tasks. Given embeddings of individual nodes, readout function produces the embedding of the entire graph. </p>
<script type="math/tex; mode=display">
h_{G}=\operatorname{CONCAT}\left(\operatorname{READOUT}\left(\left\{h_{v}^{(k)} | v \in G\right\}\right) | k=0,1, \ldots, K\right)</script><h2 id="GNN-GRAPH-NEURAL-NETWORK"><a href="#GNN-GRAPH-NEURAL-NETWORK" class="headerlink" title="GNN - GRAPH NEURAL NETWORK"></a>GNN - GRAPH NEURAL NETWORK</h2><p>GNN do not satisfy the conditions in Theorem 3, and we conduct ablation studies in </p>
<p>*An ablation study typically refers to removing some “feature” of the model or algorithm, and seeing how that affects performance.</p>
<p>(1) 1-layer perceptrons instead of MLPs</p>
<p>We are interested in understanding whether 1-layer perceptrons are enough for graph learning.</p>
<p><strong>Lemma 7.</strong> There exist ﬁnite multisets $X1 \neq X2$ so that for any linear mapping $W$, $\sum_{x\in X_1} ReLU(Wx) =\sum_{x\in X_2} ReLU(Wx). $</p>
<p>(2) mean or max-pooling instead of the sum.</p>
<p>Mean learns distributions, and max-pooling learns sets with distinct elements.</p>
<p>Consider $X_1 = (S,m)$ and $X_2 = (S,k ·m)$, where $X_1$ and $X_2$ have the same set of distinct elements, but $X_2$ contains $k$ copies of each element of $X_1$.</p>
<p><strong>Corollary 8.</strong> Assume $X$ is countable. There exists a function $f : \mathcal{X} → \mathbb{R^n}$ so that for $h(X) = \frac{1}{|X|}\sum{x\in X} f(x), h(X_1) = h(X_2)$ if and only if multisets $X_1$ and $X_2$ have the same distribution. That is, assuming $|X_2|\geq|X_1|$, we have $X_1 = (S,m)$ and $X_2 = (S,k\cdot m)$ for some $k \in \mathbb{N}_{\geq 1}.$</p>
<p><strong>Corollary 9</strong>. Assume $X$  is countable. Then there exists a function $f : X → \mathbb{R}^{\infty}$ so that for $h(X) = max_{x\in X} f(x), h(X_1) = h(X_2)$ if and only if $X_1$ and $X_2$ have the same underlying set. </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://nobugs.dev/2020/03/02/GNN-survey/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xinyi HU">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xinyi's gadget">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/03/02/GNN-survey/" itemprop="url">GNN survey</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-03-02T12:56:21+08:00">
                2020-03-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  429
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  2
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Background-and-Definitions"><a href="#Background-and-Definitions" class="headerlink" title="Background and Definitions"></a>Background and Definitions</h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>Early studies fall into the category of recurrent graph neural networks(<strong>RecGNNs</strong>). With the success of CNNs,  new approach developed. (<strong>ConvGNNs</strong>)  ConvGNNs are divided into two main streams, the <strong>spectral-based</strong> approaches and the <strong>spatial-based</strong> approaches. </p>
<p>Network embedding aims at representing network nodes as low-dimensional vector representations, preserving both network topology structure and node content information.</p>
<p>Graph kernel methods employ a kernel function to measure the similarity between pairs of graphs.  Graph kernels can embed graphs or nodes into vector spaces by a mapping function. The difference between GNN and graph kernels is that this mapping function is deterministic rather than learnable.</p>
<h2 id="Definitions"><a href="#Definitions" class="headerlink" title="Definitions"></a>Definitions</h2><p><strong>Deﬁnition 1 (Graph):</strong> A graph is represented as $G = (V,E)$ where $V$ is the set of vertices or nodes, and $E$ is the set of edges. Let $v_i \in V$ to denote a node and $e_{ij} = (v_i,v_j) \in E$ to denote an edge pointing from $v_j$ to $v_i$. The neighborhood of a node $v$ is deﬁned as $N(v) = {u \in V|(v,u) \in E}$. The adjacency matrix $A$ is a $n \times n$ matrix with $A_{ij} = 1$ if $e_{ij} \in E$ and $A_{ij} = 0$ if $e_{ij} \notin E$. A graph may have node attributes $ x_{v}$ , where $X \in R^{n×d}$ is a node feature matrix with $x_{v} \in R^d$  representing the feature vector of a node $v$. Meanwhile, a graph may have edge attributes $X^e$, where $X^e \in R^{m×c}$ is an edge feature matrix with $x^{e}_{v,u} \in R^c$ representing the feature vector of an edge $(v,u)$.</p>
<p><strong>Deﬁnition 2 (Directed Graph):</strong> A directed graph is a graph with all edges directed from one node to another. An undirected graph is considered as a special case of directed graphs where there is a pair of edges with inverse directions if two nodes are connected. A graph is undirected if and only if the adjacency matrix is symmetric. </p>
<p><strong>Deﬁnition 3 (Spatial-Temporal Graph)</strong>: A spatial-temporal graph is an attributed graph where the node attributes change dynamically over time. The spatial-temporal graph is deﬁned as $G^{(t)} = (V,E,X^{(t)})$ with $X^{(t)} \in R^{n×d}$. </p>
<h1 id="Categorization-and-Frameworks"><a href="#Categorization-and-Frameworks" class="headerlink" title="Categorization and Frameworks"></a>Categorization and Frameworks</h1><p>Recurrent Neural Network - aim to learn node representations with recurrent neural architectures.</p>
<p>Convolutional graph neural networks - generate a node $v$’s representation by aggregating its own features $x_v$ and neighbors’ features $x_u$, where $u \in N(v)$ </p>
<p>Graph autoencoders(GAE)  learn network embeddings and graph generative distributions</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/uploads/avatar.jpg"
                alt="Xinyi HU" />
            
              <p class="site-author-name" itemprop="name">Xinyi HU</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/samaritanhu" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:samaritanhu@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.linkedin.com/in/xinyi-cindy-hu-b98b1a163/" target="_blank" title="Linkedin">
                      
                        <i class="fa fa-fw fa-globe"></i>Linkedin</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        ﻿<div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-snowflake-o"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xinyi HU</span>

  
</div>

<!--

  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>

-->


    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
 
    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
 



<!--

  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>

-->


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  
  







  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  


</body>
</html>
